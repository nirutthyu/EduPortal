[
    {
        "topic": "Definition and Overview",
        "url": "https://www.youtube.com/watch?v=uO9P-J356vw",
        "transcript": "Error fetching transcript for video uO9P-J356vw: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=uO9P-J356vw! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
    },
    {
        "topic": "History and Evolution",
        "url": "https://www.youtube.com/watch?v=dGiQaabX3_o",
        "transcript": "The world we live in feels normal, ordinary.\nIt feels like this is just how humans exist and always existed.\nBut, it's not.\nNever before have we humans lived in a world as sophisticated and engineered to our needs as today.\nGiving us the luxury to forget about ourselves and not worry about survival.\nFood, shelter, security \u2013 all of this is, more or less, taken for granted.\nBut we're a special few; for more than 99.99% of human history, life was completely different.\nAnd there's no such thing as just one human history.\nOur story begins 6 million years ago, when the tribe of hominini split and our relationship with the apes ended.\n2.8 million years ago, the genus of homo, the first humans, emerged.\nWe like to think about ourselves as the only humans, but this is far from the truth.\nWhen we, homo sapiens sapiens, came into existence 200,000 years ago, there were at least six other human species around.\nCousins of comparable intelligence and ability, which must have been incredibly scary, kind of like living with aliens.\nSome of them were very successful.\nHomo erectus, for example, survived for 2 million years.\nTen times longer than modern humans have existed.\nThe last of the other humans disappeared around 10,000 years ago.\nWe don't know what caused them to die out.\nModern humans have at least a few percent of neanderthal and other human DNA, so there was some mixing,\nbut certainly not enough to be a merger between species.\nSo we don't know if our cousins went away because they lost the battle over resources, or because of a series of minor genocides.\nEither way, only we remain.\nBack to the beginnings of humanity.\n2.8 million years ago, early humans used tools, but did not make a lot of progress for nearly 2 million years.\nUntil they learned to control fire.\nFire meant cooking, which made food more nutritious, which contributed to the development of our brain.\nIt also produced light and warmth, which made days longer and winters less gruesome.\nOn top of that, it not only scared predators away, it could also be used for hunting.\nA torched wood or grassland provided small animals, nuts and tubers that were pre-roasted.\nFrom 300,000 years ago, most of the different human species lived in small hunter-gatherer societies.\nThey had fire, wood and stone tools, planned for the future, buried their dead, and had cultures of their own.\nBut most importantly, they spoke to each other.\nProbably in a kind of proto-language, less complex than ours.\nIf we had a time machine, how far would we be able to go back,\nsteal a few babies and raise them today without anyone noticing that they're a bit different?\nThere is much debate.\nAnatomically, modern humans emerged 200,000 years ago,\nbut probably 70,000 years is as far as we could travel back and still snatch a behaviourally modern human.\nBefore that, the babies would probably lack a few crucial gene mutations\nNecessary to build a brain with modern language and abstract thinking abilities.\nAt some point, around 50,000 years ago, there was an explosion in innovation.\nTools and weapons became more sophisticated and culture became more complex,\nbecause at this point, humans had a multi-purpose brain,\nand a more advanced language to communicate information with each other effectively,\nand down to the last detail.\nThis allowed much closer cooperation, and is what really makes us different from any other creature on Earth.\nNot our comparatively weak bodies and inferior senses,\nbut the ability to cooperate flexibly in large groups, unlike, for example, rigid beehives\nor intimate, but tiny wolf packs.\nAs our brain evolved, we became able to do something, life had been unable to do up to this point.\nOne \u2013 expand knowledge quickly.\nTwo \u2013 preserve the knowledge gained over generations.\nThree \u2013 build on past knowledge, to gain even deeper insight.\nThis seems daft, but until then, information had to be passed on from generation to generation,\nmostly through genetics, which is not efficient.\nStill, for the next 40,000 years, human life remained more or less the same.\nThere was little to build upon.\nOur ancestors were only one animal among many.\nBuilding a skyscraper without knowing what a house is\u2026 is hard.\nBut while it is easy to be arrogant in our attitude to our ancestors, this would be ignorant.\nHumans 50,000 years ago were survival specialists.\nThey had a detailed mental map of their territory,\ntheir senses were fine-tuned to the environment,\nthey knew and memorized a great amount of information about plants and animals.\nThey could make complicated tools that required years of careful training and very fine motor skills\nTheir bodies compared to our athletes today just because of their daily routines,\nand they lived a rich social life within their tribe\nSurvival required so many skills that the average brain volume of early modern humans\nmight even have been bigger than it is today\nAs a group we know more today, but as individuals our ancestors were superior to us\nBut then around 12,000 years ago, in multiple locations, humans developed agriculture.\nEverything changed very quickly.\nBefore, survival as a hunter and forager required superb physical and mental abilities in all fields from everybody\nWith the rise of the agricultural age, individuals could increasingly rely on the skills of others for survival.\nThis meant that some of them could specialize.\nMaybe they worked on better tools, maybe they took time to breed more resistant crops or better livestock,\nMaybe they started inventing things.\nAs farming got more and more efficient, what we call civilization began\nAgriculture gave us a reliable and predictable food source,\nwhich allowed humans to hoard food on a large scale for the first time,\nwhich is much easier to do with grains than meat,\nThe food stock required protection, which led to communities living together in tighter spaces\nFirst, early defense structures were built, the need for organization grew\nThe more organized we got, the faster things became efficient\nVillages became cities, cities became kingdoms, kingdoms became empires\nConnections between humans exploded which led to opportunities to exchange knowledge\nProgress became exponential\nAbout 500 years ago the Scientific Revolution began\nMathematics, Physics, Astronomy, Biology, and Chemistry transformed everything we thought we knew\nThe Industrial Revolution followed soon after laying the foundation for the modern world\nAs our overall efficiency grew exponentially,\nmore people could spend their lifetime contributing to the progress of humanity\nRevolutions kept happening.\nThe invention of the computer, its evolution into a medium we all use on a daily basis,\nand the rise of the Internet shaped our world\nIt's hard to grasp how fast all of that happened\nIt's been about 125,000 generations since the emergence of the first human species\nAbout 7,500 generations since the physiologically modern humans saw the light of day\n500 generations ago, what we call civilization began\n20 generations ago, we learned how to do science\nAnd the Internet became available to most people only one generation ago\nToday we live in the most prosperous age humanity has ever experienced\nWe have transformed this planet, from the composition of its atmosphere to large-scale changes in its landscape\nand also in terms of the other animals in existence.\nWe light up the night with artificial stars and put people in a metal box in the sky\nSome have even walked on our Moon\nWe put robots on other planets\nWe've looked deep into the past of the universe with mechanical eyes\nOur knowledge and our way of acquiring and storing more of it has exploded\nThe average high school student today knows more about the universe than a scholar a few centuries ago\nHumans dominate this planet, even if our rule is very fragile\nWe are still not that different from our ancestors 70,000 years ago\nBut your lifestyle has existed for less than 0.001% of human history\nFrom here on, there's no saying what the future holds for us\nWe're building a skyscraper, but we're not sure if it's standing on a solid foundation\nor if we're building it on quicksand\nLet's leave it with that for now\nThe next time you miss your train, your burger is not hot enough, or someone cuts in line\nRemember how special this made-up human world is\nMaybe it's not worth being upset about all those little things.\nOK, so this was our first take on making a history-related video\nwe'd love to make much more of them, but they take even more time than our average video.\nSo we might do 3 or 4 a year.\nYour feedback's very welcome here\nThank you so much for watching,\nand if you want to support us directly,\nyou can do so on Patreon.\nIt really helps us out.\nWhile you think about it, here are more videos, if you need more distraction."
    },
    {
        "topic": "Applications and Impact",
        "url": "https://www.youtube.com/watch?v=OFS74kc7OO8",
        "transcript": "the textile industry with its Rich\nhistory dating back centuries is\nundergoing a profound transformation\nthanks to the integration of artificial\nintelligence AI this script explores the\nmultifaceted applications and\nfar-reaching impact of AI within the\ntextile Sector 10 what is AI in textiles\nbut before we dive into the applications\nlet's first understand what Ai and\ntextiles means artificial intelligence\nor AI refers to the simulation of human\nintelligence in machines in the textile\nindustry AI involves using computer\nsystems to perform tasks that typically\nrequire human intelligence like pattern\nrecognition decision- making and problem\nsolving one of the primary applications\nof AI and textiles is pattern\nrecognition and it's a GameChanger\nimagine the intricate patterns that\nAdorn Fabrics from classic designs to\nintricate motifs AI with its machine\nlearning algorithms can swiftly\nrecognize analyze and replicate these\npatterns with matched Precision this not\nonly accelerates the design process but\nalso ensures that textiles maintain the\nhighest\nquality nine AI in design and product\ndevelopment one of the significant areas\nwhere AI is making an impact in textiles\nis in design and product development\ndesigning textiles can be a\ntimeconsuming process but AI algorithms\ncan analyze Trends customer preferences\nand historical data to create Innovative\ndesigns more efficiently just about\nAesthetics it's about functionality too\nAI plays a pivotal role in decision-\nmaking during the design phase by\nanalyzing vast amounts of data including\nconsumer preferences market trends and\nhistorical patterns AI assists designers\nin making informed decisions about color\npaletes fabric types and even production\nmethods it's like having a creative\npartner with access to a world of design\nknowledge eight quality control and\ninspection iction AI is also enhancing\nthe quality control and inspection\nprocesses in the textile industry\nMachine Vision systems powered by AI can\ndetect defects and irregularities in\nFabrics with Incredible Precision\nensuring that only high quality products\nmake it to the market quality control is\nParamount in textiles and AI takes it to\na whole new level Machine Vision systems\npowered by AI have an uniring eye for\ndetail they meticulously inspect fabrics\nfor defects ensuring that only the\nhighest quality textiles make their way\nto Consumers this Precision reduces\nwaste and elevates product quality a\nwin-win for both manufacturers and\nconsumers seven sustainability and\ntextile waste reduction now let's talk\nabout sustainability the textile\nindustry has faced criticism for its\nenvironmental impact but AI can help\naddress this issue by optimizing\nproduction processes minimizing waste\nand even recycling material\nAI is contributing to a more sustainable\ntextile industry in the complex web of\ntextile production challenges inevitably\narise this is where ai's exceptional\nproblem-solving abilities Shine from\noptimizing production schedules to\nmitigating supply chain disruptions AI\nalgorithms are at the Forefront of\nresolving intricate issues they not only\nreact swiftly to problems but also\npredict and prevent them ensuring the\ntextile industry runs like a well-oiled\nmachine six impact and future so what's\nthe impact of AI in the textile industry\nit's substantial from design to\nproduction quality control Supply Chain\nmanagement and sustainability AI is\ntransforming every aspect of the\nindustry as technology continues to\nadvance we can only expect more\nInnovations and improvements in the\ntextile sector it's essential to\nunderstand that AI isn't replacing\nhumans in the textile industry it's\nenhancing human capabilities AI\ncollaborates with designers\nmanufacturers and quality control teams\naugmenting their skills and efficiency\nthis human AI Synergy results in a more\nInnovative efficient and responsive\ntextile industry than ever before five\nembracing change and challenges as with\nany Monumental shift embracing Ai and\ntextiles does present its unique set of\nchallenges from navigating data privacy\nconcerns to facilitating Workforce res\nSkilling the industry is traversing\nuncharted waters nevertheless it's\nimperative to recognize that the\npotential benefits vastly outweigh these\nchallenges four future Horizons gazing\ninto the Horizon we discern a textile\ntapestry woven with Aid driven\nInnovations the future promises AI\ngenerated Fabrics personalized textile\nexperiences and even more intelligent\nSupply chains the textile industry is in\nthe throws of evolution with AI as the\nthe shuttle crafting its\ntransformation three closing thoughts\nand Community engagement as we bring\nthis captivating journey through ai's\nimpact on the textile industry to a\nclose I invite you our wonderful\nCommunity to be part of the conversation\nwhat are your thoughts on ai's role in\nshaping the textile industry do you have\nspecific aspects you'd like us to\nexplore further please share your\ninvaluable insights in the comment\nsection below two datadriven\ndecisionmaking Predictive Analytics\ndriven by AI crunch vast data sets\nenabling informed product development\nDecisions by understanding market trends\nand consumer preferences businesses can\nminimize the risk of producing unwanted\nor unsellable\ntextiles one understanding Ai and\ntextiles before we unravel the\napplications let's thread the needle and\nunderstand what Ai and textiles truly\nmeans artificial intelligence or AI\nrefers to machines mimicking human\ncognitive functions in the textile\nsector AI involves using smart\nalgorithms and machines to perform tasks\nthat typically demand human intelligence\nsuch as pattern recognition decision-\nmaking and complex problem solving\nbefore we wrap up make sure to like this\nvideo if you found it informative and\ndon't forget to subscribe to our YouTube\nchannel for more exciting content on\ntechnology and its impact on various\nIndustries if you have any questions or\nwant to share your thoughts on AI and\ntextiles please please leave a comment\nbelow thanks for watching and I'll see\nyou in the next video"
    },
    {
        "topic": "Supervised Learning",
        "url": "https://www.youtube.com/watch?v=4dwsSz_fNSQ",
        "transcript": "Hello friends! Welcome to Gate Smashers.\nIn this video, we are going to discuss various types of learnings in AI.\nYou can also say it various types of machine learning.\nThe different types of learning we have in machine learning are\nsupervised, unsupervised, and reinforcement.\nThe difference between the three is asked many times.\nIn this video, I will explain this very simply with real-life examples\nlike the PK movie and the exit poll of elections,\non the basis of these examples, I am going to tell the difference between these three.\nAnd I am going to give their introduction.\nLet's start with supervised learning.\nWhen it comes to machine learning or artificial intelligence,\nour main aim is to make the machines intelligent.\nThe factor involved in making any machine intelligent\nis that it should learn by itself.\nSo, we have supervised, unsupervised, and reinforcement learnings.\nThese are the main three learnings!\nHowever, we have more different types of learning.\nWe are starting with supervised learning.\nI am giving you a basic technical introduction.\nIn supervised learning, we have training data.\nIt can be assessed by the name 'supervised'\nthat we have a supervisor.\nThe supervisor is referred to as a teacher who is giving instructions.\nWho's that? Training data.\nWe already have input as well as output available.\nOn the basis of that input and output, that training data,\nor on the basis of the labeled data, we create a model.\nAnd we put new input in that model\nand check whether a valid output is coming or not.\nWhat do I mean?\nLet me explain to you with a simple example.\nExit polls.\nYou must have seen that so many channels gave exit polls for this Loksabha election.\nWhat are the exit polls for the Loksabha election?\nIt's training data.\nThey took feedback from thousands and lakhs of people of different categories.\nThey took the feedback from the youngsters, kids, elder people, and everybody.\nThe feedback is their input.\nAnd there is an output on the basis of that input.\n\"BJP will win this many seats. Congress will win this many seats. And this party will win this many seats.\"\n\"He is the winning candidate. He is the losing candidate.\"\nThey have already put all these points into the training data.\nWe are working on that on the basis of the training data.\nWe have given the data to the machine.\nNow the learning algorithms will work.\nWhat are the learning algorithms?\nHere, we use the Naive Bayes algorithm.\nThe Naive Bayes works on supervised learning\nin which you have already put the input and the output.\nNow, what will happen?\nNow you gave a new input.\nWhen the actual counting started,\nthe original data of peoples' votes came out,\nand we put that data on the model.\nThe model was based on the data from the exit poll.\nThe output that came by creating dummy data and the basis of feedback,\nwe created the model on the basis of that.\n It's a prototype.\n Like \"He will win.\" \"He will lose.\" \"His winning probability is more than someone else.\"\nAs we entered the new data or the actual data,\nit gave an output on the basis of that model.\nNow, what's the output?\nIf the output is correct,\nthe results are exactly the same as the exit poll,\nthen the training data is very accurate and refined.\nAnd your algorithm is learning and classifying.\nClassification means classifying the data like\n\"He will win.\" \"He will lose.\" \"His probability of winning is more.\"\nThis way, we provide the input and the output to the machine.\nThis is supervised learning.\nIf we talk about unsupervised learning,\na simple example of that is in the PK movie.\nWhen Amir Khan came to Earth from some other planet,\nhe didn't know anything.\nHe was like a newborn baby.\nHe had no idea of right and wrong, male and female, child and adult, etc.\nHe didn't know anything.\nIf you have watched those scenes,\nyou must be knowing that he didn't know what the skin of people is like.\nSo, he said that the skin of some people is loose, while some people's skin is tight,\nand some people have colorful skin.\nHe didn't have any knowledge.\nHe just had input.\nInput is coming from the sensors.\nHe was seeing through the eyes and judging things,\nand on the basis of their physical structure, appearance, way of walking, etc.,\nhe is making clusters.\nWhat are clusters?\nHe saw that somebody has a mustache and is tall,\nor somebody has worn loose clothes like a kurta and pajama,\nhe categorized them as males.\nSimilarly, he created separate categories for ladies, players, kids, etc.\nThe category is not necessarily right.\nHe created groups/clusters on the first level.\nWe only have the input but not the output.\nHe doesn't know what this thing is.\nHe is learning gradually.\nHe is going toward the actual output by learning by himself.\nHere, we use the K-mean algorithm.\nIn K-Mean clustering,\nwe create groups and try to keep the same kind of people in the same group.\nMost of the machines have unsupervised learning.\nBecause machines don't have their own brain.\nSo, obviously, we need to supervise them.\nWe have to give them the data on the basis of which, they generate some output\nor make some decision.\nThe third is reinforcement learning.\nThe reinforcement learning\nis based on reward and policy.\nWhat reward and policy means is that\nwe have an agent.\nYou can assume anyone like Amir Khan of PK to be an agent.\nThat agent has performed some action.\nHe performed some actions in the environment.\nOn the basis of that action, either he got some reward which is positive\nor something negative that we can call a penalty.\nAnd its state has also changed.\nThe changes have been made in the environment.\nThen he created some policies on the basis of that\nand the next time, he will perform on the basis of that policy.\nYou must have seen that scene in which he gives a Rs.50 note and gets carrots in return.\nSo what did he learn here?\nHe learned that as he performed the action of giving a Rs.50 note,\nthe state changed and he got a bundle of carrots as a reward.\nHe created a policy to collect pictures of that person.\nSo he collected pictures of Mahatama Gandhi from everywhere\nand gave them to that person again.\nAs he gave them again, that person didn't recognize them.\nThen he understood that the value of this person's photograph is just on this paper and not on anything else.\nThis is an example of reinforcement\nwhere it creates a policy on the basis of penalties and rewards\nand learns while making changes in the policy.\nGenerally, this happens in games.\nYou took a step in a game and you got a point.\nWhether you got a positive point or a negative point,\non the basis of that, you will design a policy\nthat whether you should take a step similar to this or not.\nBasically, this is an example.\nWe always start with unsupervised learning\nwhere we have no knowledge but only input. We know nothing.\nHere, it creates clusters on the basis of itself.\nClusters may be anything based on their physical structure, color, appearance,\nit will create a cluster on the basis of that.\nThen it will refine those clusters gradually.\nIt may get feedback from more people.\nIt will gradually refine the clusters.\nOnce the cluster is created, as he saw a girl wearing a dress,\nhe will know that she's a girl.\nSo it will classify her as a girl.\nHe will get into supervised learning from unsupervised learning.\nThis is the main difference between supervised, unsupervised, and reinforcement.\nIn reinforcement, we generally use the Q-learning method.\nThere are so many algorithms.\nBut if you learn only these algorithms, you can implement them using Python.\nYou can also implement them using Matlab and R Programming.\nThank you!"
    },
    {
        "topic": "Linear Regression",
        "url": "https://www.youtube.com/watch?v=lzGKRSvs5HM",
        "transcript": "Error fetching transcript for video lzGKRSvs5HM: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=lzGKRSvs5HM! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
    },
    {
        "topic": "Logistic Regression",
        "url": "https://www.youtube.com/watch?v=yIYKR4sgzI8",
        "transcript": "If you can fit a line you can fit a squiggle if you can, make me laugh you can, make me giggle stat quest\nHello, i'm josh stormer and welcome to stat quest today we're going to talk about logistic regression\nThis is a technique that can be used for traditional statistics as, well as machine learning so let's get right to it\nBefore we dive into logistic regression let's take. A, step back and review, linear regression in\nAnother stat quest, we talked, about linear regression\nWe had some data\nWeight and size\nthen, we fit a line to it and\nWith that line, we could do a lot of things\nFirst we could calculate r-squared and determine if weight and size are\ncorrelated large values imply a large effect, and\nSecond calculate a p-value to determine if the r-squared value is statistically significant and\nThird, we could use the line, to predict, size given weight if a, new, mouse has this weight\nThen this, is the size that, we predict, from the weight although\nWe didn't mention it at the time using data to predict something falls under the category of machine learning\nSo plain old linear regression is a form of machine learning\nWe also talked a little bit about multiple regression\nNow, we are trying to predict, size, using weight and blood volume\nAlternatively we could, say that, we are trying to model size using weight and blood volume\nMultiple regression, did the same things that normal regression did\nwe calculated r-squared and\nwe calculated the p-value and\nWe could predict, size, using weight and blood volume and\nThis, makes multiple regression a slightly fancier machine learning method\nWe also talked, about how, we can use discrete measurements like genotype to predict size if you're\nNot familiar with the term genotype don't freak out it's. No, big deal just know that it refers to different types of mice\nlastly, we could compare models\nSo on the left side we've got normal regression, using weight to predict size and\nWe can, compare those predictions to the ones, we get from multiple regression, where we're using weight and blood volume to predict size\nComparing the simple model to the complicated one tells us if we need to measure weight and blood volume to accurately predict\nSize or if we can get, away, with just weight\nNow that we remember all the cool, things, we can, do with linear regression\nLet's talk, about logistic regression\nLogistic regression is similar to linear regression\nexcept\nLogistic regression predicts whether something, is true or false instead of predicting something continuous, like, sighs\nthese mice are obese and\nThese mice are not\nAlso instead of fitting a line to the data logistic regression fits an s-shaped logistic function\nThe curve goes from zero to one?\nAnd that, means that the curve tells you the probability that a mouse is obese based on its weight\nIf we weighed a very heavy mouse?\nThere is a high probability that the new, mouse is obese?\nIf we weighed an intermediate mouse\nThen there is only a 50% chance of the mouse is obese?\nLastly, there's only a small probability that a light mouse is obese\nAlthough, logistic regression, tells the probability that a mouse is obese or not it's usually used for classification\nFor example if the probability of mouse is obese is greater than 50%\nThen we'll classify it as obese\nOtherwise we'll classify it as not obese\nJust like with linear regression, we can, make simple models in this case, we can have obesity predicted, by weight or?\nmore complicated models in this case obesity is predicted by weight and genotype in\nThis, case, obesity is predicted. By weight and genotype and age and\nLastly, obesity is predicted by weight genotype, age and\nAstrological sign in other words just like linear regression logistic\nRegression can work with continuous data, like weight and age and discrete data like genotype and astrological sign\nWe can, also test to see if each variable is useful for predicting obesity\nhowever\nUnlike normal regression, we can't easily compare the complicated model to the simple model and we'll talk more about, why in a bit\nInstead we just test to see if a variables affect on the prediction is significantly different from zero\nIf not it, means that the variable is not helping the prediction\nWe use, wald's tests to figure this out we'll talk, about that in another stat quest in\nThis, case, the astrological sign is totes useless\nThat statistical jargon for not helping\nThat, means we can, save time and space in our study. By leaving it out\nLogistic regressions ability to provide probabilities and classify, new samples using continuous and discrete measurements\nMakes it a popular machine learning method\nOne big difference between linear regression and logistic regression is how the line is fit to the data\nWith linear regression, we fit the line, using least squares\nIn other words, we find the line that minimizes the sum of the squares of these residuals\nWe also use the residuals to calculate r. Squared and to compare simple models to complicated models\nLogistic regression doesn't have the same concept of a residual so it can't use least squares and it can't calculate r squared\ninstead it uses something called maximum likelihood\nThere's a whole stack quest on maximum likelihood so see that for details but in a nutshell\nYou, pick a probability scaled. By weight of observing an obese mouse just like this curve and\nYou, use that to calculate the likelihood of observing a, non obese mouse that weighs this much and\nthen you calculate the likelihood of observing, this mouse and\nyou, do that for all of the mice and\nLastly, you multiply all of those likelihoods together that's the likelihood of the data given this line\nthen you shift the line and calculate a new, likelihood of the data and\nthen ship the line and calculate the likelihood, again, and\nagain\nFinally the curve with the maximum value for the likelihood is selected bam\nin summary logistic regression can be used to classify samples and\nit can, use different types of data like, size and/or genotype to do that classification and\nit can, also be used to assess what variables are useful for classifying samples ie\nAstrological sign is totes useless\nHooray, we've made it to the end of another exciting stat quest do you, like this StackQuest, and want to see more please subscribe\nif you, have suggestions for future stat quests, well put them in the comments below, until next time quest on"
    },
    {
        "topic": "Decision Trees",
        "url": "https://www.youtube.com/watch?v=coOTEc-0OGw",
        "transcript": "Error fetching transcript for video coOTEc-0OGw: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=coOTEc-0OGw! This is most likely caused by:\n\nNo transcripts were found for any of the requested language codes: ('en',)\n\nFor this video (coOTEc-0OGw) transcripts are available in the following languages:\n\n(MANUALLY CREATED)\nNone\n\n(GENERATED)\n - hi (\"Hindi (auto-generated)\")[TRANSLATABLE]\n\n(TRANSLATION LANGUAGES)\n - ab (\"Abkhazian\")\n - aa (\"Afar\")\n - af (\"Afrikaans\")\n - ak (\"Akan\")\n - sq (\"Albanian\")\n - am (\"Amharic\")\n - ar (\"Arabic\")\n - hy (\"Armenian\")\n - as (\"Assamese\")\n - ay (\"Aymara\")\n - az (\"Azerbaijani\")\n - bn (\"Bangla\")\n - ba (\"Bashkir\")\n - eu (\"Basque\")\n - be (\"Belarusian\")\n - bho (\"Bhojpuri\")\n - bs (\"Bosnian\")\n - br (\"Breton\")\n - bg (\"Bulgarian\")\n - my (\"Burmese\")\n - ca (\"Catalan\")\n - ceb (\"Cebuano\")\n - zh-Hans (\"Chinese (Simplified)\")\n - zh-Hant (\"Chinese (Traditional)\")\n - co (\"Corsican\")\n - hr (\"Croatian\")\n - cs (\"Czech\")\n - da (\"Danish\")\n - dv (\"Divehi\")\n - nl (\"Dutch\")\n - dz (\"Dzongkha\")\n - en (\"English\")\n - eo (\"Esperanto\")\n - et (\"Estonian\")\n - ee (\"Ewe\")\n - fo (\"Faroese\")\n - fj (\"Fijian\")\n - fil (\"Filipino\")\n - fi (\"Finnish\")\n - fr (\"French\")\n - gaa (\"Ga\")\n - gl (\"Galician\")\n - lg (\"Ganda\")\n - ka (\"Georgian\")\n - de (\"German\")\n - el (\"Greek\")\n - gn (\"Guarani\")\n - gu (\"Gujarati\")\n - ht (\"Haitian Creole\")\n - ha (\"Hausa\")\n - haw (\"Hawaiian\")\n - iw (\"Hebrew\")\n - hi (\"Hindi\")\n - hmn (\"Hmong\")\n - hu (\"Hungarian\")\n - is (\"Icelandic\")\n - ig (\"Igbo\")\n - id (\"Indonesian\")\n - iu (\"Inuktitut\")\n - ga (\"Irish\")\n - it (\"Italian\")\n - ja (\"Japanese\")\n - jv (\"Javanese\")\n - kl (\"Kalaallisut\")\n - kn (\"Kannada\")\n - kk (\"Kazakh\")\n - kha (\"Khasi\")\n - km (\"Khmer\")\n - rw (\"Kinyarwanda\")\n - ko (\"Korean\")\n - kri (\"Krio\")\n - ku (\"Kurdish\")\n - ky (\"Kyrgyz\")\n - lo (\"Lao\")\n - la (\"Latin\")\n - lv (\"Latvian\")\n - ln (\"Lingala\")\n - lt (\"Lithuanian\")\n - lua (\"Luba-Lulua\")\n - luo (\"Luo\")\n - lb (\"Luxembourgish\")\n - mk (\"Macedonian\")\n - mg (\"Malagasy\")\n - ms (\"Malay\")\n - ml (\"Malayalam\")\n - mt (\"Maltese\")\n - gv (\"Manx\")\n - mi (\"M\u0101ori\")\n - mr (\"Marathi\")\n - mn (\"Mongolian\")\n - mfe (\"Morisyen\")\n - ne (\"Nepali\")\n - new (\"Newari\")\n - nso (\"Northern Sotho\")\n - no (\"Norwegian\")\n - ny (\"Nyanja\")\n - oc (\"Occitan\")\n - or (\"Odia\")\n - om (\"Oromo\")\n - os (\"Ossetic\")\n - pam (\"Pampanga\")\n - ps (\"Pashto\")\n - fa (\"Persian\")\n - pl (\"Polish\")\n - pt (\"Portuguese\")\n - pt-PT (\"Portuguese (Portugal)\")\n - pa (\"Punjabi\")\n - qu (\"Quechua\")\n - ro (\"Romanian\")\n - rn (\"Rundi\")\n - ru (\"Russian\")\n - sm (\"Samoan\")\n - sg (\"Sango\")\n - sa (\"Sanskrit\")\n - gd (\"Scottish Gaelic\")\n - sr (\"Serbian\")\n - crs (\"Seselwa Creole French\")\n - sn (\"Shona\")\n - sd (\"Sindhi\")\n - si (\"Sinhala\")\n - sk (\"Slovak\")\n - sl (\"Slovenian\")\n - so (\"Somali\")\n - st (\"Southern Sotho\")\n - es (\"Spanish\")\n - su (\"Sundanese\")\n - sw (\"Swahili\")\n - ss (\"Swati\")\n - sv (\"Swedish\")\n - tg (\"Tajik\")\n - ta (\"Tamil\")\n - tt (\"Tatar\")\n - te (\"Telugu\")\n - th (\"Thai\")\n - bo (\"Tibetan\")\n - ti (\"Tigrinya\")\n - to (\"Tongan\")\n - ts (\"Tsonga\")\n - tn (\"Tswana\")\n - tum (\"Tumbuka\")\n - tr (\"Turkish\")\n - tk (\"Turkmen\")\n - uk (\"Ukrainian\")\n - ur (\"Urdu\")\n - ug (\"Uyghur\")\n - uz (\"Uzbek\")\n - ve (\"Venda\")\n - vi (\"Vietnamese\")\n - war (\"Waray\")\n - cy (\"Welsh\")\n - fy (\"Western Frisian\")\n - wo (\"Wolof\")\n - xh (\"Xhosa\")\n - yi (\"Yiddish\")\n - yo (\"Yoruba\")\n - zu (\"Zulu\")\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
    },
    {
        "topic": "Unsupervised Learning",
        "url": "https://www.youtube.com/watch?v=4dwsSz_fNSQ",
        "transcript": "Hello friends! Welcome to Gate Smashers.\nIn this video, we are going to discuss various types of learnings in AI.\nYou can also say it various types of machine learning.\nThe different types of learning we have in machine learning are\nsupervised, unsupervised, and reinforcement.\nThe difference between the three is asked many times.\nIn this video, I will explain this very simply with real-life examples\nlike the PK movie and the exit poll of elections,\non the basis of these examples, I am going to tell the difference between these three.\nAnd I am going to give their introduction.\nLet's start with supervised learning.\nWhen it comes to machine learning or artificial intelligence,\nour main aim is to make the machines intelligent.\nThe factor involved in making any machine intelligent\nis that it should learn by itself.\nSo, we have supervised, unsupervised, and reinforcement learnings.\nThese are the main three learnings!\nHowever, we have more different types of learning.\nWe are starting with supervised learning.\nI am giving you a basic technical introduction.\nIn supervised learning, we have training data.\nIt can be assessed by the name 'supervised'\nthat we have a supervisor.\nThe supervisor is referred to as a teacher who is giving instructions.\nWho's that? Training data.\nWe already have input as well as output available.\nOn the basis of that input and output, that training data,\nor on the basis of the labeled data, we create a model.\nAnd we put new input in that model\nand check whether a valid output is coming or not.\nWhat do I mean?\nLet me explain to you with a simple example.\nExit polls.\nYou must have seen that so many channels gave exit polls for this Loksabha election.\nWhat are the exit polls for the Loksabha election?\nIt's training data.\nThey took feedback from thousands and lakhs of people of different categories.\nThey took the feedback from the youngsters, kids, elder people, and everybody.\nThe feedback is their input.\nAnd there is an output on the basis of that input.\n\"BJP will win this many seats. Congress will win this many seats. And this party will win this many seats.\"\n\"He is the winning candidate. He is the losing candidate.\"\nThey have already put all these points into the training data.\nWe are working on that on the basis of the training data.\nWe have given the data to the machine.\nNow the learning algorithms will work.\nWhat are the learning algorithms?\nHere, we use the Naive Bayes algorithm.\nThe Naive Bayes works on supervised learning\nin which you have already put the input and the output.\nNow, what will happen?\nNow you gave a new input.\nWhen the actual counting started,\nthe original data of peoples' votes came out,\nand we put that data on the model.\nThe model was based on the data from the exit poll.\nThe output that came by creating dummy data and the basis of feedback,\nwe created the model on the basis of that.\n It's a prototype.\n Like \"He will win.\" \"He will lose.\" \"His winning probability is more than someone else.\"\nAs we entered the new data or the actual data,\nit gave an output on the basis of that model.\nNow, what's the output?\nIf the output is correct,\nthe results are exactly the same as the exit poll,\nthen the training data is very accurate and refined.\nAnd your algorithm is learning and classifying.\nClassification means classifying the data like\n\"He will win.\" \"He will lose.\" \"His probability of winning is more.\"\nThis way, we provide the input and the output to the machine.\nThis is supervised learning.\nIf we talk about unsupervised learning,\na simple example of that is in the PK movie.\nWhen Amir Khan came to Earth from some other planet,\nhe didn't know anything.\nHe was like a newborn baby.\nHe had no idea of right and wrong, male and female, child and adult, etc.\nHe didn't know anything.\nIf you have watched those scenes,\nyou must be knowing that he didn't know what the skin of people is like.\nSo, he said that the skin of some people is loose, while some people's skin is tight,\nand some people have colorful skin.\nHe didn't have any knowledge.\nHe just had input.\nInput is coming from the sensors.\nHe was seeing through the eyes and judging things,\nand on the basis of their physical structure, appearance, way of walking, etc.,\nhe is making clusters.\nWhat are clusters?\nHe saw that somebody has a mustache and is tall,\nor somebody has worn loose clothes like a kurta and pajama,\nhe categorized them as males.\nSimilarly, he created separate categories for ladies, players, kids, etc.\nThe category is not necessarily right.\nHe created groups/clusters on the first level.\nWe only have the input but not the output.\nHe doesn't know what this thing is.\nHe is learning gradually.\nHe is going toward the actual output by learning by himself.\nHere, we use the K-mean algorithm.\nIn K-Mean clustering,\nwe create groups and try to keep the same kind of people in the same group.\nMost of the machines have unsupervised learning.\nBecause machines don't have their own brain.\nSo, obviously, we need to supervise them.\nWe have to give them the data on the basis of which, they generate some output\nor make some decision.\nThe third is reinforcement learning.\nThe reinforcement learning\nis based on reward and policy.\nWhat reward and policy means is that\nwe have an agent.\nYou can assume anyone like Amir Khan of PK to be an agent.\nThat agent has performed some action.\nHe performed some actions in the environment.\nOn the basis of that action, either he got some reward which is positive\nor something negative that we can call a penalty.\nAnd its state has also changed.\nThe changes have been made in the environment.\nThen he created some policies on the basis of that\nand the next time, he will perform on the basis of that policy.\nYou must have seen that scene in which he gives a Rs.50 note and gets carrots in return.\nSo what did he learn here?\nHe learned that as he performed the action of giving a Rs.50 note,\nthe state changed and he got a bundle of carrots as a reward.\nHe created a policy to collect pictures of that person.\nSo he collected pictures of Mahatama Gandhi from everywhere\nand gave them to that person again.\nAs he gave them again, that person didn't recognize them.\nThen he understood that the value of this person's photograph is just on this paper and not on anything else.\nThis is an example of reinforcement\nwhere it creates a policy on the basis of penalties and rewards\nand learns while making changes in the policy.\nGenerally, this happens in games.\nYou took a step in a game and you got a point.\nWhether you got a positive point or a negative point,\non the basis of that, you will design a policy\nthat whether you should take a step similar to this or not.\nBasically, this is an example.\nWe always start with unsupervised learning\nwhere we have no knowledge but only input. We know nothing.\nHere, it creates clusters on the basis of itself.\nClusters may be anything based on their physical structure, color, appearance,\nit will create a cluster on the basis of that.\nThen it will refine those clusters gradually.\nIt may get feedback from more people.\nIt will gradually refine the clusters.\nOnce the cluster is created, as he saw a girl wearing a dress,\nhe will know that she's a girl.\nSo it will classify her as a girl.\nHe will get into supervised learning from unsupervised learning.\nThis is the main difference between supervised, unsupervised, and reinforcement.\nIn reinforcement, we generally use the Q-learning method.\nThere are so many algorithms.\nBut if you learn only these algorithms, you can implement them using Python.\nYou can also implement them using Matlab and R Programming.\nThank you!"
    },
    {
        "topic": "Clustering",
        "url": "https://www.youtube.com/watch?v=4b5d3muPQmA",
        "transcript": "statcast\n[Music]\nstat quest stat quest stat quest hello\nI'm Josh stormer and welcome to stat\nquest today we're going to be talking\nabout k-means clustering we're gonna\nlearn how to cluster samples that can be\nput on a line on an XY graph and even on\na heat map and lastly we'll also talk\nabout how to pick the best value for K\nimagine you had some data that you could\nplot on a line and you knew you needed\nto put it into three clusters maybe they\nare measurements from three different\ntypes of tumors or other cell types in\nthis case the data make three relatively\nobvious clusters but rather than rely on\nour eye let's see if we can get a\ncomputer to identify the same three\nclusters to do this we'll use k-means\nclustering we'll start with raw data\nthat we haven't yet clustered step one\nselect the number of clusters you want\nto identify in your data this is the K\nin k-means clustering in this case we'll\nselect K equals three that is to say we\nwant to identify three clusters there is\na fancier way to select a value for K\nbut we'll talk about that later\nstep two randomly select three distinct\ndata points these are the initial\nclusters\nstep 3 measure the distance between the\nfirst point and the three initial\nclusters this is the distance from the\nfirst point to the blue cluster this is\nthe distance from the first point to the\ngreen cluster\nand this is the distance from the first\npoint to the orange cluster well it's\nkind of yellow but we'll just call it\norange for now step 4 assign the first\npoint to the nearest cluster in this\ncase the nearest cluster is the blue\ncluster now we do the same thing for the\nnext point we measure the distances and\nthen assign the point to the nearest\ncluster now we figure out which cluster\nthe third point belongs to we measure\nthe distances and then assign the point\nto the nearest cluster the rest of these\npoints are closest to the orange cluster\nso they'll go in that one two now that\nall the points are in clusters we go on\nto step 5 calculate the mean of each\ncluster then we repeat what we just did\nmeasure and cluster using the mean\nvalues\nsince the clustering did not change at\nall during the last iteration were done\nBAM the k-means clustering is pretty\nterrible compared to what we did by eye\nwe can assess the quality of the\nclustering by adding up the variation\nwithin each cluster here's the total\nvariation within the clusters since\nk-means clustering can't see the best\nclustering it's only option is to keep\ntrack of these clusters and their total\nvariance and do the whole thing over\nagain with different starting points so\nhere we are again back at the beginning\nk-means clustering picks three initial\nclusters and then clusters all the\nremaining points calculates the mean of\neach cluster and then re clusters based\non the new means it repeats until the\ncluster is no longer change bit bit bit\nof bit of boop boop boop now that the\ndata are clustered we sum the variation\nwithin each cluster\nand then we do it all again\nat this point k-means clustering knows\nthat the second clustering is the best\nclustering so far but it doesn't know if\nit's the best overall so it will do a\nfew more clusters it does as many as you\ntell it to do and then come back and\nreturn that one if it is still the best\nquestion how do you figure out what\nvalue to use for K with this data it's\nobvious that we should set K to three\nbut other times it is not so clear one\nway to decide is to just try different\nvalues for K\nwe'll start with k equals 1 k equals 1\nis the worst case scenario we can\nquantify its badness with the total\nvariation now try K equals 2 K equals 2\nis better and we can quantify how much\nbetter by comparing the total variation\nwithin the two clusters to K equals 1\nnow try K equals 3 k equals 3 is even\nbetter we can quantify how much better\nby comparing the total variation within\nthe three clusters to k equals 2 now try\nk equals 4 the total variation within\neach cluster is less than when K equals\n3 each time we add a new cluster the\ntotal variation within each cluster is\nsmaller than before and when there is\nonly one point per cluster the variation\nequals 0 however if we plot the\nreduction in variance per value for K\nthere is a huge reduction in variation\nwith K equals three but after that the\nvariation doesn't go down as quickly\nthis is called an elbow plot and you can\npick K by finding the elbow in the plot\nquestion how is k-means clustering\ndifferent from hierarchical clustering\nk-means clustering specifically tries to\nput the data into the number of clusters\nyou tell it to hierarchical clustering\njust tells you pairwise what two things\nare most similar question what if our\ndata isn't plotted on a number line just\nlike before you pick three random points\nand we use the Euclidean distance in two\ndimensions the Euclidean distance is the\nsame thing as the Pythagorean theorem\nthen just like before we assign the\npoint to the nearest cluster and just\nlike before we then calculate the center\nof each cluster and re cluster BAM\nalthough this looks good the computer\ndoesn't know that until it does the\nclustering a few more times question\nwhat if my data is a heatmap well if we\njust have two samples we can rename them\nx and y and we can then plot the data in\nan XY graph then we can cluster just\nlike before note we don't actually need\nto plot the data in order to cluster it\nwe just need to calculate the distances\nbetween things when we have two samples\nor two axes the Euclidean distance is\nthe square root of x squared plus y\nsquared when we have three samples or\nthree axes the Euclidean distance is the\nsquare root of x squared plus y squared\nplus Z squared and when we have four\nsamples or four axes the Euclidean\ndistance is the square root of x squared\nplus y squared plus Z squared plus a\nsquared etc etc etc hooray\nwe've made it to the end of another\nexciting stat quest if you like this\nstat quest and want to see more please\nsubscribe and if you want to support\nstack quest well click the like button\ndown below and consider buying one or\ntwo of my original songs alright tune in\nnext time for another exciting stat\nquest"
    },
    {
        "topic": "Dimensionality Reduction",
        "url": "https://www.youtube.com/watch?v=SMaa3pnQmbg",
        "transcript": "Error fetching transcript for video SMaa3pnQmbg: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=SMaa3pnQmbg! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
    },
    {
        "topic": "Reinforcement Learning",
        "url": "https://www.youtube.com/watch?v=4dwsSz_fNSQ",
        "transcript": "Hello friends! Welcome to Gate Smashers.\nIn this video, we are going to discuss various types of learnings in AI.\nYou can also say it various types of machine learning.\nThe different types of learning we have in machine learning are\nsupervised, unsupervised, and reinforcement.\nThe difference between the three is asked many times.\nIn this video, I will explain this very simply with real-life examples\nlike the PK movie and the exit poll of elections,\non the basis of these examples, I am going to tell the difference between these three.\nAnd I am going to give their introduction.\nLet's start with supervised learning.\nWhen it comes to machine learning or artificial intelligence,\nour main aim is to make the machines intelligent.\nThe factor involved in making any machine intelligent\nis that it should learn by itself.\nSo, we have supervised, unsupervised, and reinforcement learnings.\nThese are the main three learnings!\nHowever, we have more different types of learning.\nWe are starting with supervised learning.\nI am giving you a basic technical introduction.\nIn supervised learning, we have training data.\nIt can be assessed by the name 'supervised'\nthat we have a supervisor.\nThe supervisor is referred to as a teacher who is giving instructions.\nWho's that? Training data.\nWe already have input as well as output available.\nOn the basis of that input and output, that training data,\nor on the basis of the labeled data, we create a model.\nAnd we put new input in that model\nand check whether a valid output is coming or not.\nWhat do I mean?\nLet me explain to you with a simple example.\nExit polls.\nYou must have seen that so many channels gave exit polls for this Loksabha election.\nWhat are the exit polls for the Loksabha election?\nIt's training data.\nThey took feedback from thousands and lakhs of people of different categories.\nThey took the feedback from the youngsters, kids, elder people, and everybody.\nThe feedback is their input.\nAnd there is an output on the basis of that input.\n\"BJP will win this many seats. Congress will win this many seats. And this party will win this many seats.\"\n\"He is the winning candidate. He is the losing candidate.\"\nThey have already put all these points into the training data.\nWe are working on that on the basis of the training data.\nWe have given the data to the machine.\nNow the learning algorithms will work.\nWhat are the learning algorithms?\nHere, we use the Naive Bayes algorithm.\nThe Naive Bayes works on supervised learning\nin which you have already put the input and the output.\nNow, what will happen?\nNow you gave a new input.\nWhen the actual counting started,\nthe original data of peoples' votes came out,\nand we put that data on the model.\nThe model was based on the data from the exit poll.\nThe output that came by creating dummy data and the basis of feedback,\nwe created the model on the basis of that.\n It's a prototype.\n Like \"He will win.\" \"He will lose.\" \"His winning probability is more than someone else.\"\nAs we entered the new data or the actual data,\nit gave an output on the basis of that model.\nNow, what's the output?\nIf the output is correct,\nthe results are exactly the same as the exit poll,\nthen the training data is very accurate and refined.\nAnd your algorithm is learning and classifying.\nClassification means classifying the data like\n\"He will win.\" \"He will lose.\" \"His probability of winning is more.\"\nThis way, we provide the input and the output to the machine.\nThis is supervised learning.\nIf we talk about unsupervised learning,\na simple example of that is in the PK movie.\nWhen Amir Khan came to Earth from some other planet,\nhe didn't know anything.\nHe was like a newborn baby.\nHe had no idea of right and wrong, male and female, child and adult, etc.\nHe didn't know anything.\nIf you have watched those scenes,\nyou must be knowing that he didn't know what the skin of people is like.\nSo, he said that the skin of some people is loose, while some people's skin is tight,\nand some people have colorful skin.\nHe didn't have any knowledge.\nHe just had input.\nInput is coming from the sensors.\nHe was seeing through the eyes and judging things,\nand on the basis of their physical structure, appearance, way of walking, etc.,\nhe is making clusters.\nWhat are clusters?\nHe saw that somebody has a mustache and is tall,\nor somebody has worn loose clothes like a kurta and pajama,\nhe categorized them as males.\nSimilarly, he created separate categories for ladies, players, kids, etc.\nThe category is not necessarily right.\nHe created groups/clusters on the first level.\nWe only have the input but not the output.\nHe doesn't know what this thing is.\nHe is learning gradually.\nHe is going toward the actual output by learning by himself.\nHere, we use the K-mean algorithm.\nIn K-Mean clustering,\nwe create groups and try to keep the same kind of people in the same group.\nMost of the machines have unsupervised learning.\nBecause machines don't have their own brain.\nSo, obviously, we need to supervise them.\nWe have to give them the data on the basis of which, they generate some output\nor make some decision.\nThe third is reinforcement learning.\nThe reinforcement learning\nis based on reward and policy.\nWhat reward and policy means is that\nwe have an agent.\nYou can assume anyone like Amir Khan of PK to be an agent.\nThat agent has performed some action.\nHe performed some actions in the environment.\nOn the basis of that action, either he got some reward which is positive\nor something negative that we can call a penalty.\nAnd its state has also changed.\nThe changes have been made in the environment.\nThen he created some policies on the basis of that\nand the next time, he will perform on the basis of that policy.\nYou must have seen that scene in which he gives a Rs.50 note and gets carrots in return.\nSo what did he learn here?\nHe learned that as he performed the action of giving a Rs.50 note,\nthe state changed and he got a bundle of carrots as a reward.\nHe created a policy to collect pictures of that person.\nSo he collected pictures of Mahatama Gandhi from everywhere\nand gave them to that person again.\nAs he gave them again, that person didn't recognize them.\nThen he understood that the value of this person's photograph is just on this paper and not on anything else.\nThis is an example of reinforcement\nwhere it creates a policy on the basis of penalties and rewards\nand learns while making changes in the policy.\nGenerally, this happens in games.\nYou took a step in a game and you got a point.\nWhether you got a positive point or a negative point,\non the basis of that, you will design a policy\nthat whether you should take a step similar to this or not.\nBasically, this is an example.\nWe always start with unsupervised learning\nwhere we have no knowledge but only input. We know nothing.\nHere, it creates clusters on the basis of itself.\nClusters may be anything based on their physical structure, color, appearance,\nit will create a cluster on the basis of that.\nThen it will refine those clusters gradually.\nIt may get feedback from more people.\nIt will gradually refine the clusters.\nOnce the cluster is created, as he saw a girl wearing a dress,\nhe will know that she's a girl.\nSo it will classify her as a girl.\nHe will get into supervised learning from unsupervised learning.\nThis is the main difference between supervised, unsupervised, and reinforcement.\nIn reinforcement, we generally use the Q-learning method.\nThere are so many algorithms.\nBut if you learn only these algorithms, you can implement them using Python.\nYou can also implement them using Matlab and R Programming.\nThank you!"
    },
    {
        "topic": "Markov Decision Processes",
        "url": "https://www.youtube.com/watch?v=lfHX2hHRMVQ",
        "transcript": "okay so rough outline for today's class\nwill be we'll start off by sort of\nlayering on more and more complexity\nwe'll start off with a basic idea um\nmarkof process or a markof chain for\nthose of you who might come across this\nbefore um and then we'll start to\nintroduce some of the um perhaps less\nfamiliar ideas which are essential to\nreinforcement learning first of all by\nadding rewards in to give a mark of\nreward process then by adding actions in\nto give a markup decision process and\nfinally we um probably won't have time\nto actually talk about them but they're\nin the lecture notes for those of you\nwho are interested there's a lot of\nextensions to mdps where we can move\nBeyond these three settings and add in\neven more complexity like partial\nobservability and so\nforth so begin with the basics I'll try\nnot to kill myself today on um any\nstage so the basic idea then is we're\ngoing to develop this formalism the mark\nof decision process and what we're\ntrying to cover here is if you remember\nin the last class we talked about agents\nand environments we have an agent that's\nour algorithm um and that's the brain\nthat we're trying to build and it's\ninteracting with some worlds some this\nmight be the the real world for a robot\nit might be you know the trading\nenvironment for a trading agent the\nfactory floor for a a factory whatever\nand this environment we want some\ndescription of that environment such\nthat we can understand it start to apply\nsome um some tools to it and really\nunderstand what it means to do\nreinforcement learning in that setting\nand that's going to be the\nmdp um and in this case what we're going\nto start with is the case the nice case\nwhere the environment is fully\nobservable so we're told the state um so\nthis is the case where we see everything\nthat there is to know about that\nenvironment all the relevant information\nis presented to our agent nothing is\nhidden\naway um and one way to think of this is\nthat the current states that it's given\nto the agent it sort of completely\ncharacterizes this process so the way in\nwhich the environment unfolds depends on\nsome State and we're told that state we\nknow that state it's fully\nobserved um and the nice thing about\nthis formalism is that really almost all\nreinforcement learning problems can be\nformalized in some way as a mark of\ndecision process so if think about even\nsome of these less familiar cases for\nthose of you who come across them these\nare things which people might not first\nof all think of as Market Precision\nprocesses but the problem of optimal\ncontrol where you've got say some\ndifferential Dynamics describing some\nfluid and you want to find the optimal\nway to make your octopus swim through\nthis fluid how do we actually deal with\nthat well there's an extension to mdps\nthat's actually a continuous markof\ndecision process with continuous actions\nand optimal control actually really\ndeals with that\ncase again partially observable problems\nnot only can we think of mdps to give us\nsome intuition into them but actually\nany partially observed problem can be\ncompletely converted into an mdp so it's\nnot the case that these things fall\noutside of the framework of mdps they\nare mdps they really are mdps and if you\nunderstand the basics you can solve all\nof these problems in in\nprinciple um some of the simplest cases\nwhich we'll deal with in a later class\nwhen we talk about the exploration\nexploitation dilemma in reinforcement\nlearning are what's known as Bandits\nthis is a a very common formalism that's\nused a lot at the moment where you get\njust a set of actions you get to take an\naction you get some reward for that\naction and that's the end of your task\nso this is like the task we talked about\nlast week where you present an advert to\nthe user on the internet and that advert\neither gets clicked on or it doesn't and\nyou want to be clicked on as much as\npossible and actually this is just a\nmarkup decision process with only one\nstate so again it's a special case of\nmdps if we can solve mdps we can solve\nall of these different cases so it's\nreally\nfundamental so what's the central idea\nto an mdp well we've already seen this\nin the last class but it's such an\nimportant concept I'm just going to\nflash up this slide again uh which is\nthe mark of property um and again the\nmark of property the central idea which\nwe're trying to understand here is this\nidea that you have some State this um\nrandom variable s this is characterizing\nwhere we are in in in our environment\nand if it has the mark of property then\nthis basically tells us that the future\nis independent of the past given the\npresent in other words what happens next\nin our environment depends only on our\nprevious state and not not on all the\nthings which came before that okay so\nthe state completely characterizes\neverything which we need to know it\ncaptures all the relevant information\nfrom the history um and once you know\nthis thing you can essentially if you\nhave St what this tells us is you can\nthrow away everything which came before\nso the set of Mark or processes or the\nset of states that have this property\nit's the set of um it's the set of\nenvironments in which everything which\ncan be uh which we know about the system\ncan be described by this this single\nstate um at the current time and we\ndon't need to retain everything that\nwe've ever\nseen\nokay um so one way to understand this is\nthat uh for any problem with the markof\nuh property for any markof process if\nyou start in some State s and you've got\nsome successor State S Prime then you\ncan actually Define the probability that\nyou'll transition from one of those\nstates to the next state so given\nremember that this state which we were\nin characterizes everything about what\nwill happen next so that must mean that\nthere's some well defined transition\nprobability that tells me that if I was\nin this state before there's some\nprobability given that I was in that\nstate that I will transition to some\nnext state there's some probability that\nyou know if my robot is here and I give\nit a little push then it will you know\nfall over to here or maybe put a foot\nforward all of these things are\ncompletely characterized by the state it\nwas in\nbefore okay um so this gives us our\nstate transition probabilities so it's\nthe probability that if I'm in s i\ntransition to S Prime and that gives us\nthe probability this random next state\num will be this particular instantiation\nof the state S Prime given that the\nprevious state we were in our current\nstate at time T was was this s now once\nwe have this idea of the transition\nprobability Matrix um we can we can form\nthis into a complete Matrix that tells\nus essentially each row of this Matrix\ntells us what would happen uh for each\nstate that I was in so if I start in\nstate one this tells me the probability\nthat I'll end up in state one here state\ntwo here state three here all the way up\nto end States here so each row of this\ncompletely characterizes the transitions\nfrom one possible starting place in this\nin this Mark process so this single\nMatrix now gives me the complete\nstructure to this markof problem tells\nme from any state How likely am I to end\nup in any other\nstate so you can imagine now that we can\nfollow this through multiple steps and\nkeep sampling from this transition\nprobabilities and that will give us um\nsome draws from this Mark of\nprocess so let's try and understand that\na little bit more so now just formally a\nmark of process is basically a random\nprocess a process that we're sort of\nsampling from iteratively um so that\nbasically our definition for this is\ngoing to be it's a sequence of random\nStates so sequence of random States S1\nS2 going on um in time that has the mark\nof property that's the definition of\nMark of process it's just a sequence of\nstates with the mark of property um and\nso all it requires to Define this thing\nis a state space s this is the set of\nstates which we can be\nand our transition probabilities that\ncharacterize how we transition from one\nstate to the next okay and now this\nfully defines the Dynamics of this whole\nsystem there's some evolution of our of\nour robot through this system or of our\ntrading agent through its environment or\nof our chess playing Agent through the\nrules of the game all of these things\ncan be defined um no actions yet no\nrewards yet but they the Dynamics can be\nfully defined by a state space and a\ntransition probability\nMatrix okay that's the mark of process\nso I think it always helps to to make\nthese things concrete so let's try and\nmake this concrete for you guys um so so\nthis is my cartoon illustration of um uh\nwhat I imagine it's like to be sitting\nin my lectures um and so we're just\ngoing to imagine there's only three\nclasses you have to sit through um if\nyou get through all three classes then D\nyou manage to pass the class if only it\nwas that easy but anyway um but the\nproblem is there's all these\ndistractions along the way so you start\noff in the first class and there's a 50%\nprobability you'll make it through to\nthe next class there also a 50%\nprobability that you guys will open up\nyour laptops start looking at Facebook\num now if you look at Facebook we all\nknow that you know once you're in\nFacebook it's a little bit addictive so\nthere's probably about a 0.9 probability\nthat you'll just kind of self transition\nand keep looking at Facebook keep\nlooking at Facebook again and then maybe\nafter you iterate around this thing for\na few times there's a a01 probability\nthat you'll actually drop out of this\nFacebook and get back to the class and\nlisten to the the rest of the class\nwhich point you might make it to the\nsecond class un fortunately you know\nmaybe we're talking about something\nreally tedious like markof processes and\nyou feel a bit sleepy and so there's\nmaybe a 20% probability you fall asleep\num but there's maybe a an 80%\nprobability you make it through to the\nfinal class um at which point uh maybe\nyou think yeah I'm doing really well and\num and you get a bit excited and go off\nto the pub um which means if you go to\nthe pub you drink a few beers um you\nmight end up either regressing back to\nclass one or class two because of all\nthe things you forget um or um if you\nyou don't go to the pub there's a 60%\nchance that you'll pass and then um\nafter that you can go to sleep and\neveryone's happy okay and this sleep\nState represents a terminal state to the\nmarkof process a terminal State doesn't\nneed any special Machinery you can think\nof this as just a self Loop an absorbing\nterminal state it just keeps looping\nround and round and round that's what\nthe square represents by the way there's\na few more seats over here I think if\nanyone wants to um just move around if\nyou're okay that's\nfine okay so now that we've got this\nstudent markof chain let's let's think\nabout what it means to take samples of\nthis so what's a sample a sample is a\nsequence it's a sequence of states where\nwe start for example in our class one um\nand now we're just going to take samples\nthrough this system for example uh one\nsample might look like this where we go\nclass one class two class three pass\nfall asleep okay that's the nice sample\num but you might also have another\nsample that's like class one Facebook\nFacebook again that's to class one\nthat's to class two and then fall asleep\num you might have more complicated um\nsamples they some of them might go on\nand on for quite a long time these are\nvariable length each of these you could\nthink of as a random sequence that's\nsampled from these Dynamics okay so\nthat's what it means to have a random\nprocess is that you get some random\nsequence um that's drawn from a\nprobability distribution over sequences\nof states and the fact it has the mark\nof property means that it can be\ndescribed by one of these diagrams if\nyou like it can be described by saying\nfrom any state there's some probability\nof transitioning to any other state is\nthat clear\ngood okay and now we can look at the\ntransition Matrix for this problem so\nthe transition Matrix is fully described\nnow over here the transition Matrix\nbasically tells us that any one of these\nstates which we might have been in\nwhat's the probability of transitioning\nto any other one of these states so for\nexample if we consider class two here\nand we look across this row here this\nrow of the transition Matrix basically\ntells us there's A8 probability and 80%\nchance\nof transitioning to class three um but\nif we go across there's also a 20%\nchance of transitioning to The Sleep\nState and if we look at all of the rows\ntogether that fully describes the entire\ndynamics of the system and once we have\nthis Matrix we're able to sample\nrepeatedly from this Matrix and get our\nsamples of these sequence of states\nafter this Mark of\nprocess okay question so these things\nwill only work if the probability is\ngoing to stay constant at each state how\ndo you make it such that if you were to\nsay go to Facebook then you have a\nreducing probability each time okay so\nthe question is how do we deal with um\nmodifications of these probabilities\nover time so so there's two answers to\nthat um one answer is that you can have\na non-stationary Markoff process Mark\nlater we'll have mdp so non-stationary\nmdp\num in that case what you can do is use\nthe same kind of algorithms we use for\nthe stationary case but incrementally\nadjust your solution algorithm to just\nkind of track the best solution you\nfound so far\num the other answer is to say well the\nfact you've got non-stationary Dynamics\njust makes it a more complicated markup\nprocess that there's some um so you're\nimagining that there's probabilities\nthat depend on how long you've been in\nthis state and so forth so now you can\naugment this state you can have a more\ncomplicated Mark process that has like a\ncounter that tells you how long you've\nbeen in this Facebook State and now\nyou've basically got lots of different\nstates depending on whether you've been\nin Facebook once or twice or three times\num and you can have an infinite set of\nthese you can have continuous set of\nthese um all of these things are\npossible but they don't change the\nfundamental structure from this being a\nmarkof\nprocess this is just a particular simple\nuh instantiation where we can see\neverything on one page but don't let\nthat mislead you into thinking that\nmicroprocesses are necessarily small or\nsimple we can solve very very complex\nmicro processes in the rest of the\ncourse we'll see how to solve you know\nmdps sometime some of them have 10 to\nthe 170 States will be one of the\nexamples we'll do later so you know\nthese could be very large and complex\nbeasts with a lot of structure to them\nokay so so far we haven't really talked\nabout reinforcement learning at all\nthere's no rewards there's no action so\nlet's start to put in some of that\nMachinery now and the first and perhaps\nmost important step is to add rewards\nand so one way to think of this is we're\nnow going to create what's called a mark\nof reward process which you could think\nof as like a mark of process with value\njudgments um so there's some value\njudgment saying how good it is to be in\num some particular you know there's some\nvalue judgment saying how much reward\nwill I have accumulated across some\nparticular sequence that we sampley from\nthis uh Mark process so all we're going\nto do to add into our our Mark process\nis we're going to add in two things so\nwe had our state space s and our\ntransition Dynamics P now what we're\ngoing to add in is some reward uh a\nreward function R and a discount Factor\ngamma let's just talk about what those\nare and make sure we understand them so\nthe reward function first of all this is\nsomething which tells us if we start in\nany state s if we're in some State um\nhow much reward do we get from that\nstate only this is just the immed reward\nhow much reward do I get from that state\nat that moment and what we care about is\nmaximizing the accumulated sum of these\nrewards that's what we care about in\nreinforcement learning so for the mark\nof reward process we're going to start\nsumming these things together over time\nbut R for now it just tells us for one\nstep at that moment if we're at time T\nin state s um at time t plus one we will\nget this reward it just tells us what\nwill happen\nnext okay um so if we go back to our\nstudent Mark of reward process what\nwe're going to do is we're just going to\nadd in some value judgments now um and\nI'm just going to tell you that you know\nU maybe you don't enjoy sitting in the\nclass so so let's call that minus two\nfor each each class um minus two reward\nhere minus two but if you pass at the\nend of the class you get this big juicy\nbonus of plus 10 okay uh Facebook maybe\nyou get like minus one per step as your\nbrain gets drained of any um sanity that\nyou used to have before you went onto\nFacebook and go to the pub maybe the\nbeer tastes good you get plus one okay\nthese old obviously subjective value\njudgments we're arbitrarily throwing\ninto this um and then what we care about\nis the total rewards that we're going to\nget across a whole chain here we don't\njust care about how much reward you get\nfor being in one class we care about the\nfact that you know maybe you get minus\ntwo here but if you follow through an\nentire sequence you might get min-2 plus\n-2 plus -2 plus 10 and that's your\noverall reward that you get for that\nsequence that sample of the the markof\nreward\nprocess and and so what we talk about is\nthis quantity called the return um so we\ncall the return G that's kind of you can\nthink of as the goal um the goal of\nreinforcement learning is to maximize\nthe return so G sort of stands the goal\nhere um and the goal here the return is\nthe reward summed over all of the time\nsteps infinitely into the future um and\nthe way that we control this return and\nmake it um finite is by using a discount\nFactor so we basically say we're going\nto Discount by a factor of gamma at each\ntime step going into the future so we're\ngoing to get a reward an immediate\nreward of R of rt+ one here but then\nwe're going to get gamma say you know. n\ntimes the reward at time Step t+ 2 plus\ngamma S9 2times the reward of t plus 3\nand so so on all the way to Infinity\nokay and that's going to be our goal our\ngoal is to maximize the sum of all of\nthese rewards Yeah question why is there\nno expect there's no expectation here\nthat's a good question why is there no\nexpectation here there's no expectation\nhere because we're just talking about a\nrandom sample at the moment so G is\nrandom G is just one samp exle from our\nmarkup reward process of the rewards\nthat we get going through that that\nsequence later in a couple of slides\nwe'll we'll look we'll introduce the\nexpectation which is what we actually\ncare\nabout so this discount Factor then has\nto be something between zero and one and\nit tells us if you like the present\nvalue of future rewards it tells you how\nmuch I care now about rewards I'll get\nin the future so it kind of tells me\nthat if I'm at time step 10 and I know\nI'm going to get a reward at time step\n20 then I should get discount that an\nadditional 10 times it's 10 time steps\ninto the future so we're going to\nDiscount 10 more\ntimes um so this thing has to be between\nZ and one um and zero kind of means\nmaximally shortsighted like if you have\na discount factor of zero you basically\nzero out anything beyond your current\ntime step and you only look at that\nfirst reward and if you have a discount\nfactor of one that's maximally\nfarsighted where you care about all\nrewards going infinitely far into the\nfuture and you just assume that your\nmark process has the property that\neventually these things will all um be\nfinite that you'll get zero like in our\nabsorbing terminal state you just get z\nz z at the\nend okay so specifically the value of\nreceiving reward r k+ one time steps\nlater is gamma to the KR we just keep\ndiscounting again and again discount\nFactor apply outside of problems with\nlike you know\nFinancial okay I'm gonna have a slide on\nthis so\num so so really why why should we do\nthis it's basically you know we're\nsomehow introducing a a judgment here\nhere which is that we prefer short-term\nreward to um delayed reward we prefer\nreward now to reward later and the\namount that we prefer reward now to\nreward later um is given by by gamma so\nthe closer it is to zero the more we\nprefer reward now and the closer it is\nto one the more indifferent we are to\nwhen those rewards arrive so why should\nthat be that was essentially the\nquestion why should we do that so I'm\ngoing to turn that over to the audience\nso in most of reinforcement learning we\nwe use a discount Factor so I just want\nto ask you guys well why why should we\ndo\nthis any think of\nreasons okay great so someone said\nthere's more uncertainty into the future\nI think that's a really great answer\nactually so um so one reason to use a\ndiscount Factor um is to basically\nrepresent the fact that we do not have a\nperfect model so if you imagine that\nwe're just building a markof process to\nmodel the environment we're building\nthis Markoff reward process but we don't\nhave a perfect model of the environment\num so so we think we've come up with\nsome you know great plan we think we\nknow exactly how much reward we're going\nto get into the future um but if we\ndon't entirely trust our our decisions\nthat we make if we don't entirely trust\nour evaluations we might choose to\nDiscount because we might say okay well\nmaybe I can take this pot of gold now or\nmaybe I should really trust my plan and\nmy model that's going to tell me that\nall these things are going to happen\nover the next 10 years and then I'll get\nan even bigger pot of gold um well you\nhave to really trust your model to wait\nall those time steps and and really\nbelieve that that things are going to\nturn out just as you planned in order to\nget that bigger pot of gold later so\nuncertainty I think is is really one of\nthe major kind of intuitive reasons to\nto Discount any other points any other\nideas yeah um when we're trying to plan\nfor the future\nvery okay absolutely so so to keep the\nthe maths bounded I mean and actually um\nso I'll do my first couple of slides\nhere so so I mean the first reason it's\njust mathematically convenient and I\nthink honestly that's the main reason we\ndo it in in the mdp framework for RL\nmost of the time it's just because the\nmath all works out it's easy to teach\nit's easy to understand um and it avoids\nthese infinite returns so if you have\nsome cycle in your in your markof\nprocess uh and and you just get rewards\nagain and again and again and again you\nyou need some mechanism to avoid\nInfinity as your um evaluation otherwise\nyou know the math just doesn't work out\nand there are other mechanisms for\ndealing with that which we'll mention\nbriefly\num we have the uncertainty um and I just\nwant to add on a couple of other ideas\nso so someone mentioned already um the\nidea that you know Financial settings\nit's rather natural so that's a quite\nspecific setting but but in a financial\nsetting um we have this idea of of\ninterest rate that you kind of know that\nthat money now is worth more than money\nlater and that's usually built into the\ndefinition of what it means to kind of\num make uh financial decisions or\ntrading uh in a trading system or\nwhatever uh so that's a specific case\nwhere there's really a meaning to this\nthat you can think of gamma as like the\ninverse of the interest\nrate um but one more point I just want\nto make is that actually you know\nanimals and humans they actually show a\npreference for immediate reward so\npeople have done tests on humans it\nturns out that that humans have not\nquite an exponential discounting but\nthey have something like a hyperbolic\ndiscounting um but certainly animals and\nhumans do prefer to get reward now than\nreward later and so you can think of\nthis as a cognitive model as well um of\nhow of how biological decision making um\noperates and Fin I just wanted to\nmention that if you uh if you don't\nagree with these points which I think is\nperfectly legitimate to say well hey\nlook you know the problem I care about\nreally um is artificial to introduce a\ndiscount Factor I'm only doing it to\nmake it convenient there are\nalternatives and there's undiscounted\nmarkof reward processes um the simplest\ncase being um where we know that all\nsequences terminate so if we go back to\nour our student um MRP we know that all\nsequences ended up with U at some point\nfalling asleep one way or another and so\nif we know that's the case then all\nsequences end um or at least end in this\nabsorbing terminal state which keeps\ngetting zero and zero and zero again and\ntherefore by definition um all returns\nare are finite um and so so then it's\nfine uh there's also a formulation\nthat's in the extended notes for this\nclass which is the average reward\nformulation and there it's possible even\nwith infinite sequences to still deal\nwith the undiscounted um um sort of\nevaluation of of Mark processes markward\nprocesses and\nmdps just to clarify\nsequences no that's not true so any\nsample of the any single sample which\nyou draw from this marker process by\ndefinition is is finite a finite length\nyouan I can't there isn't some part of\nthat process where I could just end up\nin an\ninfinite um so any sample which you draw\num there\nis so it's it's impossible to draw a\nsample that's infinite like by\ndefinition the sample that you draw will\nterminate at some point that's the\ndefinition of this process and so even\nthough the the decision process itself\ncontains infinite Loops any single\nsample that you draw will be a finite\nlength and will terminate and the only\nquestion is at what step it will\nterminate\nokay right so now someone asked about\nexpectations and so this brings us to\nthe value function which is really the\ncentral quantity which we're interested\nin um in in RL so the value function\nthis is if you this is the quantity we\nreally care about it's the long-term\nvalue of being in a state so if you're\nin this state s so if you're in some\nconcrete State s how much value um will\nyou get from there on What's the total\nreward that you'll get from that state\non so formally it's just the expected\nreturn if you start in that state so if\nI drop you into this Mark of reward\nprocess in some State like if I drop you\nin class two how much reward will you\nget from class two to the end until you\nterminate that's basically what it's\nsaying how much reward will you get from\nthen onwards um and we have this\nexpectation um because the environment\nis stochastic we were in this stochastic\nmarkup process uh the evolution of\nStates might might go one way one step\none one episode and another way the next\nepisode what we care about is the\nexpected return over all of those\ndifferent episodes and that's the value\nthat's how good it is to be in any given\nstate so this is the central quantity we\nprefer States we would prefer to be in\nstates that give us more total reward\nand so the formulation of of our problem\nis to say let's let's first of all just\nmeasure that so in an MRP there's\nconcept of maximizing we're just saying\nlet's just measure how much reward we'll\nget but when we get to mdps we want to\nmaximize this\nquantity okay so let's get concrete\nagain let's go back to the um student\nMarkov chain um actually student U\nmarkof award process um so we're going\nto sample returns now um so again we're\ngoing to look at these these samples of\nthis process that we've taken um and\nwe're consider a discount factor of\ngamma equals 0.5 um so basically the\namount that we care about getting\nrewards every step um and so this is the\ndefinition of our return with gamma\ninstantiated with a half and so now if\nwe just look at the the value of these\nguys um\nso so these are basically like the the\nreturns here so you get two at the first\nminus two at the first Deus two um\ndiscounted by a halfus two discounted by\na quarter and then at the end of it you\ngot this plus 10 reward for finishing\nthe class all three classes and that's\ndiscounted by an eight you sum those all\ntogether together you get minus 2.25 so\nthat's one sample of of the return so\nthis guy should say G1 um and now we've\ngot four different samples here these\nare um you know different samples of\ndifferent the first four um different\nsample um sequences through this markup\nreward process so what's the value of\nbeing in this state what's the value of\nthe start State uh one way to estimate\nit is you can just take a bunch of um\nsamples and just take the average of\nthese values and that would give you a\nlegitimate estimator for the value\nfunction\nthe value of that\nstate so the returns are samples they're\nrandom but the value function is is not\na random quantity it's an expectation\nover these random\nvariables okay is that clear why are we\nstarting G1 indices that we start\nfrom class one um no this this is uh\nthese subscripts are time steps so G1 um\nis so here we're basically saying so the\ndefinition if we go back to the um the\ndefinition of of\nreturn the definition of return was the\nreturn um starting at time time step T\nis the rewards from that time step\nonwards okay um so now we're considering\na bunch of different\nreturns um we are also starting from um\nS1 equal C1 in this case um but the this\nthing actually the subscripts are a Time\nsteps and they observed\nhere there's no expectations because\nessentially saying we' observed these\nsequencies These are observed samples of\nour of our random process and if we want\nto if we want to ask about the value\nfunction to make an expectation we're\nasking what's the expected return what's\nthe expected number that's going to pop\nout of this at the end you know what's\nthe average of these numbers and that's\nhow good it is to be in in this um at\nthe first date of this student markof\nchain okay so what does this look like\num so let's consider a couple of\ndifferent discount factors so first of\nall this is the maxim shortsighted view\nof the value function so this is the\nvalue function now for gamma equals zero\nthis is where we literally don't care\nabout anything except one step of\nimmediate reward so if we take any given\num State here so now if we consider our\nclass two State here we know that no\nmatter what we do afterwards we're going\nto get a reward of minus two um at this\ntime step it doesn't matter that we're\ngoing to get these other things in this\nplus 10 later that's irrelevant because\nwe're just looking at how much we get at\nthis time step onwards so the value of\nbeing in this state is minus two um\nthat's what we're going to get we're\ngoing to get minus two whether we go to\nsleep or whether we go this way we're\ngoing to get minus two that's just the\ndefinition it however um if we look at a\nmore long-sighted discount Factor so\nthis is like 0. n now um then the values\nall change um and now we have to\nconsider well if I was in this state um\nI'm going to get a reward of minus two\nnow but then um with 8 probability I'm\ngoing to transition over here into a\nsituation where I'm going to get\nadditional rewards minus two and then\nprobably plus 10 again and so forth um\nand when we factor that all together on\ncomputer and we'll see how to compute\nthese things later as we go on um the\nvalue of this state actually becomes um\nmuch better we can get 0.9 rewards from\nthis state on kind of averaging over the\nprobability of falling asleep and and\ncontinuing through the\nclasses so that's the value that's the\naverage now the expectation over all of\nthe different paths that we can take\nthrough this system and taking average\nof them that's the number that kind of\nsummarizes how good it is to be in that\nstate we're going to get 09 units of\nreward from then onwards if we start\nthere if I drop you into that class 0. n\nthat's how good you are\nokay so now we're going to talk about\nthis is maybe the most fundamental\nrelationship in in reinforcement\nlearning um might have come across this\nin dynamic programming it's it's very\nwidely um used and it's called the\nBellman equation and the idea is that\nthe the value function sort of obeys\nthis um recursive\ndecomposition and the main idea is very\nsimple that basically says that you can\ntake your your sequence of of rewards\nfrom this time step all the way onwards\nand you can break it up into two parts\nwhich basically consist of the immediate\nreward that you're going to get um and\nthen the value that you'll get from that\ntime step onwards so you break it up\ninto immediate reward and then from the\nsuccessor state that you end up in um\nthe value of of where you end up so\nimmediate reward plus the value of where\nwhere you end up you know if I'm a robot\nI might get an immediate reward of like\nplus 10 um but then I end up in this new\nstate and the question is how good is it\nto be in this new state and so the\noverall value function of being here is\nlike my 10 points of immediate reward\nplus the value of where I ended up\nthat's what the bman equation tells me\num so formally if we just step through\nthis it's very straightforward so this\nis our value function the value function\nis the expected return um given that I\nstart in a particular state so that's\nhow we want to know how good is it to be\nin this state and it's the expected\nreturn if you start in that state if we\njust unwrap the definition of return the\nreturn is just the the sum of these\nrewards going into the future discounted\num we can break down those sum of\nrewards into the immediate reward plus u\nthe discounted value the discounted uh\nreturn from The Next Step so we've got\nwe just pull out the discount Factor\nwe've got starting from RT plus 2 plus\ngamma RT plus 3 and so so forth for\npulled a fact of gamma out and now we\nsee that this thing here is just the\nreturn from the next time step the\nreturn starting from T plus2 instead of\nfrom t+1\num so that's this thing here we say it's\nthe expected immediate reward plus\ndiscounted return and then by the law of\niterated expectations what we can do is\nbasically say the expectation of this\nreturn is the same as the expectation of\nthe expectation of this return that's\ncalled The Law of iterated expectations\num which gives us this final line here\nwhich tells us that the value function\nin state s is equal to the immediately\nexpected immediate reward plus the value\nfunction of the next\nstate Yeah question I'm slightly\nconfused with indexes here like uh we\nstart in state s but the time is and we\nand count the reward the first reward\nlike t plus one okay so the question is\nwhy do we index the reward at time t\nplus one um actually you'll find both in\nthe literature so we're following the\nsattin convention which indexes rewards\nat time St t plus one rather than times\nSt T um this comes from how we think of\nthe boundary between agent and\nenvironment that the idea is that the\naction that we take goes into the\nenvironment then a Time step happens um\nwhilst you know that that we switch our\ntime index um after control passes back\nfrom the environment to the agent we're\nin a new time step um so at that point\neverything we receive from the from the\nenvironment has a new time index so\neverything after the environment is a\nTime step t plus one you might even find\nthat my slides might be slightly\ninconsistent on this point so it's a a\ngood spot so I'm sorry if that's the\ncase yeah yeah actually if you define it\nthat way when GMA Z the actual\nvalate the\nimmedi like if you go to the SL where g\nisal\nz the actual vales of the stat are going\nto be different there I think no that's\nnot true it's just a difference it's\nliterally just a reindexing so you can\nreindex everything with rt instead of RT\nplus1 it's not going to change any of\nthe math and any of the semantics it's\nall the same in the previous Slide the\nreward with zero disc was the reward in\nthe state that I'm currently in if you\nset gam to zero here it looks like the\nreward the state I'm currently in is the\nexpected reward of the next state\nexactly what it just depends no it just\ndepends on how we Define r t like where\ndoes that time step T refer to so here\nwhat we're saying is this is actually if\nthis if we move into this state of time\nstep T then this figure now indicates\nthat this reward is what happens at time\nstep t plus one that's all but we take\nwe move into this state and then we get\nthis reward no matter what we do\nindexing of the of the rewards and the\nstates is s of so so it's just it's just\na convention it's just a convention so\ndon't confuse yourselves it's just a\nconvention this stick to the very\nintuitive idea you move into a state you\nget some reward everything is good um\nokay okay so so let's just try and\nunderstand this Bellman equation a\nlittle bit more so the Bellman equation\num so this is our Bellman equation the\nvalue function now is equal to the\nexpected immediate reward plus\ndiscounted value at the next state so\nvalue now take a step um see what the\nvalue is in the state I end up um and\nthat's that's just like a a tautological\ndefinition that if we've got our value\nfunction correct it must obey U this\nidentity that's what it means to have a\nvalue function okay if we don't obey\nthis identity we haven't found the value\nfunction yet so one way to understand\nthis is by using what we call these um\nbackup diagrams here um so these backup\ndiagrams uh this is basically showing us\num you can think of this as like a\none-step look ahead search we start in\nthis state s and we can look ahead one\none step if you like so this this state\ns leads to a value function of a v of s\num if we go down to S Prime here or here\nwe're going to have a value function\nthat's V of the successor State okay and\nwe get some immediate reward along the\nway so this diagram is just kind of\nshowing us that we can think of this as\na one-step look ahead tree where you\nstart in this state you look ahead one\nstep you integrate over the prob of each\nof these um\nfunction here okay so it's a one-step\nlook ahead with like this averaging\nwhere we kind of go ahead one step we\naverage all the possible outcomes\ntogether and that gives us the value\nfunction at this step and that's\nactually kind of the way that we're\ngoing to proceed when we actually start\nto build algorithms they're going to do\nsomething like this look ahead process\nwhich is why it's useful to have these\nbackup diagrams in mind\nokay so if we think about the value\nfunction now let's just pick out one\nstate um so if we pick out this state\nhere um\nso this is the class three state um and\nso I'm just going to tell you that um\nthis is going back I think this is a a\ndiscount factor of of one now so it's\nundiscounted um and what we're looking\nat is what happens if we're in this\nstate with no discounting um so I\nclaimed that these this was the value\nfunction of this mdp but how can we\nverify that well we can use the bman\nequation to verify that we can say okay\nwell if this value function is 4.3 it\nshould really be equal to doing like a\none step look ahead averaging over all\nthe things which might happen next and\nthen ending up back here again okay so\nlet's see if that's really true well\nfrom here there are two things which\ncould happen um so we could either um\nbasically get um so we're going to get\nminus two immediate reward no matter\nwhat happens okay that's this minus two\nhere um and then with 6 probability\nyou're going to move through to the next\nclass um and end up someone with a value\nof 10 but there's also 4 probability\nthat you'll go to the pub and you'll end\nup in this state which had a value of8\nokay so if we sum those things together\nwe get min-2 plus6 * 10 plus4\ntime8 which to numerical Precision is\n4.3 okay and so that sort of validated\nthe fact that this really is the value\nof this\nmdp what thisal\nfor this\nis okay so so I I I I think I need to be\nclear about the rewards I think a few\npeople have uh so so let me try and\nexplain that more clearly what this\ndiagram represents is that you're in\nsome State um when you exit that state\nregardless of what you do you will get\nthe reward indicated by this R okay um\nso upon exiting that state regardless of\nwhere you end up you will get this minus\nso we could have duplicated this and\nwritten rus2 here rus2 here and it could\nhave been on these arcs um and actually\nwhen you Define markof reward um\ndecision processes which we'll see later\nyou and for markup reward processes you\ncan make it um dependent on the\nparticular Arc that you take you can\nmake it dependent on the action you can\nmake it dependent on the um on the\nsuccessor State all of these things are\npossible this is the simplest\nformulation the rewards just depend on\nthe state that you're in and that state\ndetermines how much immediate reward\nyou'll get um and then you have to look\nat way you ended up to see how much\nimmediate reward you'll get on the next\nstep so you start here you exit it you\nget minus two you get to here you exit\nthis state you get minus two you exit\nthis state you get minus two you exit\nthis state um you get Ral Z okay sorry\nyou get you get plus 10 you exit this\nstate repeatedly you get z z z z yeah\nlast question then I'll move\non yes\nyes thank you okay let's move\non okay so this Bellman equation you\nknow so far we've seen how to kind of\nspell this out literally state by state\nbut there's a much more concise\nformulation which is using U matrices\nand vectors um so if we just spend a\nmoment to understand that what we can do\nis basically use a um this Matrix\nformulation of the transition Matrix\nwhich we've already seen um but now what\nwe're going to do is we're going to\nintroduce a uh a vector representation\nof the value function which is the\nobvious thing it's basically we we form\na column Vector where each element of\nthe column Vector contains the value\nfunction of specific State uh for all\nstates from 1 to n okay and so now we\ncan write out our Bellman equation very\nconcisely it's just um that the value\nfunction that you start with this column\nVector here is equal to and we're going\nto do the same thing with the with the\nreward function as well we're going to\nwrite out the reward function with one\nelement per state this tells us how much\nreward you'll get from exiting State one\nall the way down to how much reward\nyou'll get from exiting State n so forth\nokay so this is your immediate reward um\nand then this is how much uh this is\nwhere you transition to and this is how\num how much value you started with\nbefore okay so it's basically telling us\num that in one sweep we can just look at\nthis thing we can say the value of of of\nthe state is equal to the immediate\nreward U plus the transition Matrix U so\nthese are all the places we might\ntransition to and then this is the value\nof where we end up okay this is the\nvalue of you know after I've taken a\nstep I might end up in this state I\nmight end up in this state I might end\nup in this state and each row of this\nthing um is going to be dot producted\nwith these values so there's some\nprobability if I'm in this state um that\nI end up in each of these successor\nStates and then we take the dot product\nof that with all of the values of where\nwe end up and that's going to give us\nthis first entry here so so it gives us\nliterally it's a way of writing out the\nentire vment equation in one very\nconcise\nform okay yeah question\nyes that should be pn1 thank you in the\nbottom\nleft okay so the vment equation is a\nlinear equation um as a result it can be\nsolved directly um it's one of the nice\nthings about linear equations this is\nnot going to be true once we move to um\nthe more complex case with Mark of\ndecision processes it's a nice property\nof just evaluating the rewards but once\nwe want to maximize rewards it's gets\nharder but at this stage at least we can\nsolve this directly assuming that our\nMatrix is small enough to invert what we\ncan do is just write out our linear\nequation now all we're going to do is\ncollect together the V terms here so\nwe've got you know um if you like\nidentity time V here and Gamma P time V\nhere so we collect those together on\nthis side IUS gamma p v equals R and\nthen we just invert this Matrix okay uh\nand this is the solution to the U to the\nmdp\num the computational complexity of\ninverting this Matrix is n cubed for n\nStates so this is not typically a\npractical solution method for large mro\nprocesses so what we'll start to look at\nin subsequent lectures is efficient ways\nto solve these problems um in particular\nyou know this is just going to be a\nbuilding block all of the things we've\nseen so far are just building blocks\ntowards what we really care about which\nis making decisions um and so this is\njust going to be a building block we\nneed to be able to efficiently figure\nout what's the value of being in this\nstate we can't always do this um but it\nis nice to know that the small U markup\nreward processes we can just invert this\nthing and and it gives us some intuition\ninto what's going\non um so later we'll talk about um next\nlecture in fact next class is dynamic\nprogramming um that's a very well-known\nclass of methods for basically doing\nthis more efficiently um then we'll talk\nabout Monte Carlo evaluation and\ntemporal difference learning that's like\nyou know the next couple of lectures of\nbasically doing exactly\nthat right so that was sort of the\nelementary building blocks leading up to\nthis point which is the mdp this is the\nthing which we actually use in\nreinforcement learning uh and so so I\nguess I should pause any questions so\nfar I guess we've been doing them as we\ngo\nso right so the mdp we're basically\ngoing to add in uh one more piece of\ncomplexity now which is actions so so\nfar there's been nothing for us to do\nyou know I just told you you get plonked\ndown in your um your student uh mark\nreward process and you just randomly\nsample these Transitions and get spat\nout at the other end you didn't get any\num you didn't have any agency you\nweren't able to take actions and make\ndecisions so let's now change that let's\nintroduce decisions um so what we're\ngoing to do now is basically take our\nMark of reward process which was a state\nspace transition Matrix reward function\nand discount Factor going to add in one\nmore component which is the the action\nspace um so a is now a finite set of\nactions in the extensions to the class\nyou'll see how this can be made um\ncontinuous or infinite um but for now\nwe'll consider the simplest case this is\na discrete U set of actions finite size\num and so now what we're going to do is\nsay the transition probability Matrix\nnow depends on which action we take so\nwhere you end up is now going to depend\non the action that you take if you take\nif you move to the left um the\nprobability you end up in all these\ndifferent states is going to be\ndifferent if you choose to move to the\nright you know the wind might blow you\ndifferent ways in each case\num but you have some agency over where\nyou're going to end up so the way we\nthink of this is that there's just one\nin the discret case there's like one um\nseparate Matrix of transition\nprobabilities for each action that you\nmight\ntake and now we've got this reward\nfunction the reward function again um\nmay depend on the action may also not\ndepend on the action that's but you know\nin general this thing can depend on the\naction apart from that everything is the\nsame the Machinery is the same we've\njust introduced actions in the simplest\npossible\nway so\nwhat we're going to do is actually redo\nour um our students U Mark process now\nbut as an mdp where where there's\ndecisions that you can take so so the\ndecisions are now um these red um labels\non the arcs and so the point now is that\nyou actually have some agency here um\nand so from this state you can choose to\neither study that's a choice now um so\nthere's no probabilities attached to\nthis you can choose to study if you\nstudy you you end up in this date um if\nyou choose to go on Facebook you end up\non this date um when you're in Facebook\nyou can choose to do it again or you can\nchoose to quit um you can choose to go\nto sleep here or to study again you can\nchoose to study again or you can choose\nto go to the pub if you choose to go to\nthe pub that's the only place now in\nthat there's any Randomness if you\nchoose to go to the pub who knows what\nmight happen you know maybe you'll drink\none or two beers not quite sure um so so\nnow there's some probability that you'll\nend up going back to class one going\nback to class two um or even going back\nto class\nthree Okay so uh there's a lot more\ncontrol that you can exert now over your\npath through this mdp this is this is\nthe reality for an agent and the the\ngoal is the game we be playing now is to\ntry and find the best path through your\ndecision- making process that maximizes\nthe sum of rewards that you get that's\nthe game that you play in an\nmdp okay so to do that the first thing\nwe need to talk about is um formalizing\nwhat it means to make decisions and to\ntake decisions and to do that we Define\nsomething um called the policy so the\npolicy is basically\num we're can talk about stochastic\npolicies today and and these policies\nare distributions over actions given\nStates so in other words if you're in\nsome State s this distribution basically\ngives you the mapping it says like if\nI'm in this state um map from that state\nto some probability of going left and\nsome probability of going right so this\nis the thing that tells me if I'm in\nclass one I am going to choose to um go\nto class 2 with probability point9 and\nto go on Facebook with probability. one\nthis is something under the agent\ncontrol it's something we decide and\nit's just a stochastic um transition\nMatrix and it's useful to make it\nstochastic because that allows us to do\nthings like exploration so we'll make\nuse of that later in the in the\ncourse um so once you have a policy it\ncompletely defines how that agent will\nbehave um and in an mdp um the policies\ndepend just on the current state that\nwas the mark of property uh doesn't\ndepend on the history and as a result uh\nwe consider U what we call stationary\npolicies we don't consider um um Pol so\nall of the policies the policy the same\nno matter what time step we're in in the\nmdp we just have the same policy at each\ntime step and the only thing that this\ndepends on is the state that we're in\nnot the time Step At which it happened\nand that's sufficient to behave\noptimally because by definition we we\nhave the mark of property and and and\nthis St s fully characterizes everything\nthat will happen next Yeah\nquestion Maxim\nthe question was why are there no\nrewards in this equation um and that's\nbecause the state s fully characterizes\nyour future rewards so in a mark of U\nreward process or Mark of decision\nprocess the mark of property means that\ns fully characterizes the evolution from\nthis state onwards um in the process and\nso what you're looking for is the policy\num which given the state you're in will\num you want to pick actions that will\nget you the most future award so all you\nneed to look at is what state am I in\nand what action should I take next and\nwe're trying to pick those actions in\nthe way given that state that will get\nus the most future reward so the rewards\nare in the future we don't care about\nhow many rewards we've got in the past\nthose rewards have gone now they've been\nconsumed um all we care about is how\nmuch reward we can get from now into the\nfuture so we don't actually care when we\ntry to maximize this thing we don't care\nif we're currently if we've already\nachieved a million reward or minus a\nmillion reward that's irrelevant what we\nwant to do is maximize the reward from\nnow onwards that's the way we're going\nto behave optimally our definition of\nState information wise includes\ninform yes yes\nexactly okay so one important thing to\nnote about um the connection between\nMarkov decision processes and Markov\nreward processes um is that we can\nalways recover an MRP or a Marco process\nfrom our decision process so just to\nunderstand that um we can think about\nbasically the sample of of states that\nwe draw like if we if we have some\npolicy that helps us pick actions we're\njust going to draw this sequence of\nStates um and the sequence of states\nthat we draw when we follow our\nparticular process is actually a mark of\nchain it's a mark of process itself no\nmatter what policy we choose that policy\ndefines some Mark of chain that actually\ndefines our Dynamics the way that we're\ngoing to move through and evolve through\nthis system um and if we look at the\nsequence of states and rewards that we\nreceive as we pass through like if once\nwe fix the policy if you fix the policy\nand look just at the sequence of states\nand rewards that you see as you go\nthrough this mdp that sequence\nis a mark of reward process and the way\nto understand that is that we're going\nto define basically transition Dynamics\nand and reward function which just\naverage over our policy so we're just\ngoing to average over all the things\nwhich happen under our policy we're\ngoing to average over so so we're going\nto define the transition Dynamics to be\nthe average of the transition Dynamics\nfor all of the things that we might do\nso if we're going to go left with\nprobability point5 and write with\nprobability .5 I'm just going to take5\ntimes the Dynamics all the probability\ngoing to all the states from here plus5\ntime the probability of all the states\nthat I'm going to end up in over here\nsum those together and those average\nDynamics Define some Markov reward\nprocess so we can always flatten our mdp\ngiven our current policy back into a\nMarkoff chain um this is just a useful\npoint it's not like Central so um you\ncan come back to this and think about it\noffline if you\nlike okay what is Central is the concept\nof the value function so we already had\nthe value function for a markof reward\nprocess but there was no agency there\nthere was no decisions now we've got\nthis policy there's some way that we can\nchoose to behave in our in our markof\nprocess you know I might be following\nthis path straight through the student\nMarkov chain where I I collect all the\nthe rewards by going through all the\nclasses or I might choose to spend a lot\nof time on Facebook these are going to\ngive different rewards there's not one\nexpectation anymore there are different\nexpectations depending on how I behave\nand so we subscript our value function\nby the policy that we're interested in\nevaluating so V Pi of s now tells us how\ngood is it to be in state s if I'm\nfollowing policy\nPi so if I'm following the policy that\njust goes straight through the classes\nhow much reward will I get or if I'm\nfollowing the policy where I stay on\nFacebook for as long as possible how\nmuch reward will I get from each state\nons um and so this basically is defined\nwe've just got this expectation here now\nthis e Pi basically means the\nexpectation when we sample all actions\naccording to this policy pi sort of\nhidden in that Epi notation we're also\ngoing to define a second type of value\nfunction called the action value\nfunction so so far we've got the state\nvalue function this V we call the state\nvalue function that tells us how good is\nit to be in a particular State s we're\nalso going to find the action value\nfunction which tells us how good is it\nto take a particular action from a\nparticular State and this is the thing\nwhich we intuitively care about when we\nwant to decide which action should I\ntake should I take should I go left or\nshould I go right well I should choose\nthe one that's going to be give me more\nreward um and the way I evaluate these\nthings um is by looking at the the\naction values in other words we're\nbasically going to look at so Q Pi so\nfor a given policy for a given way to\nbehave we want to know if I'm in this\nstate s and I take this action a what's\nthe expected return the total reward\nthat I'll get after I've taken that\naction how much reward will I get from\nthat point\nonwards okay that's the key quantity\nthat we're going to use to help us\noptimize our mdp and pick pick the best\num the best\nactions so let's make this concrete\nagain um so this is the state value\nfunction for the student mdp U\nundiscounted so the discount factor is\none um and this is just for uniform\nrandom Behavior so we're fixing the\npolicy to say we're always when we've\ngot a choice we're always going to\nrandomly pick 50/50 between those\nchoices okay and so this is the the\nvalue function that we end up with um we\nsee that you know we end up thinking\nit's not very good to start in this\nstate here because there's a probability\nof ending up in the Facebook State um\nbecause we we're choosing things\n50/50\num now what we can Define is another\nBellman equation so the Bellman equation\nuh we saw this in the mark of reward\nprocess now we're going to define a\nBellman equation for these value\nfunctions in the mdp case so again we\ncan use the same idea that the value\nfunction can be decomposed into an\nimmediate reward Plus the discounted\nvalue at the next stage so again there's\nthis idea that you know wherever you are\nyou take one step um and you get your\nimmediate reward for that step um and\nthen you look at the value of where you\nend up and the sum of those things\ntogether tells you how good it was to be\nin your original state that's still true\nin an mdp um now what we're saying is\nthat you start in this state and we know\nthat we're following policy Pi but the\nvalue of being in this state is still\nthe immediate reward that you get plus u\nthe value of your success estate if you\nknow that you're going to follow policy\nPI from there onwards so we just look at\nwhat happened for one state of um for\none step of following a policy and then\nwe ask how much more reward will I get\nfollowing that policy and we sum those\nthings together that gives us the\nBellman equation we can do the same\nthing with the action value function\nwith this these Q values and this\nbasically tells us now um if I'm in one\nstate and I take an action from there\nthen I'll get some immediate reward for\nthat action for that specific action and\nthen I'll look at I end up and I can ask\nwell what's the action value of the\nstate that I end up in under the action\nthat I would pick from that point\nonwards okay um so let's look at these\nkind of pictorially and try and\nunderstand them with these sort of look\nahead search diagrams so so the way to\nunderstand this is first of all to\nunderstand how V and Q relate to each\nother so if you're in a state value\nfunction here really it's what it's\nsaying is that we're going to average\nover um basically we're going to average\nover the actions um that we might take\nso there's sub probability that we'll\ntake this action here these black dots\nrepresent actions and these Open Circles\nrepresent States now um so we're in some\nState here um we might take this action\nhere we might take this action here the\nprobabilities of these things are\ndefined by our policy so this is like\nthe probability I'll go left the\nprobability I'll go right and for each\nof those actions we might take there's a\nthere's a q Val there's an action value\ntelling us how good it is to take that\naction from that state and so what we're\ndoing is we're doing like a one step\nlook ahead saying the state value how\ngood it is to in this state just look\nahead one step look at the action values\naverage them together and that tells us\nthe value of being in that state at the\ntop\nthere now let's do the the converse\nunderstand the opposite step so what\nhappens if we start off taking some\naction so now the root of this tree is a\nstate and we're considering a specific\naction that we take from that state\nthat's this black circle here so this is\nlike saying you know I'm in this\nparticular State here and I'm\nconsidering I'm asking the question how\ngood is it to go right from that state\nokay and how good it is we now have to\naverage over the Dynamics of our mdp the\nenvironment might might blow me over\nhere the environment might blow me over\nhere so after I've gone right I might\nget blown to all these different\nsituations and we want to ask for each\nof these situations I might get blown to\nhow good is it what's the the value of\nbeing in that situation under my current\nfollowing my policy after that point we\naverage over all these things um using\nthe probabilities of our transition\nDynamics we average them together and\nthat gives us the action value function\nat the root\nhere\nokay so V is telling us how good is it\nin to be in a particular state q is\ntelling us how good is it to take a\nparticular action from a given state\nstate value function action value\nfunction we can put these together now\nso this is basically just stitching\ntogether the two figures from the last\ntwo slides and now what we'll see is we\nget like a recursion that helps us\nunderstand V in terms of itself and this\nis how we end up solving um Mark of\ndecision\nprocesses so at the root of the tree now\nwe've got the value function for a\nparticular State tells us how good is it\nto be in this state and the way we're\ngoing to understand that is by doing a a\ntwo-step look ahead so we're going to\nlook ahead we could consider all the\nactions we might take next go left go\nright we could consider all the things\nthat the environment might do to us\nmight blow me over it might keep me\nstanding up might blow blow me over\nmight keep me standing up there are all\nthe things the environment might do and\nthen for each of those things the\nenvironment might do there's some\nsuccess state that we'll end up in we\nwant to know well how good is it to be\nin that state um and carry on with my\nusual policy how much reward will I get\nif I Carry On from that point now if we\naverage these things all together\nbasically averaging a two ways now we're\naveraging over our policy um we're\nwaiting each of these arcs by the\nprobability that our policy will select\nleft or right we're averaging each of\nthese arcs by the transition\nprobabilities that will end up getting\nblown in One Direction or another we\naverage it all together and that gives\nus the value of being in the root of\nthis diagram here tells us how good is\nit to be in a particular state so this\nis the Bellman\nequation um you can do exactly the same\nthing for Action values there's a\nrecursive relationship you can look at\nthis offline I don't want to sort of\nbelabor the point but it's just\nstitching the diagram together the other\nway around and we end up with exactly\nthe same idea that starting from\nparticular state in action we can now\nlook ahead two steps consider where the\nwind might blow us first now and then\nconsider from the state the wind might\nblow us to um which action might I take\nnext I might choose to go left or I\nmight choose to go right and now we\naverage over this thing again we look at\nwe look at the value of that particular\naction the Q value average these things\nall together and it tells us about the Q\nvalue at this start action here so the Q\nvalue relates to the Q values at the\nnext step in this slide and in the\nprevious slide um the state values here\nrelate to the state values at the next\nstep so you get these two recursive\nrelationships explaining how the value\nfunction relates to itself at the next\nstep but in all of these cases if you\njust look beneath the map the math it's\na very simple idea which says the value\nfunction at the current time step is\nequal to the immediate reward um plus\nthe value function of where you end\nup that's all these things are saying\njust with different ways of putting\ntogether the\nmaths okay so let's do that for one\nexample in our student mdp and then\nwe'll move on a little bit um so so\nhere's our our student mdp again we're\ngoing to consider just one state this\nred State here so the class three state\num and again I'm just going to verify\nwe're not kind of comput using this\nBellman equation to to comp these value\nfunctions we're going to use it to\nverify that this indeed does have the\nvalue of 7.4 okay um so how can we\nverify it well we can unroll we can do\nour our look ahead um and what we're\ngoing to do is we're going to do our our\nlook ahead for the state value function\nhere um so we're saying the state value\nfunction 7.4 is equal to um all the\nthings which we might do from there so\nwhat are the things we might do from\nthere well um we might study um remember\nwe're doing things 50/50 with a\num for now with 50% chance we we choose\nto study and 50% chance we go to the P\nso under that policy where we do\neverything\n50/50 um so yeah this whole diagram is\nshowing us the value function for the\npolicy where we choose all of our\ndecisions 50/50 and under that policy we\nstart in this state there's a0 five\nchance we'll take this Arc um and get\nplus 10 reward that's this part here\nthere's also a .5 chance will end up\ntaking this AR\nend up in the pub and the pub might take\nus with2 probability over here4\nprobability over here4 probability over\nhere and in each of those cases we'll\nend up in a different state with a\ndifferent value so there's a two chance\nwe'll end up in this value State here\nwith value minus\n1.3 as a point four chance will end up\nin this state with a value of 2.7 and um\nand a point four chance will end up in\nin this state here with a value of 7.4\nagain and end up where we started so you\ncan see this is quite cyclic this thing\num and so that's this equation here we\njust sum those things together so we're\nsumming together this look ahead of what\nmight happen after one step of taking\nactions and one step of where we end up\num and when you add this together to one\ndecimal place you get um you get\n7.4 so we sort of verify that these\nnumbers are self-consistent for this one\nstate you can verify them for all of the\nother states and you would discover that\nthis Bellman equation does indeed hold\nand therefore these numbers do represent\nthe value function for this mdp so that\nbasically tells us now we really know\nhow good is it to be in each one of\nthese states that you really will get\n7.4 units of reward in expectation if\nyou just behave according to this\npolicy okay is that\nclear right so that's all very well but\nit doesn't tell us the best way to\nbehave and what we really care about in\nmdp is figuring out the best way to\nbehave\num\nso okay just briefly before we do that\nso I briefly introduce uced earlier and\nI said it wasn't essential um but you\ncould this idea that you could flatten\num any mdp back into a Markoff reward\nprocess by defining these averaged State\ntransition Dynamics and averaged reward\nfunction and of course once we've\naveraged those things that gives us a\nmark of reward process we know how to\nsolve those things already so at least\none way to solve that another way to\nhave solved for these numbers would have\nbeen to form these matrices uh form the\nMatrix of our transition probabilities\nfor all of our actions average them\ntogether to get our P Pi average\ntogether our immediate rewards to get\nour R Pi solve for this equation here\nand that would have spat out our actual\nvalue function so that's one way to\narrive at the solutions and we'll look\nat more efficient ways to do it um going\ninto the\nfuture but you should at least\nunderstand the idea that the bman\nequation gives us a description of the\nsystem that we can solve once we solve\nfor this thing it tells us exactly what\nthe value function is so one linear\nequation we that you've got the value\nfunction you're\ndone\nokay right\nnow for the last um half an hour I want\nto talk about you know the essential\nproblem we really care about which is\nfinding the best behavior in the mdp but\nI'll just pause for a moment just um\nsorry I know there's a lot of material\nso any questions at this\npoint online are slightly different just\nminor changes to these are you going to\nput the newest ones online I'll put the\nnewest one's online immediately\nafterwards yeah so there it's mostly\nnotation change I um change the notation\nto match the new edition of s and can\nyou just explain again what happened to\nthe state which seems to have turned\ninto some\nother be a state right yeah so when we\nmoved from The Mark of reward process to\nthe mark of decision process we changed\nthe definition of the problem so Pub\nused to be a state now it's a decision\nthat we're making we're taking a\ndecision now um so so we're making these\ndecisions of whether to go to a pub um\nor whether to study\nso it's actually a different problem\nthat we're we're studying now to Pub is\nan action here Pub is an action yeah but\nit ends up at something which isn't a\nstate where study ends up in something\nwhich is a state so what no everything\nends up in a state so so if you if you\ntake the pub action you end up at this\nthis black dot represents like a chance\nmode yeah but you can't stay there you\ncan't stay there no the action takes you\nto here and you immediately the\nenvironment immediately transitions you\nto here to here or to here with these\nprobabilities so so you can think of\nthis is basically telling you the\nDynamics um if you take if you take this\nPub action you're basically going to be\nteleported to here here or here you'll\nget a plus one reward along the way and\nthe probability of ending up here here\nor here is given by these numbers that's\nwhat it means okay why did you do that I\ndon't understand why did you now have\nthis separate sort of trans you know\nthis sort transient State because I\nwanted to illustrate an example where\nyou have some agency where you have some\ncontrol over what happens in the\nenvironment right um and now it's a\nchoice whether to go to the pub or not\nand the randomness is now um what\nhappens after you go to the pub so all\nthe other states could be considered to\nbe the same as that but with just a\nprobability of one\nof to exactly everything you can\nimplicitly imagine there's one of these\nblack dots in every one of these other\nArts so if you go to Facebook there's a\nblack dot here that says the action is\ntaking you to that black dot but that\nblack dot is always taking you here with\nprobability one okay and actually now\nsay action also have Rewards\nthat the state has and now that's action\nyeah if we go back to the this was all\nin the definition of\nmdp um so if we go back to the\ndefinition of mdp um the red um\nhighlights here indicate what change\nfrom The Mark of reward process and one\nof the things was changed was that the\nreward function um was Now indexed by\nboth state and action so the reward does\ndepend on on the action you take that's\nthe definition of\nmdp okay so what you do might affect the\nMed reward you get as well as the state\nyou end up in so sometimes there's a\ncost to the action you take that's the\nsimplest case like um imagine a trading\nagent you buy um ,000 pounds of goods\nfrom from someone that immediately costs\nyou ,000 pounds compared to the action\nof do nothing which costs you zero so\nthere's a cost associated with the\nspecific actions and they take you to\ndifferent states so that's really the\ncommonest\ncase so you should intuitively expect\nthat rewards have different um effect\ndepending on on the particular action\nyou take so it's a form of\nnon-determinism of possibly taking an\naction but ending up probabilistically\nin one of a number of different states\nan mdp is stochastic an mdp is\nstochastic um the stochastic transitions\ndepend on the action that you take so\nyou get to take an action the\nenvironment gets to roll the dice and\ntell you where you end up so it's a\ncombination of your actions and the dice\nwhich get rolled by the environment\nwhich determine what happens\nnext okay good\nokay so now let's talk about um how to\nfind the best possible solution to the\nmdp so what we really want we don't\nreally care about how much reward we'll\nget following this crazy 50/50 policy in\nthe in the the um student markof chain\nwhat we care about is finding the best\npath through the system in general you\nwant to find the optimal the optimal way\nto solve your problem and so let's\ndefine what that really means now and so\nwe're going to start with value\nfunctions we're basically going to say\num this V Star U the optimal State value\nfunction V Star is the maximum value\nfunction over all\npolicies okay um so what we're saying is\nthere's all kinds of different policies\nwe could follow in our Markov chain and\nwhat we care about is the best of those\nwe care about understanding what's the\nmaximum possible amount of reward that\nyou can extract from this system if you\nthere's all these different ways you can\nTraverse this system each of your\ndifferent policies is going to lead to a\ndifferent Evolution um stochastic\nEvolution we care about which of those\nis going to lead to the most reward in\nexpectation that's V Star tells you not\nwhat the best policy was but what's the\nmaximum possible reward you can extract\nfrom the\nsystem um similarly the optimal action\nvalue function qar tells you um the\nmaximum amount of rewards you can\nextract um starting in state s and\ntaking action a so given that you\nactually commit to a particular action\nwhat's the most possible reward you can\nget from that point onwards so you start\nin some State you know that you're going\nto take left but what's the maximum\npossible reward you can get after you've\nmoved\nleft and what's really important about\nthis guy here is that if you know qar\nthen you're basically done if you want\nto know the optimal way to behave in\nyour mdp what Would You Do Well if\nsomeone told you the maximum possible\namount of reward you could get over all\npolicies given every action that you\nmight take so this is basically telling\nyou under all different ways you could\nbehave if you go left you might get 70\nunits of reward but under all different\npolicies if you go right you can get 80\nunits of reward well what do you do\nwhich way do you go you go right you get\n80 units of reward rather than 70 units\nof reward so this immediately tells you\nthe right action to take if you have qar\nyou're done you can kind of declare\nVictory you've got the quantities\nnecessary to behave optimally within\nyour mdp\nso we could say it informally that mdp\nis solved when we know this optimal\nvalue function so solving an mdp you can\nthink of as finding\nqar um so let's just look at that what\nis the optimal value function here so\nagain we're going to look at the\nundiscounted case just to keep the\nnumber simple um and in this case it\nshould be fairly intuitive you can kind\nof even work this out you know just by\neyeballing it that if you're in this\nstate here um should you go to the pub\num should you study well if you study\nyou're going to get this plus 10 U big\njuicy reward and go to sleep so that\nturns out to be the optimal value we'll\nsee more formally how to derive that in\na minute um if we go back back up one\nstate here we can either go to sleep and\nget a reward of zero along the way or we\ncan study and end up in this nice juicy\nstate with a value of 10 um and get\nminus two along the way so the value of\nbeing here is is eight um if we back up\none state again we can see the value of\nbeing in this state is is six because\nwith after Award of minus two we'll end\nup in this state with with with eight um\nso that's the optimal value that you can\nactually get this is V Star um in this\nmdp tells you how good it is to be in\neach of these states um and that tells\nyou the most possible juice you can\nextract from this mdp it doesn't yet\ntell you how to behave in it to do that\num there's two things you can do um well\nwhat what we do is we we basically\nDefine our Q stars now in order to\nfigure out what's the best action to\ntake we look at our action value\nfunction so the action value function is\nnow labeling these arcs with what's the\nvalue of each Arc so it's saying what's\nthe optimal value of each Arc so we know\nfor example the optimal value of this\nArc here that's 10 you're just going to\ntake this you're going to get your plus\n10 reward you'll be done so that's the\nvalue of this Arc here the value of this\nArc here um you know we know that you're\ngoing to basically get minus two for\ntaking that Arc and you're going to end\nup in a state that's going to get you 10\nunits of reward so value of this Arc is\neight the value of this Arc is zero um\nso now we can actually be able to make\ndecisions we can choose what's the\noptimal decision to take from here well\nclearly you would pick eight above zero\nso you would choose to take this Arc\nover here um similarly um oh yeah over\nhere we didn't talk about the arc that\ngoes down to the pub um and the value of\nthis Arc and we'll see how to compute\nthis in a minute um is actually 8.4 um\nbut once you've got this value um you\ncan say so this Arc is basically\naveraging over all the things which\nmight happen next and looking at the\nvalue of six8 and 10 and weighing them\num and so now you can choose between\nwell should I go into a state that gives\nme a um a reward of of 10 units going\nonwards or should I go to the pub where\nI get this nice plus one reward now and\nthen still have a chance to get my 10\nwell you have to weigh them up and\ndepends on the discount factor and all\nthese other things in this case we see\nthat this Arc has a value of 10 this Arc\nhas a value of 8.4 so fortunately it\nturns out I'm not recommending that\nyou'll go and drink beer and and um you\nstay and listen to the course yeah\nquestion there was a question ear about\ninfinite MP it doesn't happen but what\nhappens then if you have a positive\ncycle\nyou\nstud um so I was talking about a mark of\num chain at that point okay\nso\nso yeah so if you've got there are there\nare technical conditions under which um\nreasonably defined Mark comp decision\nprocesses are guaranteed to to terminate\nso if you want to find out more about\nthat there's a great book um by uh Bert\nseus he goes into all the technical\nconditions under which Mark competion\nprocesses are well defined um what we're\ngoing to talk about is largely the\ndiscounted case because it's simpler in\nthe discounted case you don't need to\nworry about any of that stuff so if you\ndo want to kind of remove discount\nfactors um and and and and consider the\ncase where you don't have discounting at\nall then you need to consider what\nhappens if you have infinite cycles and\nso forth and um and there's two ways to\ndeal with that um one of which is to\nlook at all the technical conditions\nmake sure your mdp satisfies those\ntechnical conditions um the second case\nis basically to um um use the average\nreward um version of of reinforcement\nlearning which is in the extension to to\nthis class if you had a large\nenough on a\nc\nc\nyes yeah I should have added so so there\nwere some technical conditions to my\nanswer earlier which were you know with\nwith certain probability transitions you\nso with the transition probabilities we\nhad before it wasn't possible to have um\ninfinite\nsequences of course if you have a a self\ncycle with probability one that does\nhave an infinite um cycle that can be\ndrawn from that sample so you have to\nhave if you want if you want to deal\nwith discounts of one there's Machinery\nyou need let's not worry about that\nthings are much easier when we have\ndiscounting\nokay yeah yeah sorry so um um\nso it's in all his books but um um so\nneurodynamic programming it's there but\nhe has his new um addition U probably\nthe best place to look is volume two of\nhis new edition of his um dynamic\nprogramming\num yes and I think it's volume two that\nhas has\nthis Okay so we've talked about optimal\nvalue function um that doesn't so so\nwhat we really care about you know\nwhat's the thing we really care about we\ncare about the optimal policy um so the\noptimal policy we need to understand\nwhat that really means what's the best\npossible way to behave in an mdp so\nwe've talked about policies a policy is\njust a like a stochastic mapping and\nmapping from states to actions that we\ntake and now we want to understand\nwhat's the best one of these things so\nfar we've just talked about how much\njuice we can extract from the system\nwhat's the maximum amount of reward we\ncan get we haven't yet talked about the\npolicy itself um and so to understand\nwhat it means to be optimal we need to\ndefine a notion of optimality um and to\ndo that we need to know what it means\nfor one policy to be better than another\npolicy um and so so what we do is we\nactually Define just a partial ordering\nover policies so this partial ordering\nbasically tells us let's consider two\npolicies um pi and Pi Prime U so these\nare two arbitrary policies and we're\njust going to Define this greater than\nor equal to um operator this is going to\nbe some partial ordering over policy\nspace it basically tells us the\nintuitive thing that one policy is\nbetter than another policy um if the\nvalue function for that policy is\ngreater than the value function for the\nother policy in all\nstates okay um so greater than or equal\nto in both cases um so what this means\nis that it can't be worse that that it's\nnot possible to say a policy is better\nthan another policy if it's actually\nworse than that policy in one state has\nto be at least as good in all states for\nus to say that it's greater than or\nequal in the partial\nordering okay and then there's this very\nimportant theorem which you'll find in p\nand all the important texts on on ntps\num and this basically tells us that um\nfor any mdp any mdp at all there is an\noptimal policy that is better than or\nequal to all other policies so you\nshould think about that like wow okay\nthis is a good thing you know you don't\nend up with this weird situation where\nyou know sometimes you need to think\nabout um taking this policy for a little\nbit of the mdp and this other policy for\na little bit of the mdp and that that\ncombination might be better there's\nalways one unique policy sorry there's\nalways at least one optimal policy P\nstar not necessarily unique that is\nbetter than or equal to all other\npolicies so there is always an optimal\npolicy there's you can this P star tells\nyou one way to behave in your mdp that\nwill extract the maximal juice from the\nmdp and we've already seen that that's\ntrue from the qar like if you have qar\nand you pick the action that chooses the\narc with the biggest Q star that gives\nyou one such policy okay but the theorem\nbasically tells you that that's true for\nany mdp that this really is the best\nthing that you could find at the mdp\nthere's not going to be some other thing\nthat's better just by there not like\nweird averaging policy that's going to\ndo better than this um furthermore it is\npossible to have more than one optimal\npolicy so it's possible that for example\nthe simplest case is that imagine\nthere's one action in your mdp um where\nthere's two separate actions that take\nyou to the same state doesn't really\nmatter which one of the those you take\nyou're going to end up in the same state\num they both can be optimal doesn't\nmatter which one of those you choose\nyou're going to get the same optimal\nvalue out of this mdp whether you go\nleft or go right okay so there can be\nmore than optimal policy but if they are\noptimal then all optimal policies\nachieve they extract the same amount of\nreward from the system they get the same\namount of juice um so the value\nfunctions for those policies are the\nsame and specifically the amount of\nvalue that they get um can be given by\nthe optimal value function by the\noptimal action value function\nhere okay so that the value of pi star\num is actually exactly the optimal value\nfunction that we defined earlier so the\nV Star and the Q star that we defined in\nthe previous slides are precisely the\nthe the numbers that you extract if you\ntry to evaluate the optimal policy so if\nyou look at how good the optimal policy\nis then it is indeed the maximum amount\nthat we thought we could get out of the\nsystem so this is like a sanity theorem\nlike that all the things you would hope\nto be true are true and everything is\nSaye and there is this thing called an\noptimal policy and it does give you the\nmaximum amount you can get from your\nfrom your\nmdp Okay so how do we find this thing\nwell we've already seen this intuitively\nso this is just writing it down in in\nequations it's just saying that all we\nneed is what you solve for qar and then\nyou pick the action that gives you the\nmost qar so in every state what you do\nif you're in state s um well you just\npick the action a with probability one\num that maximizes qar and that is the\naction that will give you maximum\npossible\nreward and there is always always a\ndeterministic optimal policy um so not\nonly is there an optimal policy there's\na deterministic optimal policy and this\ngives you one such deterministic optimal\npolicy okay and the good thing about\nthis is you know once we've got qar\nwe're done the game is solved we're\nhappy so let's do this um One Last Time\nback at our student mdp um so now what\nwe're going to look at is the optimal\npolicy um so now what we're looking at\nis these ARS remember represent the\nactions that we can take like we can we\ncan study or we can go into Facebook we\ncan study or we can um go to the pub and\nthese red arcs now are um highlighting\nthe optimal action value the optimal\npolicy Pi star it's highlighted in um in\nred here this is the undiscounted case\nagain um so it's just this simple path\nall the way through our um studying and\nstudying and studying again that we'd\nalready identified but we can see that\nit must be the case again just by\nlooking at the qar values um which are\nalso highlighted and seeing that the the\noptimal policy is always the one that\npicks you know whenever you've got two\nchoices it picks the one with the\nhighest Q so specifically over here\nwe've got the choice between Q staral 10\num for studying again qar equal 8.4 for\ngoing to the pub um so the optimal\npolicy the red Arc is the one that\nmaximizes that and gets us the most\nreward going into the\nfuture okay so how do we arrive at this\nqar in practice how do we actually um\narrive at fig out these qar values these\nqar values we've seen time and again\nthey're Central these this is the\ncentral quantity we're trying to figure\nout so that we can just bang pick the\nright action without even any look ahead\nor or further thought um so once we have\nqar we're done we've solved the mdp but\nhow do we arrive at qar so intuitively\nwhat did we do well we kind of worked\nour way backwards we started off at this\nfinal State um we looked at the the\nvalue of taking this Arc which was kind\nof trivially 10 you go to here you you\nfinish you get plus 10 along the way\nthen we work our way backwards and we\nkind of did this look ahead um with a\none step look ahead where we started\nhere and we said okay we start here you\nlook ahead one step you end up with 10\num work your way backwards and that\nworking backwards is precisely what we\nget out of a a Bellman equation so now\nwhat we're going to talk about is the\nBellman optimality equation so if you\njust open a textbook and hear about the\nBellman equation for mdps they're\ntalking about this one not the one we\nlooked at before it's the belman\noptimality equation this is the one that\ntells you how to really solve your mdp\nthe one that tells you you know how do\nyou relate the optimal value function to\nitself so before we looked at the bman\nexpectation equation for mdps which told\nus how V Pi related to itself so we\nlooked at for example um you know under\nthis average Behavior policy where you\nmight go right or left with equal\nprobability um how did the values relate\nto themselves now we're looking at\nreally the optimal values we're looking\nat the best things we're looking at\nthese um you know something like these\nthese red highlighted guys like the\nvalues in these circles now six6 a 10\nthe maximum amount of juice you can get\nfrom your system how does that number\nrelate to itself um and what we can do\nis do like a one-step look ahead again\nand so the way to understand this is\nAgain by this one step look ahead you\nstart in you can ask well what's the\noptimal value of being in some State\nwell you can consider each of the\nactions that you might take uh will take\nyou to one of these chance nodes here\none of these action nodes um and we can\nsay when we reach that action we can\nlook at the action value this is the red\nArc the number on the red Arc we can say\nthis had a number on the red Arc which\nwas eight and this had a number on the\nred Arc which was 10 or whatever it was\nand now what we do is instead of taking\nthe average of these guys we take the\nmax over these guys so we're basically\nsaying look at the value of each of the\nactions you can take and pick the max of\nthem and that's going to tell you how\ngood it is to be in this state here it's\nsimply the max of all the Q\nvalues I can go left I can go right I\nlook at how much reward I get in each\ncase and I pick the one gives gives me\nthe most okay so the value function of a\nstate is the max of the Q values of each\nof the actions you can take from that\nstate that\nclear um now we're going to do the other\nhalf so remember we had V going to Q now\nwe're going to do Q going to v um so now\nwe want to know well how good is one of\nthose red arcs in that diagram how how\ndo we know how what the optimal value is\nfor being in a particular State and\ntaking a particular action how do we\nknow that the uh but the study Arc has a\nparticular optimal value to it well\nagain we can do a one step look ahead U\nbut now we're looking ahead over what\nthe Dynamics might do what the\nenvironment might do to us where the\nwind might blow us we don't control this\nthis is like going to the pub and then\nyou know we don't know what will happen\nin the pub it's crazy place and we might\nend up in all these different situations\num and each of those states that we end\nup in has some optimal value and so we\nkind of you know this is like an\ninductive argument we sort of assume\nthat we know inductively um the optimal\nvalue of each of those states that we\nmight end up in um and if we know the\nvalue of each of those states that we\nmight end up in we just need to average\nover them now there's no Max here we\ndon't get to pick where the wind blows\nus we have to average over all the\nthings the environment might do to us\nand that tells us how good our action is\nso our action is um the optimal action\nvalue is just the immediate reward plus\nthe average over all the probability\nthat the wind might blow us left the\nprobability the wind might blow us right\nU multiplied by the optimal value of\nbeing in that\nstate\nokay and again what we can do is put\nthose two pieces together when you put\nthose two pieces together you have a\nrecursive relationship that relates V\nStar to itself so this gives us an\nequation that we can solve okay so this\nis now a like two-step look ahead we're\nlooking ahead over the actions we can\ntake here and maximizing over those and\nwe're also looking ahead over the dice\nthat the environment might roll\nand we don't maximize over the dice we\ndon't control the dice instead we\naverage over the dice that the\nenvironment can roll we do this two-step\nlook ahead over our actions from this\nstate and all the things the environment\nmight do to us then we look at the\noptimal value of where we end up we back\nthese things all the way up and that\ntells us how good it is to be in this\nstate here so that's the Bellman\noptimality equation\nV okay um\nso so finally just before we give one\nlast example um we can do the same thing\nflipping the diagram around and starting\nwith the action values and this is to\narrive at a recursive relationship\nbetween the qstar values and themselves\nso this isn't really saying anything\ndifferent to the previous slide it's\njust like a reordering of the same idea\nand the reordering tells us that if we\nstart and we want to know how good is\none of these arcs like how good is it to\nbe in a particular State and take a\nparticular action now we first of all\nconsider where the wind will blow us we\num average over the the the dice which\nthe environment's rolling and wherever\nthe dice roll we get to make a decision\nso wherever the wind blows us we get to\ndecide pick one action after that and we\nget to maximize over the decisions we\ntake so we average over the dice that we\nroll and we maximize over the decisions\nwe take and for each of these leaves we\nconsider the action value the optimal\naction value like how good is it to be\nin this St taking this particular action\nor in this state taking this action we\nback that up all the way to the\nbeginning and that tells us the Q value\nof the root of this\ndiagram okay so let's make that concrete\nuh by picking one state in this diagram\num probably stick Absolut stick to the\nteeth of this um student mdp by now um\nanyway I promise it won't come up too\nmuch again um so here we start off in\nthis state um we consider\nwe have two possible actions that we can\ntake um so this one's rather\nstraightforward there's no um noise that\nin the environment we're not considering\nany dice rolls it's just like people\nsaid before implicitly there's like a um\nthe environment always rolls the same\nway and takes us here if we take this\naction the environment always rolls the\ndice the same way to take us here so\nwe're just looking ahead one step by\nsaying that the value of this state six\nso this is V Star of this eight six is\nequal to the max over all the things we\ncan do here um of the value function of\nwhere we end up so we know that\ninductively if we assume that these\nother values are correct then we assume\nthat this has an optimal value of six\nthis has an optimal value of eight so if\nwe maximize over all the things we can\ndo we see that the um\num oh yeah there's a minus two along the\nway as well that's why this doesn't work\nso you can either get min - one U\nfollowed by um ending up in a state with\nsix so this has um um a value of five if\nwe take this Arc so qar of this Arc is\nfive or this way we get minus two from\nour eight and we get six so we choose to\ngo this way and our value function is\nsix\nokay so how do we solve this in practice\nso so previously we saw that we had\nthese Bellman equations with\nexpectations that we could just solve by\ndoing a matrix inversion so that's very\nappealing for those of you like mat lab\nand these nice tools where you can just\nuh you type in your Matrix type in your\num your vector representing the the\nrewards and Bam do your Matrix inversion\nyou're done unfortunately that doesn't\nwork for optimizing the bman optimality\nequation because we have these nonlinear\nequations now um we've got you know a\nMax in addition to our expectation and\nwe want to solve for this equation that\nrelates V Star to itself or qar to\nitself with a a Max of these\nexpectations so in general there's no\nclose form to this thing um and we have\nto be smarter and how do we be smarter\nwell using it ative solution methods U\nof which the best known examples are\nvalue iteration policy iteration um\nwhich we'll talk about in the next class\nthese are dynamic programming methods\nfor iterative iteratively solving these\nrecursive\nequations and Q learning which we'll\ntalk about in our subsequent\nvecture The Matrix solution itself is\nimplicitly iterative underneath we don't\nget away from doing any stive\ncalculations and you know to solve that\nMatrix equation\nproblem where you have to iterate in\nsome sense to get the\nsolution well I mean there's computation\nrequired to solve The Matrix and there's\nsteps to that computation um but it's\nnot uh so by iterative solution method\nwe mean something more like um where you\nhave um an equation which you\niteratively um apply at each step you\napply the next step of that update so\nyou start with some estimate you update\nyour estimate towards a new estimate and\nso forth now of course there are\niterative approaches to Matrix inversion\nthe Sherman Morrison trick which can be\napplied to to reinforcement learning as\nwell and so there are cases where the\nMatrix inversion can be reduced in\ncomplexity from n Cub to N2 those are\nspecial cases and so I'm talking about\nthe general approach here so yes and no\num okay\nso so we're just about out of time um I\nthink I want to really just take\nquestions and make sure people are\nfollowing rather than going into the\nextensions in any great detail um so so\nthis is a great time if you feel that\nthat something was unclear and you know\nnow's your chance to to\nask otherwise I'm just going to assume\nthat you know that's all the notes were\nincredibly perfectly clear and everyone\nunderstood everything which I'm fairly\nconf confident is not true yes please if\nyou have\nANF or missing\ninformation how would that translate to\nyour diagram of\nor have uni distrib where you should\nhave\none EnV okay so it's a great question so\nthe question is you know in in reality\num the mdp is just a model uh we're\ntrying to model some real phenomenon out\nin the world some real environment um\nhow do we represent the fact that our\nmodel is imperfect um we're actually\ngoing to have a class on um where we'll\ntalk about modelbased reinforcement\nlearning if there's time I'll talk about\nsome of the approaches to uncertainty um\nso some of the classical approaches to\nuncertainty include um explicitly\nrepresenting the uncertainty in the mdp\num so you could be basian for example\nhave some um posterior estimate of what\nthe mdp Dynamics are and then solve not\njust for one single mdp but for a whole\ndistribution over mdps and find the\npolicy that's optimal with respect to\nall of them that's computationally very\ndifficult another approach is to\nactually factor in the uncertainty in\nyour representation of of what's going\non world into your mdp itself so you say\nyou're in a state where not only in my\nin a state where my robots here but I'm\nin a state where my robot's here and I\nthink that the world has these Dynamics\num or I'm in a state where the robot's\nhere and I you I've seen these\nobservations so far which implicitly\ntell me something about the uncertainty\nof of what's going on in the mdp um so\nthat can be much more tractable you\ndon't necessarily need to explicitly\nreason about all the uncertainty in the\nenvironment um but it's maybe a little\nless intuitive to understand um the form\nof the uncertainty the uncertainty isn't\nmade explicit um and then maybe the only\nother thing to say which we already\ntouched on is that sometimes it's\nsufficient to just uh to just say well\naccept that the world is our model is\nimperfect and use a discount factor to\ncapture the fact that we've got some\nuncertainty you can also make the V the\ndiscount Factor variable to represent\nthe fact that you're more uncertain in\nsome states than others so you don't\nhave to have 0 n everywhere you could\nhave you know 0 n in one state 0 N9 in\nanother state repres presenting how how\nbad your model is in that situation um\nso it's a very active research area I\nthink it's a great\nquestion um yeah so everything you've\nshow shown us today seems to be sort of\ngeared towards like maximizing reward\nbut is there any is there anything\nimplicit in there that would also like\nhave a consideration of risk so like if\nyou you know I mean you might be\ninterested in iing your award at a\nfuture time and choosing that policy but\nthere might be some chance that choosing\nthat policy you know that that there's\nalso an increased risk um okay so um so\nthe question is um everything's just\ntalking about maximizing reward and\nexpectations of those reward without\nexplicitly considering risk or variance\nof of those returns um so I think the\ncorrect answer to that is to say that\nyou can always transform any mdp um with\nno risk\ninto so if there's some amount that you\ncare about your risk like let's say that\nthere's a cost that depends on on the\nvariance of your of your returns there\nis another mdp with which has a\ndifferent reward function that already\nfactors in um the variance of those um\nrewards into the actual reward that you\nsee at the end of your trajectory so\nthat can be implicit in in this in this\nmodel you've shown us today basically so\nthere is a transformation of any risk\nsensitive mdp into another mdp however\nthat transformation can be quite complex\nso you might need to remember all the\nthings that you've seen so far and it\ncan be it can be a more complicated mdp\nthat you end up having to solve so\nthat's the first answer um the second\nanswer is to say that you know a bunch\nof people do specifically consider risk\nsensitive mdps where you don't just\nconsider expectations but you consider\nyou know what's the variance of those\nthose rewards at the end of the day um\nso so there's a a bunch of work out\nthere and I'm happy to give you point as\nto that if you're if you're\ninterested any other questions\nokay one more and then I just want to\nsay a couple of words before everyone\npiles out oh yeah sorry this guy was\nactually waiting for longer some example\nintuition devation for instance I\nobviously we have two parts which\nbalance and thing is just pure M\nsituations forance somebody balance it\nknow ter of but in terms of like\nrealation\nsitot what does it make\nokay so the question is what's the\nintuition behind the Bellman equation in\nin some real example um so if we take\nthe Atari game as an example um so so\nthe bman optimality equation let's focus\non that one that's the you know the Crux\nof where we got to um the qar or or V\nStar is telling you you know what's the\nmaximum amount of score you can get you\nknow you're in this screen here what's\nthe maximum score you can get from this\nscreen here and the intuition is that\nall you need to do to get that maximum\nscore this is called the principle of\noptimality so the principle of\noptimality tells you that the way to get\nthe maximum score is to behave optimally\nfor one step and then to behave\noptimally for the remainder of your\ntrajectory okay if you behave optimally\nfor the remainder of your trajectory\nthat's the value function from the state\nthat you end up in and so now all you\nneed to do is to figure out well how do\nI behave optimally for one step and the\nway that you behave optimally for one\nstep is to maximize over um those\noptimal value functions in the places\nyou might end up in so it's like you\nmight move your you might move Pac\ncommand to the left um and you might\nmove Pacman to the right um if you move\nPacman to the right and and you can get\n100 points of um optimal value from that\npoint onwards or so so moving to the\nright you might get plus 10 and then 100\npoints of optimal value afterwards uh\nmoving to the left you might get plus 20\nuh but then only 50 points of optimal\nvalue afterwards and so the intuition is\nthat just by breaking down your\ntrajectory into these two parts into the\noptimal decision at one step and the\noptimal decision from there onwards you\ncan you can describe what it means to\nhave optimal Dynamics for the whole\nproblem okay uh one more question then I\njust want to say a couple of words about\nthe extensions but without any details\nat all so don't get scared yes in terms\nofal applications say you have million\nStates so if you have a million States\nand a million actions per state you have\na large mdp and so now you're asking how\ndo you how do you actually solve the mdp\nto to find the optimal value function is\nthat the question was the question how\ndo you solve it or how do you decide on\nthe immediate\nrewards\num I see so the question isn't how do\nyou solve the mdp it's how do you how do\nyou model the mdp how do you how do you\nrepresent a large MTP so typically the\nreward function is is given by the um by\nthe environment Dynamics um so it's\ngiven by uh so for example it might be\nyou know imagine you're playing Atari um\nthe score that you get there are many\nmore than a million States in Atari um\nand yet the score is just a function of\nthat state that you're in so it's\nsomething which you know you're in this\nstate you query the emulator you say\nwhat's the score or you extract the\nscore from the screen and it's just a\nfunction that depends on your state um\nso typically there's some function\nmapping state to reward and that\nfunction characterizes like the this is\nlike you know if imagine you're a\nprogrammer and you're trying to tell\nyour program how to what what the\nproblem is you're trying to solve the\nreward function is kind of like the\ndefinition of the the program you're\nasking the program to solve\nthis particular problem it's part of the\nproblem definition so the reward is just\na function of the it's part of the\nenvironment it's part of the thing\ndescribing how the environment state\ngets mapped into reward so in any\nreasonable problem you'll see that\nthere's a way to transform this and and\ndescribe the reward normally quite\nconveniently there's a big open question\nwhich is how do you best model the\nreward in the way that actually leads to\nintuitively what we as humans think of\nas the best\nsolution and yeah okay very briefly I\njust wanted to put up this um this slide\njust to highlight that there are some\nadditional slides extensions to mdps\nthis is not examinable Material I do not\nexpect you guys to go off and revise\nstuff which I haven't exam which I\nhaven't told you about and in general\nfor these classes I'll try to make it\nvery clear what's non-ex examinable\nmaterial and if we run out of time any\npoint Don't Panic you know it's you\nwon't suddenly find a question that crop\nup in the exam which we wer didn't cover\num but the extensions are on the slides\njust there is additional material in\nparticular telling you how to deal with\ninfinite mdps continuous mdps partially\nobservable mdps undiscounted average\nreward mdps these are the major\nextensions to the mdp framework uh just\nto give you some sense I think just out\nof interest you can get some sense it's\nnot details it's just a brief outline\nshowing you how you can move Beyond this\nto the case where where um where you're\ntrying to control some you know real\nairplane or helicopter with continuous\nactions and perhaps even continuous time\nsteps um or where the the state that you\nsee doesn't tell you everything about\nthe environment but you just get to see\nlike um the robot's camera um at every\ngiven time step and you don't you don't\nget exposed to the mark of state that's\nactually inside the environment you just\nget to see what what your agent Sees at\nthat moment um and finally the case\nwhere we we can deal with undiscounted\nCase by basically looking at um a\nlimiting case where we average over um\nwhere we look at the average reward\ngoing into the future the average reward\nper time step um instead of the\ndiscounted sum of rewards it's a\ndifferent definition of return that\nactually works out very neatly um so\njust if you're feeling enthusiastic go\nand have a look there's pointers to\nelsewhere um and otherwise the remainder\nis next week we'll start to talk about\nhow to solve these things so this is\njust defining the problem next week"
    },
    {
        "topic": "Q-Learning",
        "url": "https://www.youtube.com/watch?v=aCEvtRtNO-M",
        "transcript": "Error fetching transcript for video aCEvtRtNO-M: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=aCEvtRtNO-M! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
    },
    {
        "topic": "Neural Networks",
        "url": "https://www.youtube.com/watch?v=aircAruvnKk",
        "transcript": "This is a 3.\nIt's sloppily written and rendered at an extremely low resolution of 28x28 pixels, \nbut your brain has no trouble recognizing it as a 3.\nAnd I want you to take a moment to appreciate how \ncrazy it is that brains can do this so effortlessly.\nI mean, this, this and this are also recognizable as 3s, \neven though the specific values of each pixel is very different from one \nimage to the next.\nThe particular light-sensitive cells in your eye that are firing when \nyou see this 3 are very different from the ones firing when you see this 3.\nBut something in that crazy-smart visual cortex of yours resolves these as representing \nthe same idea, while at the same time recognizing other images as their own distinct \nideas.\nBut if I told you, hey, sit down and write for me a program that takes in a grid of \n28x28 pixels like this and outputs a single number between 0 and 10, \ntelling you what it thinks the digit is, well the task goes from comically trivial to \ndauntingly difficult.\nUnless you've been living under a rock, I think I hardly need to motivate the relevance \nand importance of machine learning and neural networks to the present and to the future.\nBut what I want to do here is show you what a neural network actually is, \nassuming no background, and to help visualize what it's doing, \nnot as a buzzword but as a piece of math.\nMy hope is that you come away feeling like the structure itself is motivated, \nand to feel like you know what it means when you read, \nor you hear about a neural network quote-unquote learning.\nThis video is just going to be devoted to the structure component of that, \nand the following one is going to tackle learning.\nWhat we're going to do is put together a neural \nnetwork that can learn to recognize handwritten digits.\nThis is a somewhat classic example for introducing the topic, \nand I'm happy to stick with the status quo here, \nbecause at the end of the two videos I want to point you to a couple good \nresources where you can learn more, and where you can download the code that \ndoes this and play with it on your own computer.\nThere are many many variants of neural networks, \nand in recent years there's been sort of a boom in research towards these variants, \nbut in these two introductory videos you and I are just going to look at the simplest \nplain vanilla form with no added frills.\nThis is kind of a necessary prerequisite for understanding any of the more powerful \nmodern variants, and trust me it still has plenty of complexity for us to wrap our minds \naround.\nBut even in this simplest form it can learn to recognize handwritten digits, \nwhich is a pretty cool thing for a computer to be able to do.\nAnd at the same time you'll see how it does fall \nshort of a couple hopes that we might have for it.\nAs the name suggests neural networks are inspired by the brain, but let's break that down.\nWhat are the neurons, and in what sense are they linked together?\nRight now when I say neuron all I want you to think about is a thing that holds a number, \nspecifically a number between 0 and 1.\nIt's really not more than that.\nFor example the network starts with a bunch of neurons corresponding to \neach of the 28x28 pixels of the input image, which is 784 neurons in total.\nEach one of these holds a number that represents the grayscale value of the \ncorresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.\nThis number inside the neuron is called its activation, \nand the image you might have in mind here is that each neuron is lit up when its \nactivation is a high number.\nSo all of these 784 neurons make up the first layer of our network.\nNow jumping over to the last layer, this has 10 neurons, \neach representing one of the digits.\nThe activation in these neurons, again some number that's between 0 and 1, \nrepresents how much the system thinks that a given image corresponds with a given digit.\nThere's also a couple layers in between called the hidden layers, \nwhich for the time being should just be a giant question mark for \nhow on earth this process of recognizing digits is going to be handled.\nIn this network I chose two hidden layers, each one with 16 neurons, \nand admittedly that's kind of an arbitrary choice.\nTo be honest I chose two layers based on how I want to motivate the structure \nin just a moment, and 16, well that was just a nice number to fit on the screen.\nIn practice there is a lot of room for experiment with a specific structure here.\nThe way the network operates, activations in one \nlayer determine the activations of the next layer.\nAnd of course the heart of the network as an information processing mechanism comes down \nto exactly how those activations from one layer bring about activations in the next layer.\nIt's meant to be loosely analogous to how in biological networks of neurons, \nsome groups of neurons firing cause certain others to fire.\nNow the network I'm showing here has already been trained to recognize digits, \nand let me show you what I mean by that.\nIt means if you feed in an image, lighting up all 784 neurons of the input layer \naccording to the brightness of each pixel in the image, \nthat pattern of activations causes some very specific pattern in the next layer \nwhich causes some pattern in the one after it, \nwhich finally gives some pattern in the output layer.\nAnd the brightest neuron of that output layer is the network's choice, \nso to speak, for what digit this image represents.\nAnd before jumping into the math for how one layer influences the next, \nor how training works, let's just talk about why it's even reasonable \nto expect a layered structure like this to behave intelligently.\nWhat are we expecting here?\nWhat is the best hope for what those middle layers might be doing?\nWell, when you or I recognize digits, we piece together various components.\nA 9 has a loop up top and a line on the right.\nAn 8 also has a loop up top, but it's paired with another loop down low.\nA 4 basically breaks down into three specific lines, and things like that.\nNow in a perfect world, we might hope that each neuron in the second \nto last layer corresponds with one of these subcomponents, \nthat anytime you feed in an image with, say, a loop up top, \nlike a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.\nAnd I don't mean this specific loop of pixels, \nthe hope would be that any generally loopy pattern towards the top sets off this neuron.\nThat way, going from the third layer to the last one just requires \nlearning which combination of subcomponents corresponds to which digits.\nOf course, that just kicks the problem down the road, \nbecause how would you recognize these subcomponents, \nor even learn what the right subcomponents should be?\nAnd I still haven't even talked about how one layer influences the next, \nbut run with me on this one for a moment.\nRecognizing a loop can also break down into subproblems.\nOne reasonable way to do this would be to first \nrecognize the various little edges that make it up.\nSimilarly, a long line, like the kind you might see in the digits 1 or 4 or 7, \nis really just a long edge, or maybe you think of it as a certain pattern of several \nsmaller edges.\nSo maybe our hope is that each neuron in the second layer of \nthe network corresponds with the various relevant little edges.\nMaybe when an image like this one comes in, it lights up all of the \nneurons associated with around 8 to 10 specific little edges, \nwhich in turn lights up the neurons associated with the upper loop \nand a long vertical line, and those light up the neuron associated with a 9.\nWhether or not this is what our final network actually does is another question, \none that I'll come back to once we see how to train the network, \nbut this is a hope that we might have, a sort of goal with the layered structure \nlike this.\nMoreover, you can imagine how being able to detect edges and patterns \nlike this would be really useful for other image recognition tasks.\nAnd even beyond image recognition, there are all sorts of intelligent \nthings you might want to do that break down into layers of abstraction.\nParsing speech, for example, involves taking raw audio and picking out distinct sounds, \nwhich combine to make certain syllables, which combine to form words, \nwhich combine to make up phrases and more abstract thoughts, etc.\nBut getting back to how any of this actually works, \npicture yourself right now designing how exactly the activations in one layer might \ndetermine the activations in the next.\nThe goal is to have some mechanism that could conceivably combine pixels into edges, \nor edges into patterns, or patterns into digits.\nAnd to zoom in on one very specific example, let's say the \nhope is for one particular neuron in the second layer to pick \nup on whether or not the image has an edge in this region here.\nThe question at hand is what parameters should the network have?\nWhat dials and knobs should you be able to tweak so that it's expressive \nenough to potentially capture this pattern, or any other pixel pattern, \nor the pattern that several edges can make a loop, and other such things?\nWell, what we'll do is assign a weight to each one of the \nconnections between our neuron and the neurons from the first layer.\nThese weights are just numbers.\nThen take all of those activations from the first layer \nand compute their weighted sum according to these weights.\nI find it helpful to think of these weights as being organized into a \nlittle grid of their own, and I'm going to use green pixels to indicate positive weights, \nand red pixels to indicate negative weights, where the brightness of \nthat pixel is some loose depiction of the weight's value.\nNow if we made the weights associated with almost all of the pixels zero \nexcept for some positive weights in this region that we care about, \nthen taking the weighted sum of all the pixel values really just amounts \nto adding up the values of the pixel just in the region that we care about.\nAnd if you really wanted to pick up on whether there's an edge here, \nwhat you might do is have some negative weights associated with the surrounding pixels.\nThen the sum is largest when those middle pixels \nare bright but the surrounding pixels are darker.\nWhen you compute a weighted sum like this, you might come out with any number, \nbut for this network what we want is for activations to be some value between 0 and 1.\nSo a common thing to do is to pump this weighted sum into some function \nthat squishes the real number line into the range between 0 and 1.\nAnd a common function that does this is called the sigmoid function, \nalso known as a logistic curve.\nBasically very negative inputs end up close to 0, \npositive inputs end up close to 1, and it just steadily increases around the input 0.\nSo the activation of the neuron here is basically a \nmeasure of how positive the relevant weighted sum is.\nBut maybe it's not that you want the neuron to \nlight up when the weighted sum is bigger than 0.\nMaybe you only want it to be active when the sum is bigger than say 10.\nThat is, you want some bias for it to be inactive.\nWhat we'll do then is just add in some other number like negative 10 to this \nweighted sum before plugging it through the sigmoid squishification function.\nThat additional number is called the bias.\nSo the weights tell you what pixel pattern this neuron in the second \nlayer is picking up on, and the bias tells you how high the weighted \nsum needs to be before the neuron starts getting meaningfully active.\nAnd that is just one neuron.\nEvery other neuron in this layer is going to be connected to \nall 784 pixel neurons from the first layer, and each one of \nthose 784 connections has its own weight associated with it.\nAlso, each one has some bias, some other number that you add \non to the weighted sum before squishing it with the sigmoid.\nAnd that's a lot to think about!\nWith this hidden layer of 16 neurons, that's a total of 784 times 16 weights, \nalong with 16 biases.\nAnd all of that is just the connections from the first layer to the second.\nThe connections between the other layers also have \na bunch of weights and biases associated with them.\nAll said and done, this network has almost exactly 13,000 total weights and biases.\n13,000 knobs and dials that can be tweaked and \nturned to make this network behave in different ways.\nSo when we talk about learning, what that's referring to is \ngetting the computer to find a valid setting for all of these \nmany many numbers so that it'll actually solve the problem at hand.\nOne thought experiment that is at once fun and kind of horrifying is to imagine sitting \ndown and setting all of these weights and biases by hand, \npurposefully tweaking the numbers so that the second layer picks up on edges, \nthe third layer picks up on patterns, etc.\nI personally find this satisfying rather than just treating the network as a total \nblack box, because when the network doesn't perform the way you anticipate, \nif you've built up a little bit of a relationship with what those weights and biases \nactually mean, you have a starting place for experimenting with how to change the \nstructure to improve.\nOr when the network does work but not for the reasons you might expect, \ndigging into what the weights and biases are doing is a good way to challenge \nyour assumptions and really expose the full space of possible solutions.\nBy the way, the actual function here is a little cumbersome to write down, \ndon't you think?\nSo let me show you a more notationally compact way that these connections are represented.\nThis is how you'd see it if you choose to read up more about neural networks.\n214\n00:13:41,380 --> 00:13:40,520\nOrganize all of the activations from one layer into a column as a vector.\nThen organize all of the weights as a matrix, where each row of that matrix corresponds \nto the connections between one layer and a particular neuron in the next layer.\nWhat that means is that taking the weighted sum of the activations in \nthe first layer according to these weights corresponds to one of the \nterms in the matrix vector product of everything we have on the left here.\nBy the way, so much of machine learning just comes down to having a \ngood grasp of linear algebra, so for any of you who want a nice visual \nunderstanding for matrices and what matrix vector multiplication means, \ntake a look at the series I did on linear algebra, especially chapter 3.\nBack to our expression, instead of talking about adding the bias to each one of \nthese values independently, we represent it by organizing all those biases into a vector, \nand adding the entire vector to the previous matrix vector product.\nThen as a final step, I'll wrap a sigmoid around the outside here, \nand what that's supposed to represent is that you're going to apply the \nsigmoid function to each specific component of the resulting vector inside.\nSo once you write down this weight matrix and these vectors as their own symbols, \nyou can communicate the full transition of activations from one layer to the next in an \nextremely tight and neat little expression, and this makes the relevant code both a lot \nsimpler and a lot faster, since many libraries optimize the heck out of matrix \nmultiplication.\nRemember how earlier I said these neurons are simply things that hold numbers?\nWell of course the specific numbers that they hold depends on the image you feed in, \nso it's actually more accurate to think of each neuron as a function, \none that takes in the outputs of all the neurons in the previous layer and spits out a \nnumber between 0 and 1.\nReally the entire network is just a function, one that takes in \n784 numbers as an input and spits out 10 numbers as an output.\nIt's an absurdly complicated function, one that involves 13,000 parameters \nin the forms of these weights and biases that pick up on certain patterns, \nand which involves iterating many matrix vector products and the sigmoid \nsquishification function, but it's just a function nonetheless.\nAnd in a way it's kind of reassuring that it looks complicated.\nI mean if it were any simpler, what hope would we have \nthat it could take on the challenge of recognizing digits?\nAnd how does it take on that challenge?\nHow does this network learn the appropriate weights and biases just by looking at data?\nWell that's what I'll show in the next video, and I'll also dig a little \nmore into what this particular network we're seeing is really doing.\nNow is the point I suppose I should say subscribe to stay notified \nabout when that video or any new videos come out, \nbut realistically most of you don't actually receive notifications from YouTube, do you?\nMaybe more honestly I should say subscribe so that the neural networks \nthat underlie YouTube's recommendation algorithm are primed to believe \nthat you want to see content from this channel get recommended to you.\nAnyway, stay posted for more.\nThank you very much to everyone supporting these videos on Patreon.\nI've been a little slow to progress in the probability series this summer, \nbut I'm jumping back into it after this project, \nso patrons you can look out for updates there.\nTo close things off here I have with me Lisha Li who did her PhD work on the \ntheoretical side of deep learning and who currently works at a venture capital \nfirm called Amplify Partners who kindly provided some of the funding for this video.\nSo Lisha one thing I think we should quickly bring up is this sigmoid function.\nAs I understand it early networks use this to squish the relevant weighted \nsum into that interval between zero and one, you know kind of motivated \nby this biological analogy of neurons either being inactive or active.\nExactly.\nBut relatively few modern networks actually use sigmoid anymore.\nYeah.\nIt's kind of old school right?\nYeah or rather ReLU seems to be much easier to train.\nAnd ReLU, ReLU stands for rectified linear unit?\nYes it's this kind of function where you're just taking a max of zero \nand a where a is given by what you were explaining in the video and \nwhat this was sort of motivated from I think was a partially by a \nbiological analogy with how neurons would either be activated or not.\nAnd so if it passes a certain threshold it would be the identity function but if it did \nnot then it would just not be activated so it'd be zero so it's kind of a simplification.\nUsing sigmoids didn't help training or it was very difficult \nto train at some point and people just tried ReLU and it happened \nto work very well for these incredibly deep neural networks.\nAll right thank you Lisha."
    },
    {
        "topic": "Architecture and Types",
        "url": "https://www.youtube.com/watch?v=fdvZ0SdTl7U",
        "transcript": "would you study this let's see what we\nhave today architecture you wake up put\non an all-black outfit grab your paper\nstraws and head to the vegan coffee shop\ndown the street after that you head to\nyour second home the studio you polish\nup the design you've been working on for\nweeks but your professor wants you to\nmake some changes again again and again\non the bright side you finished your\nwork early today it's only 3 15 am you\ngrab some dinner from the vending\nmachine pull out your sleeping bag and\ncrash on the studio floor"
    },
    {
        "topic": "Backpropagation",
        "url": "https://www.youtube.com/watch?v=Ilg3gGewQ5U",
        "transcript": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn.\nAfter a quick recap for where we are, the first thing I'll do is an intuitive walkthrough \nfor what the algorithm is actually doing, without any reference to the formulas.\nThen, for those of you who do want to dive into the math, \nthe next video goes into the calculus underlying all this.\nIf you watched the last two videos, or if you're just jumping in with the appropriate \nbackground, you know what a neural network is, and how it feeds forward information.\nHere, we're doing the classic example of recognizing handwritten digits whose pixel \nvalues get fed into the first layer of the network with 784 neurons, \nand I've been showing a network with two hidden layers having just 16 neurons each, \nand an output layer of 10 neurons, indicating which digit the network is choosing \nas its answer.\nI'm also expecting you to understand gradient descent, \nas described in the last video, and how what we mean by learning is \nthat we want to find which weights and biases minimize a certain cost function.\nAs a quick reminder, for the cost of a single training example, \nyou take the output the network gives, along with the output you wanted it to give, \nand add up the squares of the differences between each component.\nDoing this for all of your tens of thousands of training examples and \naveraging the results, this gives you the total cost of the network.\nAnd as if that's not enough to think about, as described in the last video, \nthe thing that we're looking for is the negative gradient of this cost function, \nwhich tells you how you need to change all of the weights and biases, \nall of these connections, so as to most efficiently decrease the cost.\nBackpropagation, the topic of this video, is an \nalgorithm for computing that crazy complicated gradient.\nAnd the one idea from the last video that I really want you to hold \nfirmly in your mind right now is that because thinking of the gradient \nvector as a direction in 13,000 dimensions is, to put it lightly, \nbeyond the scope of our imaginations, there's another way you can think about it.\nThe magnitude of each component here is telling you how \nsensitive the cost function is to each weight and bias.\nFor example, let's say you go through the process I'm about to describe, \nand you compute the negative gradient, and the component associated with the weight on \nthis edge here comes out to be 3.2, while the component associated with this edge here \ncomes out as 0.1.\nThe way you would interpret that is that the cost of the function is 32 times \nmore sensitive to changes in that first weight, \nso if you were to wiggle that value just a little bit, \nit's going to cause some change to the cost, and that change is 32 times greater \nthan what the same wiggle to that second weight would give.\nPersonally, when I was first learning about backpropagation, \nI think the most confusing aspect was just the notation and the index chasing of it all.\nBut once you unwrap what each part of this algorithm is really doing, \neach individual effect it's having is actually pretty intuitive, \nit's just that there's a lot of little adjustments getting layered on top of each other.\nSo I'm going to start things off here with a complete disregard for the notation, \nand just step through the effects each training example has on the weights and biases.\nBecause the cost function involves averaging a certain cost per example over \nall the tens of thousands of training examples, \nthe way we adjust the weights and biases for a single gradient descent step also \ndepends on every single example.\nOr rather, in principle it should, but for computational efficiency we'll do a little \ntrick later to keep you from needing to hit every single example for every step.\nIn other cases, right now, all we're going to do is focus \nour attention on one single example, this image of a 2.\nWhat effect should this one training example have \non how the weights and biases get adjusted?\nLet's say we're at a point where the network is not well trained yet, \nso the activations in the output are going to look pretty random, \nmaybe something like 0.5, 0.8, 0.2, on and on.\nWe can't directly change those activations, we \nonly have influence on the weights and biases.\nBut it's helpful to keep track of which adjustments \nwe wish should take place to that output layer.\nAnd since we want it to classify the image as a 2, \nwe want that third value to get nudged up while all the others get nudged down.\nMoreover, the sizes of these nudges should be proportional \nto how far away each current value is from its target value.\nFor example, the increase to that number 2 neuron's activation \nis in a sense more important than the decrease to the number 8 neuron, \nwhich is already pretty close to where it should be.\nSo zooming in further, let's focus just on this one neuron, \nthe one whose activation we wish to increase.\nRemember, that activation is defined as a certain weighted sum of all \nthe activations in the previous layer, plus a bias, \nwhich is all then plugged into something like the sigmoid squishification function, \nor a ReLU.\nSo there are three different avenues that can \nteam up together to help increase that activation.\nYou can increase the bias, you can increase the weights, \nand you can change the activations from the previous layer.\nFocusing on how the weights should be adjusted, \nnotice how the weights actually have differing levels of influence.\nThe connections with the brightest neurons from the preceding layer have the \nbiggest effect since those weights are multiplied by larger activation values.\nSo if you were to increase one of those weights, \nit actually has a stronger influence on the ultimate cost function than increasing \nthe weights of connections with dimmer neurons, \nat least as far as this one training example is concerned.\nRemember, when we talk about gradient descent, \nwe don't just care about whether each component should get nudged up or down, \nwe care about which ones give you the most bang for your buck.\nThis, by the way, is at least somewhat reminiscent of a theory in \nneuroscience for how biological networks of neurons learn, Hebbian theory, \noften summed up in the phrase, neurons that fire together wire together.\nHere, the biggest increases to weights, the biggest strengthening of connections, \nhappens between neurons which are the most active, \nand the ones which we wish to become more active.\nIn a sense, the neurons that are firing while seeing a 2 get more \nstrongly linked to those are the ones firing when thinking about a 2.\nTo be clear, I'm not in a position to make statements one way or another about \nwhether artificial networks of neurons behave anything like biological brains, \nand this fires together wire together idea comes with a couple meaningful asterisks, \nbut taken as a very loose analogy, I find it interesting to note.\nAnyway, the third way we can help increase this neuron's \nactivation is by changing all the activations in the previous layer.\nNamely, if everything connected to that digit 2 neuron with a positive \nweight got brighter, and if everything connected with a negative weight got dimmer, \nthen that digit 2 neuron would become more active.\nAnd similar to the weight changes, you're going to get the most bang for your buck \nby seeking changes that are proportional to the size of the corresponding weights.\nNow of course, we cannot directly influence those activations, \nwe only have control over the weights and biases.\nBut just as with the last layer, it's helpful \nto keep a note of what those desired changes are.\nBut keep in mind, zooming out one step here, this \nis only what that digit 2 output neuron wants.\nRemember, we also want all the other neurons in the last layer to become less active, \nand each of those other output neurons has its own thoughts about \nwhat should happen to that second to last layer.\nSo, the desire of this digit 2 neuron is added together with the \ndesires of all the other output neurons for what should happen to this \nsecond to last layer, again in proportion to the corresponding weights, \nand in proportion to how much each of those neurons needs to change.\nThis right here is where the idea of propagating backwards comes in.\nBy adding together all these desired effects, you basically get a \nlist of nudges that you want to happen to this second to last layer.\nAnd once you have those, you can recursively apply the same process to the \nrelevant weights and biases that determine those values, \nrepeating the same process I just walked through and moving backwards through the network.\nAnd zooming out a bit further, remember that this is all just how a single \ntraining example wishes to nudge each one of those weights and biases.\nIf we only listened to what that 2 wanted, the network would \nultimately be incentivized just to classify all images as a 2.\nSo what you do is go through this same backprop routine for every other training example, \nrecording how each of them would like to change the weights and biases, \nand average together those desired changes.\nThis collection here of the averaged nudges to each weight and bias is, \nloosely speaking, the negative gradient of the cost function referenced \nin the last video, or at least something proportional to it.\nI say loosely speaking only because I have yet to get quantitatively precise \nabout those nudges, but if you understood every change I just referenced, \nwhy some are proportionally bigger than others, \nand how they all need to be added together, you understand the mechanics for \nwhat backpropagation is actually doing.\nBy the way, in practice, it takes computers an extremely long time to \nadd up the influence of every training example every gradient descent step.\nSo here's what's commonly done instead.\nYou randomly shuffle your training data and then divide it into a whole \nbunch of mini-batches, let's say each one having 100 training examples.\nThen you compute a step according to the mini-batch.\nIt's not going to be the actual gradient of the cost function, \nwhich depends on all of the training data, not this tiny subset, \nso it's not the most efficient step downhill, but each mini-batch does give \nyou a pretty good approximation, and more importantly, \nit gives you a significant computational speedup.\nIf you were to plot the trajectory of your network under the relevant cost surface, \nit would be a little more like a drunk man stumbling aimlessly down a hill but taking \nquick steps, rather than a carefully calculating man determining the exact downhill \ndirection of each step before taking a very slow and careful step in that direction.\nThis technique is referred to as stochastic gradient descent.\nThere's a lot going on here, so let's just sum it up for ourselves, shall we?\nBackpropagation is the algorithm for determining how a single training \nexample would like to nudge the weights and biases, \nnot just in terms of whether they should go up or down, \nbut in terms of what relative proportions to those changes cause the \nmost rapid decrease to the cost.\nA true gradient descent step would involve doing this for all your tens of \nthousands of training examples and averaging the desired changes you get.\nBut that's computationally slow, so instead you randomly subdivide the \ndata into mini-batches and compute each step with respect to a mini-batch.\nRepeatedly going through all of the mini-batches and making these adjustments, \nyou will converge towards a local minimum of the cost function, \nwhich is to say your network will end up doing a really good job on the training examples.\nSo with all of that said, every line of code that would go into implementing backprop \nactually corresponds with something you have now seen, at least in informal terms.\nBut sometimes knowing what the math does is only half the battle, \nand just representing the damn thing is where it gets all muddled and confusing.\nSo for those of you who do want to go deeper, the next video goes through the same \nideas that were just presented here, but in terms of the underlying calculus, \nwhich should hopefully make it a little more familiar as you see the topic in other \nresources.\nBefore that, one thing worth emphasizing is that for this algorithm to work, \nand this goes for all sorts of machine learning beyond just neural networks, \nyou need a lot of training data.\nIn our case, one thing that makes handwritten digits such a nice example is that \nthere exists the MNIST database, with so many examples that have been labeled by humans.\nSo a common challenge that those of you working in machine learning will be familiar with \nis just getting the labeled training data you actually need, \nwhether that's having people label tens of thousands of images, \nor whatever other data type you might be dealing with."
    },
    {
        "topic": "Convolutional Neural Networks",
        "url": "https://www.youtube.com/watch?v=YRhxdVk_sIs",
        "transcript": "[Music]\nin this video we'll be discussing\nconvolutional neural networks a\nconvolutional neural network also known\nas a CNN or comp net is an artificial\nneural network that is so far been most\npopularly used for analyzing images\nalthough image analysis has been the\nmost widespread use of CNN's they can\nalso be used for other data analysis or\nclassification problems as well most\ngenerally we can think of a CNN as an\nartificial neural network that has some\ntype of specialization for being able to\npick out or detect patterns and make\nsense of them\nthis pattern detection is what makes CNN\nso useful for image analysis so if a CNN\nis just some form of an artificial\nneural network what differentiates it\nfrom just a standard multi-layer\nperceptron or MLP well a CNN has hidden\nlayers called convolutional layers and\nthese layers are precisely what makes a\nCNN well a CNN now CNN's can and usually\ndo have other non convolutional layers\nas well but the basis of a CNN is the\nconvolutional layers all right so what\ndo these convolutional layers do just\nlike any other layer a convolutional\nlayer receives input then transforms the\ninput in some way and then outputs the\ntransform input to the next layer with a\nconvolutional layer this transformation\nis a convolution operation we'll come\nback to this operation in a bit for now\nlet's look at a high-level idea of what\nconvolutional layers are doing as\nmentioned earlier convolutional neural\nnetworks are able to tech patterns and\nimages more precisely the convolutional\nlayers are able to detect patterns well\nactually let's be a little more precise\nthan that with each convolutional layer\nwe need to specify the number of filters\nthe layers should have and will speak\ntechnically about what a filter is in\njust a few moments but for now\nunderstand that these filters are\nactually what detect the patterns now\nwhen I say that the filters are able to\ndetect patterns what precisely do I mean\nby patterns well think about how much\nmay be going on in any single image\nmultiple edges shapes textures objects\netc\nso one type of quote pattern that a\nfilter could detect could be edges and\nimages so this filter would be called an\nedge detector for example some filters\nmay detect corners some may detect\ncircles other squares now these simple\nand kind of geometric filters are what\nwe'd see at the start of our network\nthe deeper our network goes the more\nsophisticated these filters become so in\nlater layers rather than edges in simple\nshapes our filters may be able to detect\nspecific objects like eyes ears hair or\nfur feathers scales and beaks even and\nin even deeper layers the filters are\nable to take even more sophisticated\nobjects like full dogs cats lizards and\nbirds to understand what's actually\nhappening here with these convolutional\nlayers and their respective filters\nlet's look at an example so say we have\na convolutional neural network that's\naccepting images of handwritten digits\nlike from the amnesty ADA set and our\nnetwork is classifying them into their\nrespective categories of whether the\nimages of a 1 2 3 etc let's now assume\nthat the first hidden layer in our model\nis a convolutional layer as mentioned\nearlier when adding a convolutional\nlayer to a model we also have to specify\nhow many filters we want the layer to\nhave a filter can technically just be\nthought of as a relatively small matrix\nfor which we decide the number of rows\nand number of columns that this matrix\nhas and the values within the matrix are\ninitialized with random numbers so for\nthis first convolutional layer in this\nexample of ours we're going to specify\nthat we want the layer to contain one\nfilter of size 3 by 3 now when this\nconvolutional layer receives input the\nfilter will slide over each 3x3 set of\npixels from the input itself until it\nslid over every 3x3 block of pixels from\nthe entire image this sliding is\nactually referred to as convolving so\nreally we should say that the filter is\ngoing to convolve across each 3x3 block\nof pixels from the input to actually\nillustrate this I'm going to use an\nexample that Jeremy Howard used in one\nof his lectures for fast AI his example\nreally gave me a lot of insight behind\nwhat was going on within a convolutional\nlayer so I'd like to share that with you\nall too I've also linked to his lecture\nin the description of this video\nso here we have our matrix\nrepresentation of an image of a7 from\nthe emne status set the values in this\nmatrix are the individual pixels from\nthe image alright so this is our input\nand this input will be passed to a\nconvolutional layer as just discussed\nwe've specified this layer to only have\none filter and this filter is going to\nconvolve across each 3x3 block of pixels\nfrom the input so here's our 3x3 filter\nof random numbers here when the filter\nfirst lands on the first 3x3 block of\npixels the dot product of the filter\nitself with the 3x3 block of pixels from\nthe input will be computed and stored\nthis will occur for each 3x3 set of\npixels that the filter convulse so look\nwe would just take the dot product of\nthe filter here with this first 3x3\nblock and then we'd store it over here\nnow we slide to the next 3x3 block take\nthe dot product and then store the value\nhere if we look at the formula for each\nof these cells we can see that it is\njust indeed the dot product of the\nfilter with each 3x3 section of pixels\nfrom the input so here we have this\nfirst value is the dot product of this\ninput with this filter and then if I\nclick on another random value over here\nwe can see that this value is the dot\nproduct of the filter with this input so\nafter this filter has convolve the\nentire input will be left with a new\nrepresentation of our input which is\ngoing to be made up of the entire matrix\nof those store dot products we got from\nthe filter this matrix of dot products\nis going to be the output of this layer\nand is represented here this is what\nwill then be passed to the next layer as\ninput in this same process that we just\nwent through with the filter will happen\nto this new output with the next layers\nfilters now this was just a very simple\nillustration but as mentioned we can\nthink of these filters as pattern\ndetectors so we can't really observe any\nspecific pattern that was picked out\nfrom our filter in the example we just\nlooked at in Excel but let's show our\noriginal image of the 7 here and now\nlet's say we have 4 3 by 3 filters for\nour convolutional layer and these\nfilters are filled with the values you\nsee here and these values can be\nrepresented visual\nas these filters where the minus ones\ncorrespond to black ones correspond to\nwhite and zeros correspond to gray\nso if we convolve our original image of\na7 with each of these four filters\nindividually this is what the output\nwould look like for each filter we can\nsee that all four of these filters are\ndetecting edges and the output that\nbrightest pixels can be interpreted as\nwhat the filter has detected so this\nfirst one we can see detects top\nhorizontal edges of the seven and that's\nindicated by the brightest pixels here\nthe second detects the left vertical\nedges again being displayed with the\nbrightest pixels the third detects\nbottom horizontal edges and the fourth\ndetects right vertical edges now these\nfilters are really basic and just detect\nedges these are filters we may see\ntowards the start of our network more\ncomplex filters would be located deeper\nin the network and would gradually be\nable to detect more sophisticated\npatterns like the ones shown here we can\nsee the shapes that the filters on the\nleft detected from the images on the\nright this one here detects circles and\nthis one at the bottom is detecting\ncorners and as we go even further into\nour layers the filters are able to\ndetect much more complex patterns like\nthese dog faces being interpreted in\nthis filter or even the bird legs\ndetected in this one all right so now if\nyou're interested in seeing how to work\nwith cnn's and code then check out the\nCNN and fine tuning videos in Mike\nHarris deep learning playlist so I hope\nthat you now have a basic understanding\nof convolutional neural networks and how\nthese networks are made up of\nconvolutional layers which themselves\nare made up of filters and I hope you\nfound this video helpful if you did\nplease like the video subscribe suggest\nand comment and thanks for watching\n[Music]"
    }
]