[
    {
        "topic": "Overview of Machine Learning",
        "url": "https://www.youtube.com/watch?v=ukzFI9rgwfU",
        "transcript": "we know humans learn from their past\nexperiences\nand machines follow instructions given\nby humans\nbut what if humans can train the\nmachines to learn from the past data and\ndo what humans can do and much faster\nwell that's called machine learning but\nit's a lot more than just learning it's\nalso about understanding and reasoning\nso today we will learn about the basics\nof machine learning\nso that's paul he loves listening to new\nsongs\nhe either likes them or dislikes them\npaul decides this on the basis of the\nsong's tempo\ngenre\nintensity and the gender of voice for\nsimplicity let's just use tempo and\nintensity for now so here tempo is on\nthe x axis ranging from relaxed to fast\nwhereas intensity is on the y axis\nranging from light to soaring we see\nthat paul likes the song with fast tempo\nand soaring intensity while he dislikes\nthe song with relaxed tempo and light\nintensity so now we know paul's choices\nlet's say paul listens to a new song\nlet's name it as song a song a has fast\ntempo and a soaring intensity so it lies\nsomewhere here looking at the data can\nyou guess whether paul will like the\nsong or not correct so paul likes this\nsong by looking at paul's past choices\nwe were able to classify the unknown\nsong very easily right let's say now\npaul listens to a new song let's label\nit as song b so song b\nlies somewhere here with medium tempo\nand medium intensity neither relaxed nor\nfast neither light nor soaring now can\nyou guess whether paul likes it or not\nnot able to guess whether paul will like\nit or dislike it are the choices unclear\ncorrect we could easily classify song a\nbut when the choice became complicated\nas in the case of song b yes and that's\nwhere machine learning comes in let's\nsee how in the same example for song b\nif we draw a circle around the song b we\nsee that there are four votes for like\nwhereas one would for dislike if we go\nfor the majority votes we can say that\npaul will definitely like the song\nthat's all this was a basic machine\nlearning algorithm also it's called k\nnearest neighbors so this is just a\nsmall example in one of the many machine\nlearning algorithms quite easy right\nbelieve me it is but what happens when\nthe choices become complicated as in the\ncase of song b that's when machine\nlearning comes in it learns the data\nbuilds the prediction model and when the\nnew data point comes in it can easily\npredict for it more the data better the\nmodel higher will be the accuracy there\nare many ways in which the machine\nlearns it could be either supervised\nlearning unsupervised learning or\nreinforcement learning let's first\nquickly understand supervised learning\nsuppose your friend gives you one\nmillion coins of three different\ncurrencies say one rupee one euro and\none dirham each coin has different\nweights for example a coin of one rupee\nweighs three grams one euro weighs seven\ngrams and one dirham weighs four grams\nyour model will predict the currency of\nthe coin here your weight becomes the\nfeature of coins while currency becomes\nthe label when you feed this data to the\nmachine learning model it learns which\nfeature is associated with which label\nfor example it will learn that if a coin\nis of 3 grams it will be a 1 rupee coin\nlet's give a new coin to the machine on\nthe basis of the weight of the new coin\nyour model will predict the currency\nhence supervised learning uses labeled\ndata to train the model here the machine\nknew the features of the object and also\nthe labels associated with those\nfeatures on this note let's move to\nunsupervised learning and see the\ndifference suppose you have cricket data\nset of various players with their\nrespective scores and wickets taken when\nyou feed this data set to the machine\nthe machine identifies the pattern of\nplayer performance so it plots this data\nwith the respective wickets on the\nx-axis while runs on the y-axis while\nlooking at the data you'll clearly see\nthat there are two clusters the one\ncluster are the players who scored\nhigher runs and took less wickets while\nthe other cluster is of the players who\nscored less runs but took many wickets\nso here we interpret these two clusters\nas batsmen and bowlers the important\npoint to note here is that there were no\nlabels of batsmen and bowlers hence the\nlearning with unlabeled data is\nunsupervised learning so we saw\nsupervised learning where the data was\nlabeled and the unsupervised learning\nwhere the data was unlabeled and then\nthere is reinforcement learning which is\na reward based learning or we can say\nthat it works on the principle of\nfeedback here let's say you provide the\nsystem with an image of a dog and ask it\nto identify it the system identifies it\nas a cat so you give a negative feedback\nto the machine saying that it's a dog's\nimage the machine will learn from the\nfeedback and finally if it comes across\nany other image of a dog it will be able\nto classify it correctly that is\nreinforcement learning to generalize\nmachine learning model let's see a\nflowchart input is given to a machine\nlearning model which then gives the\noutput according to the algorithm\napplied if it's right we take the output\nas a final result else we provide\nfeedback to the training model and ask\nit to predict until it learns i hope\nyou've understood supervised and\nunsupervised learning so let's have a\nquick quiz you have to determine whether\nthe given scenarios uses supervised or\nunsupervised learning simple right\nscenario one facebook recognizes your\nfriend in a picture from an album of\ntagged photographs\nscenario 2 netflix recommends new movies\nbased on someone's past movie choices\nscenario 3 analyzing bank data for\nsuspicious transactions and flagging the\nfraud transactions think wisely and\ncomment below your answers moving on\ndon't you sometimes wonder how is\nmachine learning possible in today's era\nwell that's because today we have\nhumongous data available everybody is\nonline either making a transaction or\njust surfing the internet and that's\ngenerating a huge amount of data every\nminute and that data my friend is the\nkey to analysis also the memory handling\ncapabilities of computers have largely\nincreased which helps them to process\nsuch huge amount of data at hand without\nany delay and yes computers now have\ngreat computational powers so there are\na lot of applications of machine\nlearning out there to name a few machine\nlearning is used in healthcare where\ndiagnostics are predicted for doctor's\nreview the sentiment analysis that the\ntech giants are doing on social media is\nanother interesting application of\nmachine learning fraud detection in the\nfinance sector and also to predict\ncustomer churn in the e-commerce sector\nwhile booking a gap you must have\nencountered surge pricing often where it\nsays the fair of your trip has been\nupdated continue booking yes please i'm\ngetting late for office\nwell that's an interesting machine\nlearning model which is used by global\ntaxi giant uber and others where they\nhave differential pricing in real time\nbased on demand the number of cars\navailable bad weather rush r etc so they\nuse the surge pricing model to ensure\nthat those who need a cab can get one\nalso it uses predictive modeling to\npredict where the demand will be high\nwith the goal that drivers can take care\nof the demand and search pricing can be\nminimized great hey siri can you remind\nme to book a cab at 6 pm today ok i'll\nremind you\nthanks no problem comment below some\ninteresting everyday examples around you\nwhere machines are learning and doing\namazing jobs so that's all for machine\nlearning basics today from my site keep\nwatching this space for more interesting\nvideos until then happy learning"
    },
    {
        "topic": "Types of Machine Learning",
        "url": "https://www.youtube.com/watch?v=yN7ypxC7838",
        "transcript": "welcome to wide world programming where\nwe simplify programming for you with\neasy to understand by code videos and\ntoday I'll be giving you a brief\nexplanation of all machine learning\nmodels so let's get started\nbroadly speaking all machine learning\nmodels can be categorized as supervised\nor unsupervised we'll uncovered each one\nof them and what all types they have\n[Music]\nnumber one supervised learning it\ninvolves a series of function that map's\nan input to an output based on a series\nof example input-output pairs for\nexample if we have a data set of two\nvariables one being age which is the\ninput and other being the shoe size as\noutput we could implement a supervised\nlearning models to predict the shoe size\nof a person based on their age further\nwith supervised learning there are two\nsub categories one is regression and\nother is classification\nin relation model we find a target value\nbased on independent predictors that\nmeans you can use this to find\nrelationship between a dependent\nvariable and an independent variable in\nregression models the output is\ncontinuous some of the most common types\nof resistant model include number one\nlinear regression which is simply\nfinding a line that fits the data its\nextensions include multiple linear\nregression that is finding a plane of\nbest fit and polynomial regression that\nis finding a curve for best fit next one\ndecision tree it looks something like\nthis where each square above is called a\nnode and the more nodes you have the\nmore accurate your decision tree will be\nin general next and the third type\nrandom forest\nthese are assemble learning techniques\nthat builds off over decision trees and\ninvolve creating multiple decision trees\nusing bootstrap data sets of original\ndata and randomly selecting a subset of\nvariables at each step of the decision\ntree the model then selects the mode of\nall the predictions of each decision\ntrees and by relying on the majority\nwinds model it reduces the risk of error\nfrom individual tree next neural network\nit is quite popular and is a multi\nlayered model inspired by human minds\nlike the neurons in our brain the circle\nrepresents a node the blue circle\nrepresents an input layer the black\ncircle represents a hidden layer and the\ngreen circle represents the output layer\neach node in the hidden layer represents\na function that input goes through\nultimately leading to the output in the\ngreen circles\nnext classification so with regression\ntypes being over now let's jump to\nclassification so in classification the\noutput is discrete some of the most\ncommon types of classification models\ninclude first logistic regression which\nis similar to linear regression but is\nused to model the probability of a\nfinite number of outcomes typically two\nnext support vector machine it is a\nsupervised classification technique that\ncarries an objective to find a hyper\nlane in n-dimensional space that can\ndistinctly classify the data points next\nnavies it's a classifier which acts as a\nprobabilistic machine learning model\nused for classification tasks the crux\nof the classifier is based on the Bayes\ntheorem coming up next\ndecision trees random forests and neural\nnetworks these models follow the same\nlogic as previously explained the only\ndifference here is that the output is\ndiscrete rather than continuous now next\nlet's jump over to unsupervised learning\nunlike supervised learning unsupervised\nlearning is used to draw inferences and\nfind patterns from input data without\nreferences to the labeled outcome two\nmain methods used in supervised learning\ninclude clustering and dimensionality\nreduction clustering involves grouping\nof data points it's frequently used for\ncustomer segmentation fraud detection\nand document classification common\nclustering techniques include k-means\nclustering hierarchical clustering means\nshape clustering and density based\nclustering while each technique has\ndifferent methods in finding clusters\nthey all aim to achieve the same thing\ncoming up next dimensionality reduction\nit is a process of reducing dimensions\nof your feature set Auto States simply\nreducing the number of features most\ndimensionality reduction techniques can\nbe categorized as either feature\nelimination or feature extraction a\npopular method of dimensionality\nreduction is called principal component\nanalysis or PCA obviously there's a ton\nof complexity if we dive into any\nparticular model to help you with each I\nwill be publishing new videos so be sure\nto smash that subscribe button to be\nnotified on every upload next if this\nvideo helped you be sure to like it and\nshare it with someone who might need it\n[Music]"
    },
    {
        "topic": "Applications of Machine Learning",
        "url": "https://www.youtube.com/watch?v=ukzFI9rgwfU",
        "transcript": "we know humans learn from their past\nexperiences\nand machines follow instructions given\nby humans\nbut what if humans can train the\nmachines to learn from the past data and\ndo what humans can do and much faster\nwell that's called machine learning but\nit's a lot more than just learning it's\nalso about understanding and reasoning\nso today we will learn about the basics\nof machine learning\nso that's paul he loves listening to new\nsongs\nhe either likes them or dislikes them\npaul decides this on the basis of the\nsong's tempo\ngenre\nintensity and the gender of voice for\nsimplicity let's just use tempo and\nintensity for now so here tempo is on\nthe x axis ranging from relaxed to fast\nwhereas intensity is on the y axis\nranging from light to soaring we see\nthat paul likes the song with fast tempo\nand soaring intensity while he dislikes\nthe song with relaxed tempo and light\nintensity so now we know paul's choices\nlet's say paul listens to a new song\nlet's name it as song a song a has fast\ntempo and a soaring intensity so it lies\nsomewhere here looking at the data can\nyou guess whether paul will like the\nsong or not correct so paul likes this\nsong by looking at paul's past choices\nwe were able to classify the unknown\nsong very easily right let's say now\npaul listens to a new song let's label\nit as song b so song b\nlies somewhere here with medium tempo\nand medium intensity neither relaxed nor\nfast neither light nor soaring now can\nyou guess whether paul likes it or not\nnot able to guess whether paul will like\nit or dislike it are the choices unclear\ncorrect we could easily classify song a\nbut when the choice became complicated\nas in the case of song b yes and that's\nwhere machine learning comes in let's\nsee how in the same example for song b\nif we draw a circle around the song b we\nsee that there are four votes for like\nwhereas one would for dislike if we go\nfor the majority votes we can say that\npaul will definitely like the song\nthat's all this was a basic machine\nlearning algorithm also it's called k\nnearest neighbors so this is just a\nsmall example in one of the many machine\nlearning algorithms quite easy right\nbelieve me it is but what happens when\nthe choices become complicated as in the\ncase of song b that's when machine\nlearning comes in it learns the data\nbuilds the prediction model and when the\nnew data point comes in it can easily\npredict for it more the data better the\nmodel higher will be the accuracy there\nare many ways in which the machine\nlearns it could be either supervised\nlearning unsupervised learning or\nreinforcement learning let's first\nquickly understand supervised learning\nsuppose your friend gives you one\nmillion coins of three different\ncurrencies say one rupee one euro and\none dirham each coin has different\nweights for example a coin of one rupee\nweighs three grams one euro weighs seven\ngrams and one dirham weighs four grams\nyour model will predict the currency of\nthe coin here your weight becomes the\nfeature of coins while currency becomes\nthe label when you feed this data to the\nmachine learning model it learns which\nfeature is associated with which label\nfor example it will learn that if a coin\nis of 3 grams it will be a 1 rupee coin\nlet's give a new coin to the machine on\nthe basis of the weight of the new coin\nyour model will predict the currency\nhence supervised learning uses labeled\ndata to train the model here the machine\nknew the features of the object and also\nthe labels associated with those\nfeatures on this note let's move to\nunsupervised learning and see the\ndifference suppose you have cricket data\nset of various players with their\nrespective scores and wickets taken when\nyou feed this data set to the machine\nthe machine identifies the pattern of\nplayer performance so it plots this data\nwith the respective wickets on the\nx-axis while runs on the y-axis while\nlooking at the data you'll clearly see\nthat there are two clusters the one\ncluster are the players who scored\nhigher runs and took less wickets while\nthe other cluster is of the players who\nscored less runs but took many wickets\nso here we interpret these two clusters\nas batsmen and bowlers the important\npoint to note here is that there were no\nlabels of batsmen and bowlers hence the\nlearning with unlabeled data is\nunsupervised learning so we saw\nsupervised learning where the data was\nlabeled and the unsupervised learning\nwhere the data was unlabeled and then\nthere is reinforcement learning which is\na reward based learning or we can say\nthat it works on the principle of\nfeedback here let's say you provide the\nsystem with an image of a dog and ask it\nto identify it the system identifies it\nas a cat so you give a negative feedback\nto the machine saying that it's a dog's\nimage the machine will learn from the\nfeedback and finally if it comes across\nany other image of a dog it will be able\nto classify it correctly that is\nreinforcement learning to generalize\nmachine learning model let's see a\nflowchart input is given to a machine\nlearning model which then gives the\noutput according to the algorithm\napplied if it's right we take the output\nas a final result else we provide\nfeedback to the training model and ask\nit to predict until it learns i hope\nyou've understood supervised and\nunsupervised learning so let's have a\nquick quiz you have to determine whether\nthe given scenarios uses supervised or\nunsupervised learning simple right\nscenario one facebook recognizes your\nfriend in a picture from an album of\ntagged photographs\nscenario 2 netflix recommends new movies\nbased on someone's past movie choices\nscenario 3 analyzing bank data for\nsuspicious transactions and flagging the\nfraud transactions think wisely and\ncomment below your answers moving on\ndon't you sometimes wonder how is\nmachine learning possible in today's era\nwell that's because today we have\nhumongous data available everybody is\nonline either making a transaction or\njust surfing the internet and that's\ngenerating a huge amount of data every\nminute and that data my friend is the\nkey to analysis also the memory handling\ncapabilities of computers have largely\nincreased which helps them to process\nsuch huge amount of data at hand without\nany delay and yes computers now have\ngreat computational powers so there are\na lot of applications of machine\nlearning out there to name a few machine\nlearning is used in healthcare where\ndiagnostics are predicted for doctor's\nreview the sentiment analysis that the\ntech giants are doing on social media is\nanother interesting application of\nmachine learning fraud detection in the\nfinance sector and also to predict\ncustomer churn in the e-commerce sector\nwhile booking a gap you must have\nencountered surge pricing often where it\nsays the fair of your trip has been\nupdated continue booking yes please i'm\ngetting late for office\nwell that's an interesting machine\nlearning model which is used by global\ntaxi giant uber and others where they\nhave differential pricing in real time\nbased on demand the number of cars\navailable bad weather rush r etc so they\nuse the surge pricing model to ensure\nthat those who need a cab can get one\nalso it uses predictive modeling to\npredict where the demand will be high\nwith the goal that drivers can take care\nof the demand and search pricing can be\nminimized great hey siri can you remind\nme to book a cab at 6 pm today ok i'll\nremind you\nthanks no problem comment below some\ninteresting everyday examples around you\nwhere machines are learning and doing\namazing jobs so that's all for machine\nlearning basics today from my site keep\nwatching this space for more interesting\nvideos until then happy learning"
    },
    {
        "topic": "Data Gathering and Cleaning",
        "url": "https://www.youtube.com/watch?v=jxq4-KSB_OA",
        "transcript": "in this video we'll go from having a raw data cell\u00a0\nlike this to converting it into a clean Excel file\u00a0\u00a0\nin just 10 steps so let's get into it and you can\u00a0\ndownload this exact same Excel file in the video\u00a0\u00a0\ndescription for free so over here you can see that\u00a0\nwe have the data set this is the row one basically\u00a0\u00a0\nbut before we make any changes to it we should\u00a0\nprobably save a copy so you can either save a\u00a0\u00a0\nnew Excel file or just save this sheet by clicking\u00a0\nthe control key and just dragging that sheet so\u00a0\u00a0\nyou have two separate ones alright now we can get\u00a0\nstarted with step 1 out of 10 and first you can\u00a0\u00a0\nsee here that these columns are just too short if\u00a0\nI stretch them out the number looks okay but if I\u00a0\u00a0\nclose it down you get this sign over here same\u00a0\nthing with the rows some rows are just too long\u00a0\u00a0\nlike this one right over here so let's go ahead\u00a0\nand refit this by just going to control a that's\u00a0\u00a0\ngoing to select the entire table for us and then\u00a0\nwe'll go over to the side where it says format\u00a0\u00a0\nand you can autofit the row width and\u00a0\nthe column width now the shortcut for\u00a0\u00a0\nthese is just alt h o i that's gonna\u00a0\nauto fit the columns and then alt H\u00a0\u00a0\nO A to Auto filter rows now you can see\u00a0\nthat we can read everything a lot better\u00a0\u00a0\nnow that we can start to see the data set a bit\u00a0\nbetter you notice that the client names are very\u00a0\u00a0\nvery long so let's suppose that we would like\u00a0\nto shorten it such that we don't have anything\u00a0\u00a0\nwithin these parenthesis so for this what we can\u00a0\ndo is select the whole column you can just click\u00a0\u00a0\nup over here and then we're gonna go to the\u00a0\nreplace tool so that's going to be over to the\u00a0\u00a0\nside under find and select and just clicking on\u00a0\nreplace there the shortcut for it is simply Ctrl H\u00a0\u00a0\nand So within it what we want to find is anything\u00a0\nwithin the parenthesis but what makes it hard here\u00a0\u00a0\nis that there's sometimes of different lengths so\u00a0\nwhat we can do is just put the parenthesis sign\u00a0\u00a0\nuse this asterisk and close apprentices that\u00a0\nbasically means that anything that's within a\u00a0\u00a0\nparenthesis is going to remove so replace with\u00a0\nnothing then we're gonna hit on replace all\u00a0\u00a0\nhit OK there and just close and now you can\u00a0\nsee that's looking a lot better we can double\u00a0\u00a0\nclick here to resize continuing on with the client\u00a0\nnames and let's suppose that we want this to be in\u00a0\u00a0\nlowercase so they're a bit easier to read what we\u00a0\ncan do is First add a new column so we'll go ahead\u00a0\u00a0\nand select this column D and just hit Ctrl shift\u00a0\nplus and now that's a new column for us we'll call\u00a0\u00a0\nthis a client again and then the function that\u00a0\nwe're going to use is called Lower so equals\u00a0\u00a0\nlower and it converts all the letters and look\u00a0\ninto lowercase so we're gonna go top there and\u00a0\u00a0\nwanna convert all of these letters over here\u00a0\nso we'll just hit enter there on the first one\u00a0\u00a0\nand then we can double click here on the side to\u00a0\njust drag it down all the way to the bottom there\u00a0\u00a0\nnow because we have the same column twice we could\u00a0\njust delete this one so hitting Ctrl minus but the\u00a0\u00a0\nproblem there is that it all breaks and the reason\u00a0\nfor that is because it's referencing this column\u00a0\u00a0\nover here so first we're gonna need to paste these\u00a0\nas values instead of as a formula as they are now\u00a0\u00a0\nso we'll go Ctrl shift down to select all of them\u00a0\nCtrl C and then we're gonna paste these as values\u00a0\u00a0\nso down over here you can see that we have a lot\u00a0\nof different pasting features and we want to paste\u00a0\u00a0\nit as a value which is this one right here the\u00a0\nshortcut there is alt H VV now we can go ahead\u00a0\u00a0\nand remove this column so Ctrl minus next up\u00a0\nin number four you can see that we have the\u00a0\u00a0\ndifferent contacts but the problem is here they\u00a0\nhave some very odd spacing and sometimes they're\u00a0\u00a0\ncapitalized sometimes they're not so let's go\u00a0\nahead and make that change so we'll go ahead and\u00a0\u00a0\ninsert a new column here so control space Ctrl\u00a0\nshift plus we'll call this one contact again\u00a0\u00a0\nand so first let's say we want to remove the\u00a0\nspacing we can simply use the trim function\u00a0\u00a0\nhit the top key there and we'll just select\u00a0\nthis and you can see that it's removed all\u00a0\u00a0\nthat spacing problem and similarly\u00a0\nwe have what's known as equals proper\u00a0\u00a0\nhit the top key there and just select that and\u00a0\nwhat this one does is that it only puts a capital\u00a0\u00a0\nletter at the first letter of each word so we can\u00a0\nnow just merge these two together so we've got the\u00a0\u00a0\nproperty there but at the very front we're gonna\u00a0\nadd the trim hit the top key there and then we\u00a0\u00a0\nneed to close the parenthesis for both and just\u00a0\nhit enter now you can see that it looks cleaned\u00a0\u00a0\nup and we'll just double click there to drag it\u00a0\ndown and then again we need to paste it as a value\u00a0\u00a0\nso we'll select the column by hitting Ctrl space\u00a0\nCtrl C and then alt H VV that's going to paste it\u00a0\u00a0\nas a value there now we can remove this column\u00a0\nover here by hitting Ctrl minus and if you're\u00a0\u00a0\nliking this video and want to level up your Excel\u00a0\nskills you can consider checking out our Excel for\u00a0\u00a0\nbusiness and finance course and what makes this\u00a0\ncourse different is that it's all applied to the\u00a0\u00a0\nreal world while we still cover theoretical\u00a0\nlessons like formatting formulas and charts\u00a0\u00a0\nwe also offer case studies that simulate the type\u00a0\nof work you might be assigned in your day-to-day\u00a0\u00a0\nranging from Financial modeling to cleaning a\u00a0\ndata set and presenting some visual insights\u00a0\u00a0\nand if you get stuck along the way you can always\u00a0\nask us the course instructors any questions on\u00a0\u00a0\nthe discussions Forum we also offer several other\u00a0\ncourses including power bi VBA and macros and more\u00a0\u00a0\nso if you're interested in checking it out go to\u00a0\nthe link in the description below alright back\u00a0\u00a0\nto the video moving up to step 5 and you can see\u00a0\nover here that the department is split into the\u00a0\u00a0\ndepartment name and the region so we would like\u00a0\nto separate this into two different columns so\u00a0\u00a0\nlet's go ahead and add a new one just by hitting\u00a0\nCtrl shift plus and let's call this one the region\u00a0\u00a0\nfrom here what we can use is this trick called\u00a0\ntext columns so first let's select the whole\u00a0\u00a0\narea there so Ctrl shift down to select all\u00a0\nof these then we're gonna head over to data\u00a0\u00a0\ntext to columns so this one right here\u00a0\nand then we're just gonna hit on next\u00a0\u00a0\nand here you can see basically this is what\u00a0\nit's going to separate each of these columns\u00a0\u00a0\nby and in our case it's that underscore so we\u00a0\nwant to head over to other and just type an\u00a0\u00a0\nunderscore in there and you'll see under preview\u00a0\nwhat that currently looks like would hit on OK\u00a0\u00a0\nthere and then the destination where we want the\u00a0\noutput to be we want it to be right here in E3\u00a0\u00a0\nonce we're all okay with it we're just gonna\u00a0\nhit on finish there and you can see that it's\u00a0\u00a0\nsplit it into two separate columns now in Step six\u00a0\nbefore we move any further we should try to check\u00a0\u00a0\nif there's any duplicate values so for this we\u00a0\ncan select the whole table by hitting Ctrl a and\u00a0\u00a0\nthen we're gonna go over to data again and it's\u00a0\ngonna be this icon right here that says remove\u00a0\u00a0\nduplicates so just click on that we're okay with\u00a0\nthese our data does have headers and just hit on\u00a0\u00a0\nOK and it says that it's found three duplicates\u00a0\nand it's removed them so that looks all cool\u00a0\u00a0\nalright now in number seven let's move on to the\u00a0\nnext column which is going to be the payments one\u00a0\u00a0\nin this case and you can see here that we have\u00a0\nsome issues in that there's some blank cells now\u00a0\u00a0\nfor this it would be nice to not have it as blank\u00a0\nand maybe type something like an N A but suppose\u00a0\u00a0\nthis is a very long data set and so it's going\u00a0\nto take a lot of time to go one by one instead\u00a0\u00a0\nwhat we can do is select the whole table so Ctrl\u00a0\na then under home we're gonna go over to find and\u00a0\u00a0\nselect go to special and within this we want to\u00a0\nselect all of the blank cells so we're going to\u00a0\u00a0\ngo over to Blanks and just hit on OK and you'll\u00a0\nnotice there that it selected the blank ones\u00a0\u00a0\nfor us from here we want to add an N A Sign so\u00a0\nwe'll go up over here under the formula bar just\u00a0\u00a0\ntype an A and then it's very important that we\u00a0\nhit the control enter and not just enter there\u00a0\u00a0\nand you can see there that it's\u00a0\nchanged all of these into n a for us\u00a0\u00a0\ngreat now moving on to step 8 and over here under\u00a0\ncolumn J you'll notice that we have this formula\u00a0\u00a0\nwhich is simply The Profit divided by the revenue\u00a0\nthe problem is down over here because we've got\u00a0\u00a0\ntext on one side it doesn't quite work and it\u00a0\ngives us this error sign so to work around that\u00a0\u00a0\nwe're gonna use the if error formula so right\u00a0\nhere at the very front we're gonna type if error\u00a0\u00a0\nhit the top key there and the idea here is that\u00a0\nif there is no error it's gonna do the normal\u00a0\u00a0\ncalculation we'll hit the comma there now if there\u00a0\nis an error we need to give it an alternate result\u00a0\u00a0\nwhich in our case is the n a which we need\u00a0\nto put in quotations because it's a formula\u00a0\u00a0\nso we're gonna put it in there\u00a0\nclose the parenthesis and hit enter\u00a0\u00a0\nand then we're gonna drag that down awesome now\u00a0\nwe can see how those signs have changed to an\u00a0\u00a0\na for us moving on to step 9 and now that we're\u00a0\nhappy with the values let's go ahead and format\u00a0\u00a0\nthe header row so we'll select it by hitting Ctrl\u00a0\nshift right let's say we want a bold end so Ctrl B\u00a0\u00a0\nand we can also change the highlight colors\u00a0\nlet's say we go for a dark blue and we change\u00a0\u00a0\nthis over here to a white so we can see the\u00a0\nheader there and finally in Step 10 we can get\u00a0\u00a0\nrid of these grid lines as you can sometimes be a\u00a0\nbit distracting we'll head over to view there and\u00a0\u00a0\njust click on gridlines for that the shortcut is\u00a0\nalt w v g that's going to activate it again for\u00a0\u00a0\nme so let me just remove them there awesome now\u00a0\nthat we've cleaned the data The Next Step would\u00a0\u00a0\nbe to create some visuals out of it and you can\u00a0\nlearn how to do that with this video over here or\u00a0\u00a0\nby taking our course over here hit that like and\u00a0\nthat subscribe and I'll catch you in the next one"
    },
    {
        "topic": "Feature Engineering",
        "url": "https://www.youtube.com/watch?v=xhB-dmKmzRk",
        "transcript": "hello all my name is krishnak and\nwelcome to my youtube channel so guys\ntoday in this particular video we are\ngoing to discuss what all steps we\nactually perform in what kind of order\nto complete the feature engineering\nprocess\nnow in a data science project guys if i\njust consider feature engineering it\ntakes somewhere around\n30 percentage of the entire project time\nright 30 so it is very very huge you\nknow and there are many many people who\nhave asked me questions like krish what\nis the exact order see after i get the\ndata the raw data what should i first do\nyou know and if you remember in the life\ncycle of a data science project the\nfirst module that actually comes is\nfeature engineering and then after that\nfeature selection then you have model\ncreation then you have model deployment\nthen you have hyper parameter tuning\nbefore model deployment you have hyper\nparameter tuning then you also have\nincremental learning and there are many\nmore steps as such but the most\nimportant the crux the backbone of the\nentire data science project is feature\nengineering because you will be cleaning\nthe data you will be doing a lot of\nsteps so let me talk about every step\nthat you may be performing in future\nengineering step by step okay so the\nfirst step that i want to discuss about\nis i'm just going to write it down the\nstop one is basically eda\nnow eda is nothing but\nexploratory data analysis\nexploratory data analysis\nnow this is a very very important thing\nand remember guys in my youtube channel\ni have created dedicated playlist on\nfeature engineering on eda everything so\ni'll also be giving those entire link at\nthe last okay so first step step one is\nbasically eda that is exploratory data\nanalysis\nnow you may be thinking okay fine\nexploratory data analysis is it only\nabout data analysis no there are many\nmany steps that we actually perform in\nthis so let me write it down one by one\nin eda as soon as we get the raw\ndata\nokay as soon as we get the raw data\nbecause\nthe entire feature engineering is\nactually done on the raw data itself\nright so as soon as we get the raw data\nwhat do we do we start doing the\nanalysis now what kind of analysis we do\nfirst of all i i'll just give you one\nexample first of all what i actually\nfollow as soon as i get the data i\nbasically see that how many numerical\nfeatures may be there\nokay how many numerical features\nmay be there\nright\nthen i may go up with how many\ncategorical or discrete categorical\nfeatures may be there\ncategorical features\ni may i may try to see this numerical\nfeatures i'll try to dis\ndefine or draw different different\ndiagrams like histogram\nright like pdf function\nright and obviously you know all these\nthings you can use libraries like c bond\nright i hope everybody's familiar with c\nbond we use c bond you know you can also\nuse matplotlib to see all this kind of\ndiagrams right and then in the category\nfeatures you'll try to analyze the\ncategory features like how many category\nfeatures may be there you know in those\nfeatures how many categories maybe there\nis there multiple categories see all\nthis observation is actually necessary\nyou know\nall this observation is basically very\nvery much necessary okay now coming to\nthe third step that i will definitely\nfollow i'll just try to see whether\nthere is any missing values\ni will just try to clearly draw\nclearly draw\nvisual and i'll just say i'll try to\nvisualize all these graphs\nvisualize all these graphs\nyou know with the help of missing values\nalso if there is any missing values i'll\ntry to see you know probably uh i may go\nwith my fourth step i'll try to see\nwhether there are outliers and how do\nyou draw an outlier simple box plot\nright box plot i'll go with box plot\ni'll try to see whether there is any\noutliers now these observations are very\nmuch necessary because whatever diagrams\nyou are actually drawing this all needs\nto be sent to your manager\nto your analytics manager\nbecause that is what you have done in\nthe eda right and this is just a\nthe first step in the entire feature\nengineering and trust me there are many\nmore steps which i will be telling you\nin just a while right outliers missing\nvalues category features numerical\nfeatures and you know there are various\nthere are three to four different types\nof uh handling missing values missing\nvalues will be because of different\ndifferent reasons and based on that you\nhave to act accordingly right so you\nhave outliers probably you know you'll\ntry to see\nwhether the raw data needs cleaning also\nor not\ncleaning or not right so this this is\nalso very important step the raw data\nmay have many information in just one\nfeature and out of that if you wrote if\nyou require all those information or not\nright but again understand the main step\nover here what we are trying to do we\nare trying to convert\nthe raw\ndata into useful data\ninto useful data\nso that\nour ml algorithms will be able to\ningest them properly\ningest them\nfor\ngiving amazing predictions right so in\nthe eda part we see all these things\nright\nnow let's come to the second step very\nvery simple now in the second step what\ni always do\nis that\ni start handling the missing values\ni start handling the missing values very\nvery important\nthere are various ways of handling the\nmissing values you may be saying okay\nkrish we may use mean\nmedian\nmore right all these things\nright\nnot only this guys not only mean medium\nmode i'll try to analyze those features\ni'll try to see whether there is an\noutlier in that particular feature this\nthree just one some of the three steps\nand we have lot of various modes\na lot of various ways to handle the\nmissing values right mean median mode\nare one of them you know i may replace\nsome of the features by considering some\ndifferent different techniques also and\nthe entire details is mentioned in my\nfeature engineering playlist again\nfeature engineering playlist\nyou know i may analyze it i may i may\ncreate a lot of box plots to see okay if\ni'm utilizing iqr in removing you know\nif you remember there is a formula with\nrespect to iqr also to remove the\noutliers and after handling the outliers\nwhat i'll do i'll try to handle the\nmissing values by median in short if you\ndon't want the impact of\nthe outliers you can directly use median\nor mode right\nso this is basically about the second\nstep the third step what i do is that\nyou know\nstep step three\nso in the step three what i can actually\ndo is that handling imbalance data set\nyou know this is also a very very\nimportant step\nbecause not all the machine learning\nalgorithms works well with an imbalanced\ndata set right you may get a very bad\naccuracy and you may be thinking that\nokay you have got amazing accuracy but\nbecause of the imbalanced data set you\nmay get a very bad one right now the\nfourth one that i would like to do is\nthat treating the outliers\nright\nthis is also very much important step\nokay there are various there are two to\nthree ways to handle the outliers also\nwhich you should definitely explore i'm\njust telling you step by step whatever i\ndo i'll basically use this and before\nall the uh one more step that i can\nactually do is scaling the data\nright scaling the data\nin the same scale we use different\ndifferent process like standardization\nright standardization\nnormalization\nright all these techniques we actually\nused in feature engineering right coming\nto the sixth step uh this is very very\nmuch important that is converting your\ncategorical features\nconverting the categorical features\ncategorical features\ninto numerical features right\nthis is the most important step\nnumerical features one example i'll tell\nyou suppose you have an example like pin\ncode in pin code you have different\ndifferent different values right and\nhere you have so many features so many\nunique categories so what technique you\nmay probably use in order to convert\nthis categorical features into numerical\nfeatures and probably you have to\nactually use this right now coming to\nthe uh next step let's see all these\nthings what what by by this all these\nthings what we are actually doing see uh\nif i go from step one eda step two\nhandling the missing values handling\nimbalance data set treating the outliers\nscaling the data scaling down the data\njust write it scaling down the data\nright\nand then converting the category\nfeatures into numerical features once i\nperform all the steps what i think is\nthat yes\nfeature engineering is about 90\ncompleted and don't think that you'll\njust be able to do it this in one day or\ntwo day if you have a small data set\nobviously i'll say that you will be able\nto do it in three to four hours but\nunderstand i have worked with data set\nwhere you have one million records\nright and for doing all these things it\ntakes time right always make sure that\nyou follow this process you always\nremember the steps okay this is very\nvery much important let me just check\nout if i have missed any um\nscaling down category for outlier\ntreatment everything is mentioned over\nhere very clearly okay so these are most\nof the steps that we do in the future\nengineering and till here right from\nhere to here\nso what has happened now see why feature\nengineering is important let me talk\nabout it\nthe raw data\nthe raw data in this raw data you'll\nhave so many problems you'll be having\nright it'll probably the json format\nprobably it may be not having uh proper\nfeatures it may be\nnot in the proper format you know\nthere may be many things right\nthis raw data\nafter this entire process of feature\nengineering\nyou will be having this clean data\nand this d clean data will now be given\nto your\nml models\nfor the further\ntraining purpose\nnow when you have the clean data and\nyou're giving your model to for the\ntraining purpose obviously your model is\ngoing to give you better results there\nis one more step after feature\nengineering which is called as feature\nselection feature selection is pretty\nmuch simple guys in future selection\nwhat we do\nwe\nselect only those features that are\nimportant now let me tell you that if\nthere are thousand features in your data\nset\nright and out of this entire feature out\nof all these thousand features it is not\nnecessary that all the thousand features\nare required\nyou know and if you have that many\nnumber of features there is also a term\nwhich is called as curse of\ndimensionality\nright and this usually happen when you\nhave many many features\nit is also a curse so we should take\nthose features that are very important\nand in future selection what are steps\nwe do let me write it down over here\nin future selection what are steps we\nactually do\nin feature selection\nin feature selection we perform various\nsteps right if you remember\nright you have correlation\nright you have\num one step is basically correlation\nand if i talk about more uh you also\nhave k neighbors you can use k neighbors\nfor the future selection purpose you\nhave chi square\nright\nyou you have chi square you have genetic\nalgorithms\nfor doing this right genetic algorithms\nfor doing this uh you have something\ncalled as feature importance\nright dissolved techniques are there\nfeature importance internally uses extra\ntree classifier here specifically you\nuse something called as extractory\nclassifier\nright all these steps\nand i've uploaded videos on this also\nright so all these steps is basically\nused for selecting selecting the best\nfeatures\nright selecting the best features\nokay\nnow see\nthis is the most important step and\nagain if you are\nhaving any confusion with respect to\nanything what i'll do is that just just\nopen the youtube channel okay go to this\ntwo playlist one is chris eda\nokay so here is your exploratory data\nanalysis playlist you can go and have a\nlook on to this here in the same steps i\nhave explained everything if you go and\ncheck out this entire playlist right in\nthe same step eda\nthen feature engineering and then\nfeature selection\nin the same step i've actually explained\neverything and here i've also explained\nthe automated eda part okay so it will\nbe very much easy this is the one\nplaylist and the other playlist is\nbasically about the feature engineering\nso this too\nis must trust me because 30 percentage\nof the time and here all the other\ndifferent different types of feature\nengineering how do we handle category\nfeatures how do we handle missing values\nsee three days four days on handling\nmissing values only has been explained\nyou know how to handle category features\neverything is being explained what is\nstandardization transformation\neverything is explained in this handling\nmissing data and even outlines all these\nthings has been explained right so my\nsuggestion would be that go ahead have a\nlook onto this and yes uh if you like\nthis particular video please do make\nsure that you subscribe the channel\npress the bell notification icon but\nunderstand feature engineering is a very\nimportant step all together i'll see you\nall in the next video have a great day\nthank you and all bye"
    },
    {
        "topic": "Data Splitting",
        "url": "https://www.youtube.com/watch?v=smmFI8WGDDs",
        "transcript": "[Music]"
    },
    {
        "topic": "Linear Regression",
        "url": "https://www.youtube.com/watch?v=lzGKRSvs5HM",
        "transcript": "Error fetching transcript for video lzGKRSvs5HM: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=lzGKRSvs5HM! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
    },
    {
        "topic": "Simple Linear Regression",
        "url": "https://www.youtube.com/watch?v=zPG4NjIkCjc",
        "transcript": "this tutorial is an introduction to\nregression there is an X variable and a\nY variable in this case\nthe independent variables on the x-axis\nand the dependent variable is on the\ny-axis and we try to form a relationship\nbetween these two variables and draw a\nline in this case a straight line and\nover the next series of videos I'll\nexplain what all this means what we try\nto understand is as the independent\nvariable is moving or changing what\nhappens to the dependent variable does\nit go up or does it go down how does it\nchange\nif they move in the same direction if\nthe independent variable increases and\nthe dependent variable increases as well\nlike this we say there's a positive\nrelationship if on the other hand as the\nindependent variable increases and the\ndependent variable decreases like this\nwe say there's a negative relationship\nthe line would look like this\ngo downward in the linear regression we\ntry to make a line a line to make a\nlinear regression the key is on line\nright there a straight line you can also\ndo curved lines but for the this topic\nis all straight lines to actually\nconduct regression I take observations\nand always plot some more observations\nin your random play I'll stick them in\nhere like that and I try to find a line\nthat will fit a straight line that fits\nthrough all these different points and\nthis is called my regression line and\nit's based upon the least squares method\nand in the end I want to minimize the\ndifference between the estimated value\nand the actual value I want to minimize\nmy error errors this line will have a\nlot of errors if I compare the actual to\nthe estimated value and again the point\nis to minimize these errors or make them\nas small as possible now let's imagine I\nput study time on the x-axis or make\nthat my independent variable and the\ndependent variable becomes grades or GPA\nas study time increases grades should go\nup there is a positive relationship in\nregression we develop these equations\nlike this in this case y hat is\nestimated grades and it's based upon or\nit's equal to B naught plus B 1 times\nX where X is study time be not we derive\nmathematically and it is the y-intercept\nb1 we also derive mathematically and\nI'll do in a later video and it's the\nslope of the line in this case the slope\nis positive in the next video I'll\ndiscuss how you develop these equations\nnow if I change the x-axis to time on\nface book we see a negative relationship\nmore time on face book grades will\nsuffer and go down a negative\nrelationship what we're estimating is\nstill grades estimated grades is equal\nto B naught minus B 1 times X where X is\ntime on Facebook B naught is still the y\nintercept the y-intercept and it is a\ncalculated value the slope of the line\nis negative B 1 because it's downward\nsloping negative relationship and as I\nsaid before all show you how to\ncalculate this equation in the next\nvideo\nthe X is the independent variable the Y\nis the dependent variable the X is what\nwe control what we manipulate what we\nchange\nand the dependent variable is the\noutcome\nso study time is the independent\nvariable is what we control and\nmanipulate and your grades are dependent\nupon how much you study now this looks\nreally ugly and it's what I'll talk\nabout in the next video but I'll step\nyou step-by-step through it and\nhopefully make it simple for you\nyou"
    },
    {
        "topic": "Multiple Linear Regression",
        "url": "https://www.youtube.com/watch?v=dQNpSa-bq4M",
        "transcript": "- [Brandon] Now as I'm sure\nyou know and experience,\nthe world is a very complex place.\nSo when we're looking to predict the value\nof a variable, oftentimes we\ncan get better predictions\nif we use more than one other variable\nto make that prediction and that leads us\nto multiple regression.\nNow I am going to assume\nyou have some familiarity\nand some comfort with\nsimple linear regression\nwhich we covered in the previous series.\nSo if you're still a\nbit shaky on just simple\nlinear regression, I\nwould go back, review that\nand then come back to this series.\nSo without further ado, let's go ahead\nand get to learning.\nSo as always, let's\nstart out with a problem\nand a dataset that we're gonna use\nfor the next several videos.\nAnd this is called the\nregional delivery service.\nSo let's assume that you\nare a small business owner\nfor Regional Delivery\nService, Incorporated, or RDS\nfor short, who offers\nsame-day delivery for letters,\npackages, and other small cargo.\nYou are able to use Google Maps\nto group individual\ndeliveries into one trip\nto reduce time and fuel\ncosts, just like UPS would\nor FedEx or the Postal Service does.\nTherefore some trips will\nhave more than one delivery.\nNow as the owner, you would\nlike to be able to estimate\nhow long a delivery will\ntake based on two factors,\none, the total distance\nof the trip in miles\nand two, the number of\ndeliveries that must be made\nduring that trip.\nSo we're looking to estimate\nhow long a delivery trip\nwill take based on the\ndistance and the number\nof deliveries during that\ntrip, so two factors.\nSo to conduct your analysis\nyou take a random sample\nof 10 past trips and record\nthree pieces of information\nfor each trip, one, the\ntotal miles traveled,\ntwo, the number of\ndeliveries during that trip\nand three, the total travel time in hours,\nwhich is what we're trying to predict.\nSo you make a table that looks like this.\nSo we have miles traveled, num deliveries,\nwhich is the number of deliveries,\nand then travel time\nin hours along the top.\nWe have labeled them X1, X2 and Y.\nNow X1 and X2 are special\ntypes of variables\nwe'll discuss here in a minute.\nAnd Y is a distinct variable,\nwe'll talk about here\nin a minute.\nSo we can see the first\ntrip we traveled 89 miles.\nWe had four deliveries on that trip,\nand the total time was seven hours.\nThe second trip was 66 miles.\nWe only had one delivery and\nthe travel time was 5.4 hours.\nSo here are our 10 trips.\nNow remember that in this\ncase, you would like to be able\nto predict the total travel\ntime, so that's the right column\nso in the orange-brown color there,\nusing both the miles traveled,\nwhich X1, the first column,\nand number of deliveries,\nwhich is the second column X2\nof each trip.\nSo the question is, in what\nway does travel time depend\non the first two measures, miles traveled\nand number of deliveries.\nSo travel time is the dependent variable\nand miles traveled and\nnumber of deliveries\nare independent variables.\nNow one note here, some\nprefer predictor variables\nand response variable\ninstead of independent\nand dependent variables respectively.\nSince most stats textbooks\nuse independent variable\nand dependent variable, I\nam going to stick to that.\nBut I do subscribe to the\ncase of predictor variables\nand response variable.\nJust keep in mind that\ndepending on the textbook\nyou're using and your\nprofessor and things,\nyou may hear both or one or the other.\nSo what about multiple regression?\nSo multiple regression\nis just an extension\nof simple linear regression,\nagain which we talked about\nin the last series.\nSo remember in simple linear\nregression, we have a one\nto one relationship.\nSo we have a dependent\nvariable and we're going\nto utilize an independent\nvariable to explain\nthe variation in that dependent variable\nor make predictions about\nthat dependent variable.\nNow in multiple regression, we have a many\nto one relationship.\nSo we still have one dependent variable\nbut we can have two or more.\nSo in this case we have four on the screen\nbut we can have more than that or just two\nor three or whatever independent variables\nthat are all being utilized\nto explain the variation\nor predict the value of\nthe dependent variable.\nSo we go from a one to one relationship,\none independent to one\ndependent to two or more\nindependent variables and\none dependent variable.\nNow having more independent\nvariables complicates\nthings a bit.\nSo we have to have some\nnew things to consider.\nThe first is that adding\nmore independent variables\nto a multiple regression\nprocedure does not mean\nor necessarily mean the\nregression will be better\nor offer better predictions.\nIn fact, doing so can\nactually make things worse.\nThis is called overfitting.\nSo let's say we conduct a\nmultiple regression procedure\nand our model explains\n65% of the variation\nin the dependent variable.\nWell for some reason we don't like that.\nWe think well, we can do better than that.\nSo we start adding in more\nindependent variables.\nWell adding more independent\nvariables will explain\nmore of the variation in\nthe dependent variable\nbut it can do so under false pretenses.\nSo adding more variables\nwill always explain\nmore variation, but it can\nopen up a whole Pandora's box\nof other problems that we\ndefinitely want to avoid.\nSo we'll talk about that more as we go,\nbut I just wanna float it out there\nthat dumping more variables\ninto a multiple regression\nprocedure is not the way to go.\nThe idea is to pick the best\nvariables for the model.\nWe'll talk about how to\ndo that in future videos.\nThe other concept is that the addition\nof more independent\nvariables, see a pattern here,\ncreates more relationships among them.\nSo not only are the independent\nvariables potentially\nrelated to the dependent\nvariable, they are also\npotentially related to each other.\nNow when this happens, it\nis called multicollinearity.\nNow it's a mouthful of a word to say\nand I stumble it over it\nsometimes but hopefully\nwe'll get better at it as we go.\nSo it's called multicollinearity when the\nindependent variables are\ncorrelated with each other.\nSo the ideal, the perfect\nworld is for all the\nindependent variables to\nbe correlated with the\ndependent variable but\nnot with each other.\nAnd again, we'll talk about\noverfitting more as we go.\nWe'll talk about multicollinearity\nmore as we go forward\nand just keep in mind that the ideal is\nfor the independent\nvariables to be correlated\nwith dependent variable\nbut not with each other.\nNow because of multicollinearity\nand overfitting,\nthere is a fair amount of prep work to do\nbefore conducting multiple\nregression analysis\nif one is to do it properly.\nAnd in a future video,\nwe will walk through\nall those things step\nby step so that you form\nthe best model you can.\nSo things like\ncorrelations, scatter plots,\nand some simple regressions\nbetween the independent variable\neach one of them, and\nthe dependent variable\njust to see how they're related.\nSo to do multiple regression\nproperly, really running\nthe multiple regression\nis the very last step.\nThere's a lot of prep work\nto do before doing that\nand again we'll talk about it as we go.\nSo as we talked about before,\nadding more independent\nvariables creates more relationships\namong all the variables.\nSo we have this many-to-one relationship.\nNow in our problem we\nhave a dependent variable\nthat is the travel time.\nWe are trying to predict the travel time\nof these trips.\nNow we are utilizing two\nindependent variables\nthat we selected.\nWe have miles traveled, that's our X1\nand then we had the number of deliveries\nor num deliveries which is our X2.\nNow we'd like to utilize those\ntwo independent variables\nto make predictions about\nthe dependent variable.\nNow by setting it up\nthis way, we also create\na third relationship, and\nthat is the relationship\nbetween the two independent\nvariables themselves.\nSo we don't have just two\nrelationships, independent\nand dependent.\nWe now have a relationship\nbetween the independents.\nAnd having that relationship\nsets up the potential\nmulticollinearity risk.\nSo we're gonna have to see\nwhen we do this problem,\nwhether or not these two\nindependent variables\nare correlated with each other.\nAnd the easy way to\nthink about this is that\nif these two independent\nvariables are related\nto each other, we're\nreally not sure which one\nis explaining the variation\nin the dependent variable.\nSo if I put sea salt and\ntable salt in my dinner,\nall I know is that it tastes salty.\nBut I can't tell the\ndifference necessarily\nbetween the two because they're both salt.\nThey have the same relationship\nto my now salty dinner.\nSo we wanna have a distinction between the\nindependent variables so that it explains\nsomething different.\nWe have a different relationship\nwith the dependent variable\nover here on the right.\nSo we will walk through that\nanalysis as we go forward.\nNow let's look at this situation.\nSo here we have one dependent variable\nand four independent variables.\nSo we know we have the four relationships\nwith each independent variable\nand the dependent variable.\nSo right there we already have\nfour variable relationships.\nBut we're not done.\nWe have to account for\nall the relationships\nbetween the independent variables.\nAnd that's six more.\nSo now with four independent variables\nand one dependent variable,\nwe have 10 relationships\nwe have to consider.\nNow you can see as each\nindependent variable is added,\nthese relationships become very numerous.\nSo part of the art of multiple\nregression is deciding\nwhich independent variables make the cut\nand which do not.\nAnd we'll talk about\nthat as we go of course.\nSo the bottom line is that\nsome independent variables\nor sets of independent\nvariables are better\nat predicting the dependent\nvariable than others.\nAnd some independent\nvariables contribute nothing.\nSo we'll have to decide\nwhich independent variables\nto include in our model\nand which ones to exclude.\nSo again, the ideal is for all\nof the independent variables\nto be correlated with\nthe dependent variable,\nso the orange lines,\nbut not with each other,\nso the colored dotted lines here.\nSo this slide is not something\nyou have to really commit\nto memory, but I just wanna\nshow you sort of where\nthe multiple regression model comes from.\nSo we have our multiple regression model\nwhich is Y equals beta\nsub zero, plus beta one X1\nplus beta two, X2 plus et\ncetera, et cetera, et cetera.\nP just means the number\nof variables we have\nplus epsilon.\nNow over here on the left\nwhat we have are the sum\nof linear parameters.\nSo if beta sub zero,\nwhich is our intercept\nand then we have beta one,\nX1, which is one variable\nand its weight, then we have X2, beta two,\nwhich is another variable and its weight,\net cetera, et cetera.\nSo it's just the sum of\nsome linear parameters.\nBut over here on the right\nwe have our error term.\nSo we've seen this before\nin simple linear regression.\nSo we have an intercept plus\na set of linear parameters\nplus an error term.\nNow for the multiple regression equation,\nwe have the expected value\nof Y equals everything\nwe see up at the top but\nthere is no error term.\nWell why is that?\nThat's because in the\nmultiple regression equation,\nthe error term is assumed to be zero.\nSo zero is zero and\ntherefore it's not on the end\nof that equation.\nThe one we're gonna be\nfamiliar with is the estimated\nmultiple regression equation.\nSo again when we're using sample data,\nit's never gonna be perfect.\nWe're estimating, so we\nhave to use the estimative\nmultiple regression equation.\nSo Y hat is the predicted value of Y\nequals B sub zero plus B1\nX1 plus B2, X2, et cetera.\nAs you can see it follows the same form\nas the multiple regression\nequation above it\nand everything that you\nsee at the bottom is\njust an estimate of what is above it.\nSo B zero, B one, B two\nare all the estimates\nof beta zero, beta one,\nbeta two, et cetera\nand then Y hat is the predicted value\nof the dependent variable.\nSo again, this is not\nsomething you need to really\ncommit to memory, but I just\nwant you to see the pattern\nof how these multiple regression equations\nare gonna look and we'll\ntalk about in the next slide\nsort of what they mean.\nSo let's go ahead and look at an example.\nSo this is a multiple regression equation\nyou may generate based on\nsome analysis you conduct.\nSo if Y hat equals 6.211\nplus 0.014 X1, plus\n0.383 X2, minus 0.607 X3.\nThis is a standard form of a\nmultiple regression equation\nyou may generate.\nNow if we look at our estimated\nmultiple regression equation\nso we have Y hat equals B zero plus B1, X1\nplus B2, X2, et cetera,\nif you look at that\nand compare it to the equation at the top,\nyou can see that they're very similar.\nWe just have some stand\nin numbers that we have\nto interpret.\nSo we have our variables, so X1, X2 and X3\nare our variables and we can\nsee that they're in place\nthere at the top and of\ncourse in the equation\nat the bottom.\nThen we have some\ncoefficients and an intercept.\nSo 6.211 is our intercept\nwhich corresponds\nwith B sub zero in the equation below.\nThen we have .014 there in\nthe blue that corresponds\nto the first coefficient on the bottom,\net cetera, et cetera.\nSo we follow the same basic form.\nIntercept plus some coefficients\npaired with a variable.\nSo a coefficient with our first variable,\na coefficient with the second variable,\nin this case a coefficient\nwith the third variable.\nAnd again, these are all estimates\nof the multiple regression model.\nSo how do we interpret the coefficients\nin multiple regression?\nThey're interpreted a bit differently\nthan they are in simple linear regression.\nLet's take this example.\nSo we have Y hat equals\n27 plus 9X1 plus 12X2.\nEverything we see here is\nin thousands of dollars.\nSo X1, that's our first\nvariable stands for capital\ninvestment in the thousands of dollars.\nSo X2 stands for the\nmarketing expenditures\nin thousands of dollars.\nThat's there in the blue.\nAnd of course Y hat is\ngonna be our predicted sales\nin thousands of dollars.\nSo everything is in thousands of dollars.\nWe have to keep that\nin mind as we go about\ninterpreting it.\nSo in multiple regression\neach coefficient,\nso we have our nine and our 12 up there,\nis interpreted as the\nestimated change in Y\ncorresponding to a one\nunit change in a variable\nwhen all other variables\nare held constant.\nSo what does that mean in this problem?\nSo in this example, $9,000 is an estimate\nof the expected increase\nin sales, which is Y,\ncorresponding to a $1,000\nincrease in capital investment\nwhich is our X1 up there.\nSo remember, everything's\nin thousands of dollars\nso I was making sure\nit's actually in dollars\nhere at the bottom.\nSo $9,000 is an estimate\nof the expected increase\nin sales corresponding to a 1,000 increase\nin capital investment, which is X1.\nWell why's that?\nWith the big coefficient,\nwith our X1 variable up there,\nit is nine.\nSo if we increase X1 or\nX1 is the number one,\nwe have nine times one, so\nthat's nine times $1,000.\nThat's $9,000, assuming\nwe hold the X2 over here\non the right constant.\nAnd that's how we\ninterpret the coefficients\nin multiple regression.\nWe could flip that and say\nwell, $12,000 is an estimate\nof the expected increase\nin sales Y, corresponding\nto a $1,000 increase in\nmarketing expenditures\nwhen capital investment is held constant.\nSo a one unit increase\nwhen everything else,\nall the other variables are held constant.\nAnd again, we'll be doing\nthis more in future videos\nso we'll get some practice at it.\nBut that's the basic idea of\nhow we interpret coefficients\nin a multiple regression equation.\nLet's go ahead and do a quick review\nand then we'll be done\nwith this first video.\nSo multiple regression is an extension\nof simple linear regression.\nTwo or more independent\nvariables ar used to predict\nor explain the variance\nin one dependent variable.\nTwo problems may arise\nhowever, overfitting\nand multicollinearity.\nSo overfitting is caused\nby adding too many\nindependent variables.\nThey account for more\nvariance but really add\nnothing more to the model.\nWhen multicollinearity happens when some\nor all the independent\nvariables are correlated\nwith each other and it\nbecomes hard to tell\nwhich is actually predicting\nor explaining the variance\nin the dependent variable\n'cause they're so similar.\nIn multiple regression, each\ncoefficient is interpreted\nas the estimated change in\nY, the dependent variable,\ncorresponding to a one\nunit change in a variable\nwhen all other variables\nare held constant.\nAnd again, we'll practice\nthat more as we go.\nSo this was our first\nvideo, just the very basics\nof multiple regression.\nWe'll be doing more videos in the future\nand walking through the\nprocess by which we examine\nour variables.\nWe look at relationships\namong them before we even ever\nget into conducting\nthe multiple regression\nusing a statistics package like SPSS or R\nor Excel or whatever.\nThere's a lot of pre work\nto do and that's what\nwe're gonna cover in the next video.\nSo I hope you found this\nfirst video helpful.\nIf you like the video,\nplease give it a thumbs up,\nsubscribe, share it or\nwhatever you wanna do,\nspread the word.\nI just do these to help people learn.\nSo I hope you enjoyed it\nand I'll see you again\nin the next video.\n(light music)"
    },
    {
        "topic": "Logistic Regression",
        "url": "https://www.youtube.com/watch?v=yIYKR4sgzI8",
        "transcript": "If you can fit a line you can fit a squiggle if you can, make me laugh you can, make me giggle stat quest\nHello, i'm josh stormer and welcome to stat quest today we're going to talk about logistic regression\nThis is a technique that can be used for traditional statistics as, well as machine learning so let's get right to it\nBefore we dive into logistic regression let's take. A, step back and review, linear regression in\nAnother stat quest, we talked, about linear regression\nWe had some data\nWeight and size\nthen, we fit a line to it and\nWith that line, we could do a lot of things\nFirst we could calculate r-squared and determine if weight and size are\ncorrelated large values imply a large effect, and\nSecond calculate a p-value to determine if the r-squared value is statistically significant and\nThird, we could use the line, to predict, size given weight if a, new, mouse has this weight\nThen this, is the size that, we predict, from the weight although\nWe didn't mention it at the time using data to predict something falls under the category of machine learning\nSo plain old linear regression is a form of machine learning\nWe also talked a little bit about multiple regression\nNow, we are trying to predict, size, using weight and blood volume\nAlternatively we could, say that, we are trying to model size using weight and blood volume\nMultiple regression, did the same things that normal regression did\nwe calculated r-squared and\nwe calculated the p-value and\nWe could predict, size, using weight and blood volume and\nThis, makes multiple regression a slightly fancier machine learning method\nWe also talked, about how, we can use discrete measurements like genotype to predict size if you're\nNot familiar with the term genotype don't freak out it's. No, big deal just know that it refers to different types of mice\nlastly, we could compare models\nSo on the left side we've got normal regression, using weight to predict size and\nWe can, compare those predictions to the ones, we get from multiple regression, where we're using weight and blood volume to predict size\nComparing the simple model to the complicated one tells us if we need to measure weight and blood volume to accurately predict\nSize or if we can get, away, with just weight\nNow that we remember all the cool, things, we can, do with linear regression\nLet's talk, about logistic regression\nLogistic regression is similar to linear regression\nexcept\nLogistic regression predicts whether something, is true or false instead of predicting something continuous, like, sighs\nthese mice are obese and\nThese mice are not\nAlso instead of fitting a line to the data logistic regression fits an s-shaped logistic function\nThe curve goes from zero to one?\nAnd that, means that the curve tells you the probability that a mouse is obese based on its weight\nIf we weighed a very heavy mouse?\nThere is a high probability that the new, mouse is obese?\nIf we weighed an intermediate mouse\nThen there is only a 50% chance of the mouse is obese?\nLastly, there's only a small probability that a light mouse is obese\nAlthough, logistic regression, tells the probability that a mouse is obese or not it's usually used for classification\nFor example if the probability of mouse is obese is greater than 50%\nThen we'll classify it as obese\nOtherwise we'll classify it as not obese\nJust like with linear regression, we can, make simple models in this case, we can have obesity predicted, by weight or?\nmore complicated models in this case obesity is predicted by weight and genotype in\nThis, case, obesity is predicted. By weight and genotype and age and\nLastly, obesity is predicted by weight genotype, age and\nAstrological sign in other words just like linear regression logistic\nRegression can work with continuous data, like weight and age and discrete data like genotype and astrological sign\nWe can, also test to see if each variable is useful for predicting obesity\nhowever\nUnlike normal regression, we can't easily compare the complicated model to the simple model and we'll talk more about, why in a bit\nInstead we just test to see if a variables affect on the prediction is significantly different from zero\nIf not it, means that the variable is not helping the prediction\nWe use, wald's tests to figure this out we'll talk, about that in another stat quest in\nThis, case, the astrological sign is totes useless\nThat statistical jargon for not helping\nThat, means we can, save time and space in our study. By leaving it out\nLogistic regressions ability to provide probabilities and classify, new samples using continuous and discrete measurements\nMakes it a popular machine learning method\nOne big difference between linear regression and logistic regression is how the line is fit to the data\nWith linear regression, we fit the line, using least squares\nIn other words, we find the line that minimizes the sum of the squares of these residuals\nWe also use the residuals to calculate r. Squared and to compare simple models to complicated models\nLogistic regression doesn't have the same concept of a residual so it can't use least squares and it can't calculate r squared\ninstead it uses something called maximum likelihood\nThere's a whole stack quest on maximum likelihood so see that for details but in a nutshell\nYou, pick a probability scaled. By weight of observing an obese mouse just like this curve and\nYou, use that to calculate the likelihood of observing a, non obese mouse that weighs this much and\nthen you calculate the likelihood of observing, this mouse and\nyou, do that for all of the mice and\nLastly, you multiply all of those likelihoods together that's the likelihood of the data given this line\nthen you shift the line and calculate a new, likelihood of the data and\nthen ship the line and calculate the likelihood, again, and\nagain\nFinally the curve with the maximum value for the likelihood is selected bam\nin summary logistic regression can be used to classify samples and\nit can, use different types of data like, size and/or genotype to do that classification and\nit can, also be used to assess what variables are useful for classifying samples ie\nAstrological sign is totes useless\nHooray, we've made it to the end of another exciting stat quest do you, like this StackQuest, and want to see more please subscribe\nif you, have suggestions for future stat quests, well put them in the comments below, until next time quest on"
    },
    {
        "topic": "Decision Trees",
        "url": "https://www.youtube.com/watch?v=coOTEc-0OGw",
        "transcript": "Error fetching transcript for video coOTEc-0OGw: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=coOTEc-0OGw! This is most likely caused by:\n\nNo transcripts were found for any of the requested language codes: ('en',)\n\nFor this video (coOTEc-0OGw) transcripts are available in the following languages:\n\n(MANUALLY CREATED)\nNone\n\n(GENERATED)\n - hi (\"Hindi (auto-generated)\")[TRANSLATABLE]\n\n(TRANSLATION LANGUAGES)\n - ab (\"Abkhazian\")\n - aa (\"Afar\")\n - af (\"Afrikaans\")\n - ak (\"Akan\")\n - sq (\"Albanian\")\n - am (\"Amharic\")\n - ar (\"Arabic\")\n - hy (\"Armenian\")\n - as (\"Assamese\")\n - ay (\"Aymara\")\n - az (\"Azerbaijani\")\n - bn (\"Bangla\")\n - ba (\"Bashkir\")\n - eu (\"Basque\")\n - be (\"Belarusian\")\n - bho (\"Bhojpuri\")\n - bs (\"Bosnian\")\n - br (\"Breton\")\n - bg (\"Bulgarian\")\n - my (\"Burmese\")\n - ca (\"Catalan\")\n - ceb (\"Cebuano\")\n - zh-Hans (\"Chinese (Simplified)\")\n - zh-Hant (\"Chinese (Traditional)\")\n - co (\"Corsican\")\n - hr (\"Croatian\")\n - cs (\"Czech\")\n - da (\"Danish\")\n - dv (\"Divehi\")\n - nl (\"Dutch\")\n - dz (\"Dzongkha\")\n - en (\"English\")\n - eo (\"Esperanto\")\n - et (\"Estonian\")\n - ee (\"Ewe\")\n - fo (\"Faroese\")\n - fj (\"Fijian\")\n - fil (\"Filipino\")\n - fi (\"Finnish\")\n - fr (\"French\")\n - gaa (\"Ga\")\n - gl (\"Galician\")\n - lg (\"Ganda\")\n - ka (\"Georgian\")\n - de (\"German\")\n - el (\"Greek\")\n - gn (\"Guarani\")\n - gu (\"Gujarati\")\n - ht (\"Haitian Creole\")\n - ha (\"Hausa\")\n - haw (\"Hawaiian\")\n - iw (\"Hebrew\")\n - hi (\"Hindi\")\n - hmn (\"Hmong\")\n - hu (\"Hungarian\")\n - is (\"Icelandic\")\n - ig (\"Igbo\")\n - id (\"Indonesian\")\n - iu (\"Inuktitut\")\n - ga (\"Irish\")\n - it (\"Italian\")\n - ja (\"Japanese\")\n - jv (\"Javanese\")\n - kl (\"Kalaallisut\")\n - kn (\"Kannada\")\n - kk (\"Kazakh\")\n - kha (\"Khasi\")\n - km (\"Khmer\")\n - rw (\"Kinyarwanda\")\n - ko (\"Korean\")\n - kri (\"Krio\")\n - ku (\"Kurdish\")\n - ky (\"Kyrgyz\")\n - lo (\"Lao\")\n - la (\"Latin\")\n - lv (\"Latvian\")\n - ln (\"Lingala\")\n - lt (\"Lithuanian\")\n - lua (\"Luba-Lulua\")\n - luo (\"Luo\")\n - lb (\"Luxembourgish\")\n - mk (\"Macedonian\")\n - mg (\"Malagasy\")\n - ms (\"Malay\")\n - ml (\"Malayalam\")\n - mt (\"Maltese\")\n - gv (\"Manx\")\n - mi (\"M\u0101ori\")\n - mr (\"Marathi\")\n - mn (\"Mongolian\")\n - mfe (\"Morisyen\")\n - ne (\"Nepali\")\n - new (\"Newari\")\n - nso (\"Northern Sotho\")\n - no (\"Norwegian\")\n - ny (\"Nyanja\")\n - oc (\"Occitan\")\n - or (\"Odia\")\n - om (\"Oromo\")\n - os (\"Ossetic\")\n - pam (\"Pampanga\")\n - ps (\"Pashto\")\n - fa (\"Persian\")\n - pl (\"Polish\")\n - pt (\"Portuguese\")\n - pt-PT (\"Portuguese (Portugal)\")\n - pa (\"Punjabi\")\n - qu (\"Quechua\")\n - ro (\"Romanian\")\n - rn (\"Rundi\")\n - ru (\"Russian\")\n - sm (\"Samoan\")\n - sg (\"Sango\")\n - sa (\"Sanskrit\")\n - gd (\"Scottish Gaelic\")\n - sr (\"Serbian\")\n - crs (\"Seselwa Creole French\")\n - sn (\"Shona\")\n - sd (\"Sindhi\")\n - si (\"Sinhala\")\n - sk (\"Slovak\")\n - sl (\"Slovenian\")\n - so (\"Somali\")\n - st (\"Southern Sotho\")\n - es (\"Spanish\")\n - su (\"Sundanese\")\n - sw (\"Swahili\")\n - ss (\"Swati\")\n - sv (\"Swedish\")\n - tg (\"Tajik\")\n - ta (\"Tamil\")\n - tt (\"Tatar\")\n - te (\"Telugu\")\n - th (\"Thai\")\n - bo (\"Tibetan\")\n - ti (\"Tigrinya\")\n - to (\"Tongan\")\n - ts (\"Tsonga\")\n - tn (\"Tswana\")\n - tum (\"Tumbuka\")\n - tr (\"Turkish\")\n - tk (\"Turkmen\")\n - uk (\"Ukrainian\")\n - ur (\"Urdu\")\n - ug (\"Uyghur\")\n - uz (\"Uzbek\")\n - ve (\"Venda\")\n - vi (\"Vietnamese\")\n - war (\"Waray\")\n - cy (\"Welsh\")\n - fy (\"Western Frisian\")\n - wo (\"Wolof\")\n - xh (\"Xhosa\")\n - yi (\"Yiddish\")\n - yo (\"Yoruba\")\n - zu (\"Zulu\")\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
    },
    {
        "topic": "Clustering",
        "url": "https://www.youtube.com/watch?v=4b5d3muPQmA",
        "transcript": "statcast\n[Music]\nstat quest stat quest stat quest hello\nI'm Josh stormer and welcome to stat\nquest today we're going to be talking\nabout k-means clustering we're gonna\nlearn how to cluster samples that can be\nput on a line on an XY graph and even on\na heat map and lastly we'll also talk\nabout how to pick the best value for K\nimagine you had some data that you could\nplot on a line and you knew you needed\nto put it into three clusters maybe they\nare measurements from three different\ntypes of tumors or other cell types in\nthis case the data make three relatively\nobvious clusters but rather than rely on\nour eye let's see if we can get a\ncomputer to identify the same three\nclusters to do this we'll use k-means\nclustering we'll start with raw data\nthat we haven't yet clustered step one\nselect the number of clusters you want\nto identify in your data this is the K\nin k-means clustering in this case we'll\nselect K equals three that is to say we\nwant to identify three clusters there is\na fancier way to select a value for K\nbut we'll talk about that later\nstep two randomly select three distinct\ndata points these are the initial\nclusters\nstep 3 measure the distance between the\nfirst point and the three initial\nclusters this is the distance from the\nfirst point to the blue cluster this is\nthe distance from the first point to the\ngreen cluster\nand this is the distance from the first\npoint to the orange cluster well it's\nkind of yellow but we'll just call it\norange for now step 4 assign the first\npoint to the nearest cluster in this\ncase the nearest cluster is the blue\ncluster now we do the same thing for the\nnext point we measure the distances and\nthen assign the point to the nearest\ncluster now we figure out which cluster\nthe third point belongs to we measure\nthe distances and then assign the point\nto the nearest cluster the rest of these\npoints are closest to the orange cluster\nso they'll go in that one two now that\nall the points are in clusters we go on\nto step 5 calculate the mean of each\ncluster then we repeat what we just did\nmeasure and cluster using the mean\nvalues\nsince the clustering did not change at\nall during the last iteration were done\nBAM the k-means clustering is pretty\nterrible compared to what we did by eye\nwe can assess the quality of the\nclustering by adding up the variation\nwithin each cluster here's the total\nvariation within the clusters since\nk-means clustering can't see the best\nclustering it's only option is to keep\ntrack of these clusters and their total\nvariance and do the whole thing over\nagain with different starting points so\nhere we are again back at the beginning\nk-means clustering picks three initial\nclusters and then clusters all the\nremaining points calculates the mean of\neach cluster and then re clusters based\non the new means it repeats until the\ncluster is no longer change bit bit bit\nof bit of boop boop boop now that the\ndata are clustered we sum the variation\nwithin each cluster\nand then we do it all again\nat this point k-means clustering knows\nthat the second clustering is the best\nclustering so far but it doesn't know if\nit's the best overall so it will do a\nfew more clusters it does as many as you\ntell it to do and then come back and\nreturn that one if it is still the best\nquestion how do you figure out what\nvalue to use for K with this data it's\nobvious that we should set K to three\nbut other times it is not so clear one\nway to decide is to just try different\nvalues for K\nwe'll start with k equals 1 k equals 1\nis the worst case scenario we can\nquantify its badness with the total\nvariation now try K equals 2 K equals 2\nis better and we can quantify how much\nbetter by comparing the total variation\nwithin the two clusters to K equals 1\nnow try K equals 3 k equals 3 is even\nbetter we can quantify how much better\nby comparing the total variation within\nthe three clusters to k equals 2 now try\nk equals 4 the total variation within\neach cluster is less than when K equals\n3 each time we add a new cluster the\ntotal variation within each cluster is\nsmaller than before and when there is\nonly one point per cluster the variation\nequals 0 however if we plot the\nreduction in variance per value for K\nthere is a huge reduction in variation\nwith K equals three but after that the\nvariation doesn't go down as quickly\nthis is called an elbow plot and you can\npick K by finding the elbow in the plot\nquestion how is k-means clustering\ndifferent from hierarchical clustering\nk-means clustering specifically tries to\nput the data into the number of clusters\nyou tell it to hierarchical clustering\njust tells you pairwise what two things\nare most similar question what if our\ndata isn't plotted on a number line just\nlike before you pick three random points\nand we use the Euclidean distance in two\ndimensions the Euclidean distance is the\nsame thing as the Pythagorean theorem\nthen just like before we assign the\npoint to the nearest cluster and just\nlike before we then calculate the center\nof each cluster and re cluster BAM\nalthough this looks good the computer\ndoesn't know that until it does the\nclustering a few more times question\nwhat if my data is a heatmap well if we\njust have two samples we can rename them\nx and y and we can then plot the data in\nan XY graph then we can cluster just\nlike before note we don't actually need\nto plot the data in order to cluster it\nwe just need to calculate the distances\nbetween things when we have two samples\nor two axes the Euclidean distance is\nthe square root of x squared plus y\nsquared when we have three samples or\nthree axes the Euclidean distance is the\nsquare root of x squared plus y squared\nplus Z squared and when we have four\nsamples or four axes the Euclidean\ndistance is the square root of x squared\nplus y squared plus Z squared plus a\nsquared etc etc etc hooray\nwe've made it to the end of another\nexciting stat quest if you like this\nstat quest and want to see more please\nsubscribe and if you want to support\nstack quest well click the like button\ndown below and consider buying one or\ntwo of my original songs alright tune in\nnext time for another exciting stat\nquest"
    },
    {
        "topic": "K-Means Clustering",
        "url": "https://www.youtube.com/watch?v=4b5d3muPQmA",
        "transcript": "statcast\n[Music]\nstat quest stat quest stat quest hello\nI'm Josh stormer and welcome to stat\nquest today we're going to be talking\nabout k-means clustering we're gonna\nlearn how to cluster samples that can be\nput on a line on an XY graph and even on\na heat map and lastly we'll also talk\nabout how to pick the best value for K\nimagine you had some data that you could\nplot on a line and you knew you needed\nto put it into three clusters maybe they\nare measurements from three different\ntypes of tumors or other cell types in\nthis case the data make three relatively\nobvious clusters but rather than rely on\nour eye let's see if we can get a\ncomputer to identify the same three\nclusters to do this we'll use k-means\nclustering we'll start with raw data\nthat we haven't yet clustered step one\nselect the number of clusters you want\nto identify in your data this is the K\nin k-means clustering in this case we'll\nselect K equals three that is to say we\nwant to identify three clusters there is\na fancier way to select a value for K\nbut we'll talk about that later\nstep two randomly select three distinct\ndata points these are the initial\nclusters\nstep 3 measure the distance between the\nfirst point and the three initial\nclusters this is the distance from the\nfirst point to the blue cluster this is\nthe distance from the first point to the\ngreen cluster\nand this is the distance from the first\npoint to the orange cluster well it's\nkind of yellow but we'll just call it\norange for now step 4 assign the first\npoint to the nearest cluster in this\ncase the nearest cluster is the blue\ncluster now we do the same thing for the\nnext point we measure the distances and\nthen assign the point to the nearest\ncluster now we figure out which cluster\nthe third point belongs to we measure\nthe distances and then assign the point\nto the nearest cluster the rest of these\npoints are closest to the orange cluster\nso they'll go in that one two now that\nall the points are in clusters we go on\nto step 5 calculate the mean of each\ncluster then we repeat what we just did\nmeasure and cluster using the mean\nvalues\nsince the clustering did not change at\nall during the last iteration were done\nBAM the k-means clustering is pretty\nterrible compared to what we did by eye\nwe can assess the quality of the\nclustering by adding up the variation\nwithin each cluster here's the total\nvariation within the clusters since\nk-means clustering can't see the best\nclustering it's only option is to keep\ntrack of these clusters and their total\nvariance and do the whole thing over\nagain with different starting points so\nhere we are again back at the beginning\nk-means clustering picks three initial\nclusters and then clusters all the\nremaining points calculates the mean of\neach cluster and then re clusters based\non the new means it repeats until the\ncluster is no longer change bit bit bit\nof bit of boop boop boop now that the\ndata are clustered we sum the variation\nwithin each cluster\nand then we do it all again\nat this point k-means clustering knows\nthat the second clustering is the best\nclustering so far but it doesn't know if\nit's the best overall so it will do a\nfew more clusters it does as many as you\ntell it to do and then come back and\nreturn that one if it is still the best\nquestion how do you figure out what\nvalue to use for K with this data it's\nobvious that we should set K to three\nbut other times it is not so clear one\nway to decide is to just try different\nvalues for K\nwe'll start with k equals 1 k equals 1\nis the worst case scenario we can\nquantify its badness with the total\nvariation now try K equals 2 K equals 2\nis better and we can quantify how much\nbetter by comparing the total variation\nwithin the two clusters to K equals 1\nnow try K equals 3 k equals 3 is even\nbetter we can quantify how much better\nby comparing the total variation within\nthe three clusters to k equals 2 now try\nk equals 4 the total variation within\neach cluster is less than when K equals\n3 each time we add a new cluster the\ntotal variation within each cluster is\nsmaller than before and when there is\nonly one point per cluster the variation\nequals 0 however if we plot the\nreduction in variance per value for K\nthere is a huge reduction in variation\nwith K equals three but after that the\nvariation doesn't go down as quickly\nthis is called an elbow plot and you can\npick K by finding the elbow in the plot\nquestion how is k-means clustering\ndifferent from hierarchical clustering\nk-means clustering specifically tries to\nput the data into the number of clusters\nyou tell it to hierarchical clustering\njust tells you pairwise what two things\nare most similar question what if our\ndata isn't plotted on a number line just\nlike before you pick three random points\nand we use the Euclidean distance in two\ndimensions the Euclidean distance is the\nsame thing as the Pythagorean theorem\nthen just like before we assign the\npoint to the nearest cluster and just\nlike before we then calculate the center\nof each cluster and re cluster BAM\nalthough this looks good the computer\ndoesn't know that until it does the\nclustering a few more times question\nwhat if my data is a heatmap well if we\njust have two samples we can rename them\nx and y and we can then plot the data in\nan XY graph then we can cluster just\nlike before note we don't actually need\nto plot the data in order to cluster it\nwe just need to calculate the distances\nbetween things when we have two samples\nor two axes the Euclidean distance is\nthe square root of x squared plus y\nsquared when we have three samples or\nthree axes the Euclidean distance is the\nsquare root of x squared plus y squared\nplus Z squared and when we have four\nsamples or four axes the Euclidean\ndistance is the square root of x squared\nplus y squared plus Z squared plus a\nsquared etc etc etc hooray\nwe've made it to the end of another\nexciting stat quest if you like this\nstat quest and want to see more please\nsubscribe and if you want to support\nstack quest well click the like button\ndown below and consider buying one or\ntwo of my original songs alright tune in\nnext time for another exciting stat\nquest"
    },
    {
        "topic": "Hierarchical Clustering",
        "url": "https://www.youtube.com/watch?v=7enWesSofhg",
        "transcript": "Error fetching transcript for video 7enWesSofhg: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=7enWesSofhg! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
    },
    {
        "topic": "Dimensionality Reduction",
        "url": "https://www.youtube.com/watch?v=SMaa3pnQmbg",
        "transcript": "Error fetching transcript for video SMaa3pnQmbg: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=SMaa3pnQmbg! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
    },
    {
        "topic": "Principal Component Analysis",
        "url": "https://www.youtube.com/watch?v=FgakZw6K1QQ",
        "transcript": "StatQuest breaks it down into bite-sized pieces, hooray!\nHello, I'm Josh Starmer and welcome to StatQuest. In this StatQuest we're going to\ngo through Principal Component Analysis (PCA) one step at a time using Singular Value Decomposition (SVD).\nYou'll learn about what PCA does, how it does it, and how to use it to get deeper insight into your data.\nLet's start with a simple data set.\nWe've measured the transcription of two genes, Gene 1 and Gene 2,\u00a0in\u00a06\u00a0different mice.\nNote: If you're not into mice and genes, think of the mice as individual samples and\nthe genes as variables that we measure for each sample.\nFor example, the samples could be students in high school and the variables could\nbe test scores in math and reading, or the samples could be businesses and the variables\ncould be market capitalization and the number of employees.\nOkay, now we're back to mice and genes, because I'm a geneticist and I work in a genetics department.\nIf we only measure one gene we can plot the data on a number line.\nMice 1, 2, and 3 have relatively high values and mice 4, 5, and 6 have relatively low values.\nEven though it's a simple graph, it shows us that mice 1, 2, and 3 are more similar\nto each other than they are to mice 4, 5, and 6.\nIf we measured 2 genes, then we can plot the data on a two-dimensional X-Y graph.\nGene 1 is the x-axis and spans one of the two dimensions in this graph.\nGene 2 is the y-axis and spans the other dimension.\nWe can see that mice 1, 2, and 3 cluster on the right side and mice 4, 5, and 6 cluster on the lower left hand side.\nIf we measured three genes, we would add another axis to the graph and make it look 3D, i.e.\u00a03-dimensional.\nThe smaller dots have larger values for Gene 3 and are further away.\nThe larger dots have smaller values for Gene 3 and are closer.\nIf we measured 4 genes, however, we can no longer plot the data. 4  genes require\n4 dimensions.  So we're going to talk about how PCA can take 4 or more gene measurements,\nand thus 4 or more dimensions of data, and make a 2-dimensional PCA plot.\nThis plot will show us that similar mice cluster together.\nWe'll also talk about how PCA can tell us which gene, or variable, is the most valuable for clustering the data.\nFor example PCA might tell us that Gene 3 is responsible for separating samples along the x-axis.\nLastly, we'll talk about how PCA can tell us how accurate the 2D graph is.\nTo understand what PCA does and how it works, let's go back to the data set that only had 2 genes.\nWe'll start by plotting the data. Then we'll calculate the average measurement for\nGene 1, and the average measurement for Gene 2.\nWith the average values, we can calculate the center of the data.\nFrom this point on, we'll focus on what happens in the graph -  we no longer need the original data.\nNow, we'll shift the data so that the center is on top of the origin in the graph.\nNote: Shifting the data did not change how the data points are positioned relative to each other.\nThis point is still the highest one, and this is still the rightmost point, etc.\nNow that the data are centered on the origin, we can try to fit a line to it.\nTo do this, we start by drawing a random line that goes through the origin.\nThen we rotate the line until it fits the data as well as it can, given that it has to go through the origin.\nUltimately, this line fits best.\nBut I'm getting ahead of myself, first we need to talk about how PCA decides if a fit is good or not.\nSo, let's go back to the original random line that goes through the origin.\nTo quantify how good this line fits the data, PCA projects the data onto it and then\nit can either measure the distances from the data to the line and try to find the\nline that minimizes those distances, or it can try to find the line that maximizes\nthe distances from the projected points to the origin.\nIf those options don't seem equivalent to you, we can build intuition by looking at\nhow these distances shrink when the line fits better,\nwhile these distances get larger when the line fits better.\nNow, to understand what is going on in a mathematical way, let's just consider one data point.\nThis point is fixed and so is its distance from the origin.\nIn other words, the distance from the point to the origin doesn't change when the red dotted line rotates.\nWhen we project the point onto the line, we get a right angle between the black dotted\nline and the red dotted line.\u00a0That means that if we label the sides like this: A,\nB, and C, then we can use the Pythagorean theorem to show how B and C are inversely related.\nSince A, and thus A squared, doesn't change if B gets bigger then C must get smaller.\nLikewise, if C gets bigger then B must get smaller.\nThus, PCA can either minimize the distance to the line, or maximize the distance from the projected point to the origin.\nThe reason I'm making such a fuss about this is that, intuitively, it makes sense\nto minimize B and the distance from the point to the line, but it's actually easier\nto calculate C, the distance from the projected point to the origin, so PCA finds\nthe best fitting line by maximizing the sum of the squared distances from the projected points to the origin.\nSo, for this line, PCA projects the data onto it and then measures the distance from\nthis point to the origin, let's call it D1.\nNote: I'm going to keep track of the distances we measure up here.\nAnd then PCA measures the distance from this point to the origin, we'll call that D2.\nThen it measures D3, D4, D5, and D6.\nHere are all six distances that we measured.\nThe next thing we do is square all of them.\nThe distances are squared so that negative values don't cancel out positive values.\nThen we sum up all these squared distances, and that equals the sum of the squared distances.\nFor short we'll call this SS distances, for sum of squared distances.\nNow we rotate the line, project the data onto the line, and then sum up the squared\ndistances from the projected points to the origin. And we repeat until we end up\nwith the line with the largest sum of squared distances between the projected points and the origin.\nUltimately, we end up with this line.\nIt has the largest sum of squared distances.\nThis line is called Principal Component 1, or PC1 for short.\nPC1 has a slope of 0.25. In other words, for every 4 units that we go out along the\nGene 1 axis, we go up 1 unit along the Gene 2 axis. That means that the data are\nmostly spread out along the Gene 1 axis, and only a little bit spread out along the Gene 2 axis.\nOne way to think about PC1 is in terms of a cocktail recipe.\nTo make PC1 mix four parts Gene 1 with one part Gene 2.\nPour over ice and serve!\nThe ratio of Gene 1 to Gene 2 tells you that Gene 1 is more important when it comes\nto describing how the data are spread out.\nOh no, terminology alert!\nMathematicians call this cocktail recipe a linear combination of Genes 1 and 2.\nI mention this because when someone says PC1 is a linear combination of variables,\nthis is what they're talking about.  It's no big deal.\nThe recipe for PC1, going over 4 and up 1 gets us to this point.\nWe can solve for the length of the red line using the Pythagorean theorem, the old\nA squared equals B squared plus C squared.\nPlugging in the numbers gives us A equals 4.12.\nSo the length of the red line is 4.12.\nWhen you do PCA with SVD, the recipe for PC1 is scaled so that this length equals\u00a01.\nAll we have to do to scale the triangle so that the red line is\u00a01\u00a0unit long is to divide each side by 4.12.\nFor those of you keeping score, here's the math worked out that shows that all we\nneed to do is divide all\u00a03\u00a0sides by 4.12.\nHere are the scaled values.\nThe new values change our recipe, but the ratio is the same.\nWe still use four times as much Gene 1 as Gene 2.\nSo now we are back to looking at the data, the best fitting line, and the unit vector that we just calculated.\nOh no, another terminology alert!\nThis 1 unit long vector, consisting of 0.97 parts Gene 1 and 0.242 parts Gene 2, is\ncalled the Singular Vector, or the Eigenvector for PC1, and the proportions of each gene are called loading scores.\nAlso while I'm at it, PCA calls the average of the sums of the squared distances for\nthe best fit line the Eigenvalue for PC1. And the square root of the sums of the\nsquared distances is called the Singular Value for PC1.\nBam! That's a lot of terminology.\nNow that we've got PC1 all figured out let's work on PC2. Because this is only a two-dimensional\ngraph, PC2 is simply the line through the origin that is perpendicular to PC1 without\nany further optimization that has to be done. And this means that the recipe for\nPC2 is -1 parts Gene 1 to 4 parts Gene 2.\nIf we scale everything so that we get a unit vector, the recipe is -0.242 parts Gene 1 and 0.97 parts Gene 2.\nThis is the singular vector for PC2 or the eigenvector for PC2. These are the loading\nscores for PC2, they tell us that, in terms of how the values are projected onto\nPC2, Gene 2 is 4 times as important as Gene 1.\nLastly the eigenvalue for PC2 is the average of the sum of the squares of the distances\nbetween the projected points and the origin. Hooray! We've worked out PC1 and PC2!\nTo draw the final PCA plot, we simply rotate everything so that PC1 is horizontal.\nThen we use the projected points to find where the samples go in the PCA plot. For\nexample, these projected points correspond to sample 6, so sample 6 goes here. Sample\n2 goes here. And Sample 1 goes here. Etc. Double bam! That's how PCA is done using\nsingular value decomposition. Okay, one last thing before we dive into a slightly\nmore complicated example. Remember the eigenvalues?\nWe got those by projecting the data onto the principal components, measuring the distances\nto the origin, then squaring and adding them together. Well, if you're familiar with\nthe equation for variation, you will notice that eigenvalues are just measures of\nvariation. For the sake of this example imagine that the variation for PC1 equals\n15 and the variation for PC2 equals 3. That means that the total variation around both PCS is 15 plus 3 equals 18.\nAnd that means PC1 accounts for 15 divided by 18 equals 0.83 or 83% of the total variation\naround the PCs. PC2 accounts for 3 divided by 18 equals 17% of the total variation\naround the PCs. Oh no another terminology alert! A scree plot is a graphical representation\nof the percentages of variation that each PC accounts for. We'll talk more about\nscree plots later. Bam. Okay, now let's quickly go through a slightly more complicated\nexample. PCA with 3 variables, in this case that means 3 genes, is pretty much the\nsame as 2 variables. You center the data. You then find the best fitting line that\ngoes through the origin. Just like before, the best fitting line is PC1. But the\nrecipe for PC1 now has 3 ingredients. In this case Gene 3 is the most important ingredient\nfor PC1. You then find PC2, the next best fitting line given that it goes through\nthe origin and is perpendicular to PC1. Here's the recipe for PC2. In this case Gene\n1 is the most important ingredient for PC2. Lastly, we find PC3, the best fitting\nline that goes through the origin and is perpendicular PC1 and PC2. If we had more\ngenes, we just keep on finding more and more principal components by adding perpendicular\nlines and rotating them. In theory, there is 1 per gene or variable, but in practice\nthe number of PCs is either the number of variables or the number of samples, whichever\nis smaller. If this is confusing, don't sweat it. It's not super important and I'm\ngoing to make a separate video on this topic in the next week. Once you have all\nthe principal components figured out you can use the eigenvalues, i.e. the sums of\nsquares of the distances, to determine the proportion of variation that each PC accounts\nfor. In this case, PC1 accounts for 79% of the variation, PC2 accounts for 15% of\nthe variation and PC3 accounts for 6% of the variation. Here's the scree plot. PC1\nand PC2 account for the vast majority of the variation. That means that a 2D graph,\nusing just PC1 and PC2, would be a good approximation of this 3D graph, since it\nwould account for 94% of the variation in the data. To convert the 3D graph into\na two-dimensional PCA graph, we just strip away everything but the data and PC1 and\nPC2, then project the samples onto PC1 and PC2. Then we rotate so that PC1 is horizontal\nand PC2 is vertical. This just makes it easier to look at. Since these projected\npoints correspond to sample 4, this is where sample 4 goes in our new PCA plot. etc.\netc. etc. Double bam! To review, we started with an awkward 3D graph that was kind\nof hard to read, then we calculated the principal components, then, with the eigenvalues\nfor PC1 and PC2, we determined that a 2D graph would still be very informative. Lastly,\nwe used PC1 and PC2 to draw a two-dimensional graph with the data. If we measured\n4 genes per mouse, we would not be able to draw a 4-dimensional graph of the data,\nbut that doesn't stop us from doing the PCA math, which doesn't care if we can draw\na picture of it or not, and looking at the scree plot. In this case, PC1 and PC2\naccount for 90% of the variation, so we can just use those to draw a 2-dimensional\nPCA graph. So we project the samples onto the first 2 PCs. These 2 projected points\ncorrespond to sample 2, so sample 2 goes here. Bam! Note, if the scree plot looked\nlike this, where PC3 and PC4 account for a substantial amount of variation, then\njust using the first two PCs would not create a very accurate representation of the\ndata. However, even a noisy PCA plot like this can be used to identify clusters of\ndata. These samples are still more similar to each other than they are to the other\nsamples. Little bam. Hooray! We've made it to the end of another exciting StatQuest.\nIf you liked this StatQuest and want to see more, please subscribe. And if you want\nto support StatQuest, please consider buying one or two of my original songs, the\nlink to my bandcamp page is in the lower right corner and in the description below. All right until next time quest on!"
    },
    {
        "topic": "Singular Value Decomposition",
        "url": "https://www.youtube.com/watch?v=mBcLRGuAFUk",
        "transcript": "PROFESSOR: The\nprevious video was\nabout positive\ndefinite matrices.\nThis video is also linear\nalgebra, a very interesting way\nto break up a matrix called the\nsingular value decomposition.\nAnd everybody says SVD for\nsingular value decomposition.\nAnd what is that factoring?\nWhat are the three\npieces of the SVD?\nSo this is the fact is\nevery matrix, rectangular,\nevery matrix factors into--\nthese are the three pieces.\nU sigma V transpose.\nPeople use those letters\nfor the three factors.\nThe factor U is an orthogonal\nmatrix, an orthogonal matrix.\nThe factor sigma in the\nmiddle is a diagonal matrix.\nThe factor V\ntranspose on the right\nis also an orthogonal matrix.\nSo I have orthogonal, diagonal,\northogonal, or physically,\nrotation, stretching, rotation.\nNow we have seen\nthree factors for\na matrix, V, lambda, V inverse.\nWhat's the difference?\nWhat's the difference between\nthis SVD, this, and the V,\nlambda, V transpose,\nV inverse, V lambda,\nV inverse for diagonalizing\nother matrices?\nSo the lambda is diagonal\nand the sigma is diagonal,\nbut they're different.\nThe key point is I now have two\ndifferent matrices, not just\nV and V inverse, but\ntwo different matrices.\nBut the new great\nadvantage is they\nare orthogonal\nmatrices, both of them.\nSo by going to-- and I can do it\nfor rectangular matrices also.\nEigenvalues really worked\nfor square matrices.\nNow we really are-- we have two.\nWe have an input matrix\nand an output matrix.\nIn those spaces m and n can\nhave different dimensions.\nSo by allowing two\nseparate bases,\nwe get rectangular matrices,\nand we get orthogonal factors\nwith, again, a diagonal.\nAnd this is called--\nthese numbers\nsigma instead of eigenvalues,\nare called singular values.\nSo these are the\nsingular values.\nThese are the singular vectors,\nthe right singular vectors\nand the left singular vectors.\nThat's the statement\nof the factorization.\nBut we have to think a little\nbit, what are those factors?\nWhat are the-- can we\nsee why this works?\nSo I want that.\nAnd let me do, as\nyou see this coming,\nI'll look at A transpose\nA. I like A transpose A.\nSo A transpose will\nbe, I transpose this.\nV sigma transpose\nU transpose, right?\nThat's A transpose.\nThen I multiply by A\nU sigma V transpose.\nAnd what do I have?\nWell, I've got six matrices.\nBut U transpose U in\nhere is the identity,\nbecause U is an\northogonal matrix.\nSo I really have just the V\non one side, a sigma transpose\nsigma, that'll be diagonal,\nand a V transpose the right.\nThis I recognize.\nThis I recognize.\nHere is a single V, a diagonal\nmatrix, a V transpose.\nWhat I'm showing\nyou here, what we\nreached is the eigenvalue,\nthe diagonalization,\nthe usual eigenvalues\nare in here\nand the eigenvectors\nare in here.\nBut the matrix is A transpose A.\nOnce again, A was rectangular\nand completely general\nand we couldn't see\nperfect results.\nBut when we went\nto A transpose A,\nthat gave us a positive\nsemidefinite matrix,\nsymmetric for sure.\nIts eigenvectors\nwill be orthogonal.\nThat's how I know this V\nmatrix, the eigenvectors\nfor this symmetric\nmatrix, are orthogonal\nand the eigenvalues\nare positive.\nAnd they're the squares\nof the singular value.\nSo this is telling\nme the lambdas\nfor A transpose A are the\nsigma squareds for s-- for A.\nFor A itself.\nLambda is the same.\nLambda for A transpose A is\nsigma squared for the matrix A.\nWell that tells me V,\nthat tells me sigma,\nand U disappeared here because\nU transpose U was the identity.\nIt just went away.\nHow would I get hold of U?\nWell, here's one way to see it.\nI multiply A times A transpose\nin that order, in that order.\nSo now I have U\nsigma V transpose\ntimes the transpose,\nwhich is the V sigma\ntranspose U transpose--\nI'm having a lot of fun\nhere with transposes.\nBut V transpose V is now\nthe identity in the middle.\nSo what do I learn here?\nI learn that U is\nthe eigenvector\nmatrix for AA transpose.\nSo these have the\nsame eigenvalues,\nA times B has the\nsame eigenvalues\nas B times A in this\ncase, it comes out here.\nSame eigenvalues.\nThis has eigenvectors V,\nthis has eigenvectors U,\nand those are the V and\nthe U in the singular value\ndecomposition.\nWell, I have to\nshow you an example\nI have to show you an\nexample and an application,\nand that's it.\nSo here's an example.\nSuppose A, I'll make it a\nsquare matrix, 2, 2, minus 1,\n1, not symmetric.\nCertainly not positive definite.\nI wouldn't use the word because\nthat matrix is not symmetric.\nBut it's got an\nSVD, three factors.\nAnd I work them out.\nThis is the orthogonal matrix.\nI have to divide by square root\nof 5 to make it unit vectors.\nOops, that's not going to work.\nHow about that?\nThe two columns are orthogonal\nand that's a perfectly good U.\nAnd then in the sigma, I\ngot, well that's a-- oh,\nI did want 1 and 1.\nI did want 1 and 1, yes.\nSo I have a singular matrix,\ndeterminant 0, singular matrix.\nSo my eigenvalues will be 0 and\nit turns out square root of 10\nis the other eigenvalue for\nthat-- other singular value\nfor this guy.\nAnd now I'll put in the\nV transpose matrix, which\nis 1, 1, and 1, minus 1 is it?\nAnd those have length\nsquare root of 2,\nwhich I have to divide by.\nWell, I didn't do\nthat so smoothly,\nbut the result is clear.\nU, sigma, V transpose,\nso here's the sigma.\nAnd the singular values of this\nmatrix are square root of 10\nand then 0 because\nit's a singular matrix.\nAnd the eigenvectors, well the\nsingular vectors of the matrix\nare the left singular vectors\nand the right singular vectors.\nThat looks good to me.\nAnd now the\napplication to finish.\nA first application is,\nwell, very important.\nAll the time in\nthis century, we're\ngetting matrices\nwith data in them.\nMaybe in life sciences,\nwe test a bunch\nof sample people for genes.\nSo I have a-- my data\ncomes somehoe-- I\nhave a gene expression matrix.\nI have samples, people, people\n1, 2, 3 in those columns.\nAnd I have in the rows,\nlet me say four rows,\nI have genes, gene expressions.\nThat would be completely normal.\nA rectangular matrix,\nbecause the number of people\nand the number of\ngenes is not the same.\nAnd in reality, those are\nboth very, very big numbers,\nso I have a large matrix.\nAnd out of it, I want to--\nand each number in the matrix\nis telling me how much the gene\nis expressed by that person.\nWe may be searching for\ngenes causing some disease.\nSo we take several people, some\nwell, some with the disease,\nwe check on the genes.\nWe get a big matrix and\nwe look to understand\nsomething about of it.\nWhat can we understand?\nWhat are we looking for?\nWe're looking for the\ncorrelation, the connection,\nbetween some combination\nmaybe of genes and some--\nwe're looking for a gene\npeople connection here.\nBut it's not going to\nbe person number one.\nWe're not looking\nfor one person.\nWe're going to find a\nmixture of those people,\nso we're going to have sort of\nan eigensample, eigenpeople.\nOh, that's a terrible--\neigenperson would be better.\nSo I think I'm seeing\nan eigenperson.\nLet me see where I'm\ngoing to put this.\nSo yeah, I think my matrix\nwould be written-- oh, here\nis the main point.\nThat just as I see\nin this example,\nit's the first vector\nand the first vector\nand the biggest sigma\nthat are all important.\nWell, in that example the\nother sigma was 0, nothing.\nBut in this example,\nI'll probably\nhave three different sigmas.\nBut the largest sigma, the\nfirst, the U1 and the V1, it's\nthat combination that I want.\nI want U1 sigma 1 V1 transpose,\nthe first eigenvector\nof A transpose A\nand of AA transpose.\nAnd the first singular,\nthe biggest singular value,\nthat's the information.\nThat's the best\nsort of put together\nperson, eigenperson,\ncombination of these people\nand the best\ncombination of genes.\nIt has the-- in\nstatistics, I would\nsay the greatest variance.\nIn ordinary English, I would\nsay the most information.\nThe most information\nin this big matrix\nis in this very special\nmatrix with only rank one,\nonly a single column repeated.\nA single row\nrepeated, and a number\nsigma 1, the number\nthat tells me that.\nBecause remember,\nU is a unit vector.\nV is a unit vector.\nIt's that number sigma\n1 that's selling me.\nSo it's like that unit vector\ntimes that number, key number,\ntimes that unit\nvector, that's this.\nI'm talking here about\nprinciple component analysis.\nI'm looking for the principle\ncomponent, this part.\nPrinciple component analysis.\nA big application in\napplied statistics.\nYou know, in large\nscale drug tests,\nstatisticians really have\na central place here.\nAnd this is on\nthe research side,\nto find the-- get\nthe information out\nof a big sample.\nSo U1 is sort of a\ncombination of people.\nV1 is a combination of genes.\nSigma 1 is the biggest\nnumber I can get.\nSo that's PCA, all coming\nfrom the singular value\ndecomposition.\nThank you."
    }
]